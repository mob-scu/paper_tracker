<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-02 12:43</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251102_1243</div>
    <div class="row"><div class="card">
<div class="title">Empowering Agentic Video Analytics Systems with Video Language Models</div>
<div class="meta-line">Authors: Yuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, Lili Qiu</div>
<div class="meta-line">First: 2025-05-01T02:40:23+00:00 · Latest: 2025-10-30T03:12:42+00:00</div>
<div class="meta-line">Comments: Accepted to NDSI 2026, 19pages, 12 figures, complementary evaluations
  and appendix</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.00254v4">Abs</a> · <a href="http://arxiv.org/pdf/2505.00254v4">PDF</a> · <a href="https://github.com/I-ESC/Project-Ava">Code1</a> · <a href="https://huggingface.co/datasets/iesc/Ava-100">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI-driven video analytics has become increasingly important across diverse
domains. However, existing systems are often constrained to specific,
predefined tasks, limiting their adaptability in open-ended analytical
scenarios. The recent emergence of Vision Language Models (VLMs) as
transformative technologies offers significant potential for enabling
open-ended video understanding, reasoning, and analytics. Nevertheless, their
limited context windows present challenges when processing ultra-long video
content, which is prevalent in real-world applications. To address this, we
introduce AVA, a VLM-powered system designed for open-ended, advanced video
analytics. AVA incorporates two key innovations: (1) the near real-time
construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or
continuous video streams, and (2) an agentic retrieval-generation mechanism
that leverages EKGs to handle complex and diverse queries. Comprehensive
evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that
AVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,
respectively-significantly surpassing existing VLM and video
Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video
analytics in ultra-long and open-world video scenarios, we introduce a new
benchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours
in duration, along with 120 manually annotated, diverse, and complex
question-answer pairs. On AVA-100, AVA achieves top-tier performance with an
accuracy of 75.8%. The source code of AVA is available at
https://github.com/I-ESC/Project-Ava. The AVA-100 benchmark can be accessed at
https://huggingface.co/datasets/iesc/Ava-100.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过视频语言模型赋能自主视频分析系统</div>
<div class="mono" style="margin-top:8px">基于人工智能的视频分析在各个领域变得越来越重要。然而，现有系统通常局限于特定的预定义任务，限制了它们在开放式分析场景中的适应性。最近出现的视觉语言模型（VLM）作为变革性技术，为实现开放式视频理解、推理和分析提供了显著潜力。然而，它们有限的上下文窗口在处理超长视频内容时面临挑战，这在现实应用中普遍存在。为了解决这个问题，我们介绍了AVA，一个基于VLM的系统，旨在进行开放式的高级视频分析。AVA包含两个关键创新：（1）近实时构建事件知识图（EKG），以高效索引长或连续的视频流；（2）一种自主检索生成机制，利用EKG处理复杂多样的查询。在公共基准测试LVBench和VideoMME-Long上的全面评估表明，AVA达到了最先进的性能，分别获得62.3%和64.1%的准确率，显著超越现有的VLM和视频检索增强生成（RAG）系统。此外，为了评估超长和开放世界视频场景中的视频分析，我们引入了一个新的基准AVA-100。该基准包含8个视频，每个视频时长超过10小时，以及120对手动标注的多样且复杂的问题-答案对。在AVA-100上，AVA以75.8%的准确率达到了顶级性能。AVA的源代码可在https://github.com/I-ESC/Project-Ava获取。AVA-100基准可在https://huggingface.co/datasets/iesc/Ava-100访问。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance the adaptability of AI-driven video analytics systems, which are often limited to predefined tasks, by leveraging Vision Language Models (VLMs) for open-ended video understanding. The authors introduce AVA, a VLM-powered system that innovatively constructs Event Knowledge Graphs (EKGs) in near real-time for efficient indexing of long video streams and employs an agentic retrieval-generation mechanism to address complex queries. Experimental results on public benchmarks LVBench and VideoMME-Long show that AVA achieves state-of-the-art performance with accuracies of 62.3% and 64.1%, respectively, and on the newly introduced AVA-100 benchmark, which includes ultra-long videos, AVA reaches an accuracy of 75.8%.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过利用视觉语言模型（VLM）的能力来增强人工智能驱动的视频分析系统的适应性，这些系统通常仅限于预定义任务，从而实现开放式视频理解。作者提出了AVA，一个基于VLM的系统，创新性地构建事件知识图（EKG）以高效索引长视频流，并采用代理检索生成机制来处理复杂查询。实验结果表明，AVA在公共基准LVBench和VideoMME-Long上分别达到了62.3%和64.1%的最先进性能，并且在新引入的AVA-100基准上表现出色，在超过10小时的超长视频上达到了75.8%的准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal   Attention in Vision Encoders</div>
<div class="meta-line">Authors: Ali Rasekh, Erfan Bagheri Soula, Omid Daliran, Simon Gottschalk, Mohsen Fayyaz</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-29T23:50:57+00:00 · Latest: 2025-10-29T23:50:57+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.26027v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.26027v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://alirasekh.github.io/STAVEQ2/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite significant advances in Multimodal Large Language Models (MLLMs),
understanding complex temporal dynamics in videos remains a major challenge.
Our experiments show that current Video Large Language Model (Video-LLM)
architectures have critical limitations in temporal understanding, struggling
with tasks that require detailed comprehension of action sequences and temporal
progression. In this work, we propose a Video-LLM architecture that introduces
stacked temporal attention modules directly within the vision encoder. This
design incorporates a temporal attention in vision encoder, enabling the model
to better capture the progression of actions and the relationships between
frames before passing visual tokens to the LLM. Our results show that this
approach significantly improves temporal reasoning and outperforms existing
models in video question answering tasks, specifically in action recognition.
We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to
+5.5%. By enhancing the vision encoder with temporal structure, we address a
critical gap in video understanding for Video-LLMs. Project page and code are
available at: https://alirasekh.github.io/STAVEQ2/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过视觉编码器中的堆叠时间注意力增强视频-LLM的时间理解</div>
<div class="mono" style="margin-top:8px">尽管多模态大型语言模型（MLLM）取得了显著进展，理解视频中的复杂时间动态仍然是一个主要挑战。我们的实验表明，当前的视频大型语言模型（Video-LLM）架构在时间理解方面存在关键限制，难以处理需要详细理解动作序列和时间进展的任务。在本研究中，我们提出了一种Video-LLM架构，在视觉编码器中直接引入堆叠时间注意力模块。该设计在视觉编码器中结合了时间注意力，使模型能够更好地捕捉动作的进展和帧之间的关系，然后再将视觉标记传递给LLM。我们的结果表明，这种方法显著改善了时间推理，并在视频问答任务中优于现有模型，特别是在动作识别方面。我们在VITATECS、MVBench和Video-MME等基准上提高了多达+5.5%。通过增强视觉编码器的时间结构，我们解决了Video-LLM视频理解中的一个关键缺口。项目页面和代码可在：https://alirasekh.github.io/STAVEQ2/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the significant challenges that Multimodal Large Language Models (MLLMs) face in understanding complex temporal dynamics in videos. The authors propose a new Video-LLM architecture that integrates stacked temporal attention modules within the vision encoder to enhance the model&#x27;s ability to capture action progression and frame relationships. Experimental results demonstrate that this approach leads to substantial improvements in temporal reasoning, outperforming existing models in video question answering tasks, particularly in action recognition, with benchmark improvements of up to +5.5% on VITATECS, MVBench, and Video-MME.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多模态大型语言模型（MLLMs）在理解视频中复杂时间动态方面面临的重大挑战。作者提出了一种新的视频大型语言模型（Video-LLM）架构，该架构在视觉编码器中集成了堆叠的时间注意模块，以增强模型捕捉动作进展和帧之间关系的能力。实验结果表明，这种方法在时间推理方面显著改善，尤其在动作识别的视频问答任务中优于现有模型，在VITATECS、MVBench和Video-MME等数据集上提高了高达5.5%的基准表现。</div>
</details>
</div>
<div class="card">
<div class="title">StreamingCoT: A Dataset for Temporal Dynamics and Multimodal   Chain-of-Thought Reasoning in Streaming VideoQA</div>
<div class="meta-line">Authors: Yuhang Hu, Zhenyu Yang, Shihan Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Changsheng Xu</div>
<div class="meta-line">First: 2025-10-29T09:47:38+00:00 · Latest: 2025-10-29T09:47:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25332v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25332v1">PDF</a> · <a href="https://github.com/Fleeting-hyh/StreamingCoT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of streaming video applications demands multimodal models
with enhanced capabilities for temporal dynamics understanding and complex
reasoning. However, current Video Question Answering (VideoQA) datasets suffer
from two critical limitations: 1) Static annotation mechanisms fail to capture
the evolving nature of answers in temporal video streams, and 2) The absence of
explicit reasoning process annotations restricts model interpretability and
logical deduction capabilities. To address these challenges, We introduce
StreamingCoT, the first dataset explicitly designed for temporally evolving
reasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our
framework first establishes a dynamic hierarchical annotation architecture that
generates per-second dense descriptions and constructs temporally-dependent
semantic segments through similarity fusion, paired with question-answer sets
constrained by temporal evolution patterns. We further propose an explicit
reasoning chain generation paradigm that extracts spatiotemporal objects via
keyframe semantic alignment, derives object state transition-based reasoning
paths using large language models, and ensures logical coherence through
human-verified validation. This dataset establishes a foundation for advancing
research in streaming video understanding, complex temporal reasoning, and
multimodal inference. Our StreamingCoT and its construction toolkit can be
accessed at https://github.com/Fleeting-hyh/StreamingCoT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StreamingCoT：用于流媒体视频问答中的时间动态和多模态思维链推理的数据集</div>
<div class="mono" style="margin-top:8px">流媒体视频应用的快速增长需要具备增强时间动态理解和复杂推理能力的多模态模型。然而，当前的视频问答（VideoQA）数据集存在两个关键限制：1）静态注释机制无法捕捉时间视频流中答案的演变特性，2）缺乏明确的推理过程注释限制了模型的可解释性和逻辑推理能力。为了解决这些挑战，我们引入了StreamingCoT，这是第一个专门为流媒体VideoQA和多模态思维链（CoT）任务设计的时间演变推理数据集。我们的框架首先建立了一个动态层次注释架构，生成每秒的密集描述，并通过相似性融合构建时间依赖的语义片段，配合受时间演变模式约束的问题-答案集。我们进一步提出了一种明确的推理链生成范式，通过关键帧语义对齐提取时空对象，利用大型语言模型推导基于对象状态转移的推理路径，并通过人工验证确保逻辑一致性。该数据集为推动流媒体视频理解、复杂时间推理和多模态推理的研究奠定了基础。我们的StreamingCoT及其构建工具包可在https://github.com/Fleeting-hyh/StreamingCoT访问。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing prevalence of streaming video applications necessitates the development of multimodal models capable of understanding temporal dynamics and complex reasoning. To tackle the limitations of existing Video Question Answering (VideoQA) datasets, which include static annotations and a lack of reasoning process annotations, the authors introduce StreamingCoT, a dataset designed for temporally evolving reasoning in streaming VideoQA and multimodal Chain-of-Thought tasks. Key experimental findings demonstrate the effectiveness of a dynamic hierarchical annotation architecture and an explicit reasoning chain generation paradigm that enhances model interpretability and logical deduction capabilities, thereby laying the groundwork for future research in streaming video understanding and temporal reasoning.</div>
<div class="mono" style="margin-top:8px">随着流媒体视频应用的快速增长，迫切需要开发能够理解时间动态和复杂推理的多模态模型。为了应对现有视频问答数据集的局限性，这些数据集未能考虑答案的演变特性且缺乏明确的推理过程注释，作者提出了StreamingCoT，这是一个专为流媒体视频问答和多模态思维链任务中的时间演变推理设计的新数据集。主要实验结果表明，所提出的动态层次注释架构和明确的推理链生成范式显著增强了模型的可解释性和逻辑推理能力，从而为未来流媒体视频理解和推理任务的进展奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large   Multimodal Models</div>
<div class="meta-line">Authors: Yolo Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Junhua Huang, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu</div>
<div class="meta-line">First: 2025-10-06T17:10:44+00:00 · Latest: 2025-10-28T18:02:26+00:00</div>
<div class="meta-line">Comments: Version v1.1</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.05034v5">Abs</a> · <a href="http://arxiv.org/pdf/2510.05034v5">PDF</a> · <a href="https://github.com/yunlong10/Awesome-Video-LMM-Post-Training">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频-LMM后训练：深入探讨大型多模态模型的视频推理</div>
<div class="mono" style="margin-top:8px">视频理解是计算机视觉中最具挑战性的前沿领域，要求模型推理复杂的时空关系、长期依赖和多模态证据。最近出现的视频大型多模态模型（Video-LMMs）将视觉编码器与强大的基于解码器的语言模型结合，展示了在视频理解任务中的卓越能力。然而，将这些模型从基本感知系统转变为复杂推理引擎的关键阶段——后训练，在文献中仍然是零散的。本调查首次全面审查了Video-LMMs的后训练方法，涵盖三个基本支柱：带有思维链的监督微调（SFT）、基于可验证目标的强化学习（RL）和通过增强推理计算的测试时扩展（TTS）。我们提出了一个结构化的分类法，阐明了这些技术的角色、相互关系和视频特定的适应性，解决了时间定位、时空定位、长视频效率和多模态证据整合等独特挑战。通过对代表性方法的系统分析，我们综合了关键设计原则、见解和评估协议，同时识别了奖励设计、可扩展性和成本性能优化等关键开放挑战。我们进一步整理了必要的基准、数据集和指标，以促进对后训练有效性的严格评估。本调查旨在为研究人员和从业者提供一个统一的框架，以推动Video-LMM能力的发展。更多资源和更新请访问：https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the complexities of video understanding in computer vision, particularly the need for models that can reason about spatiotemporal relationships and multimodal evidence. The authors conducted a comprehensive survey of post-training methodologies for Video-Large Multimodal Models (Video-LMMs), focusing on supervised fine-tuning, reinforcement learning, and test-time scaling. Key findings include the identification of design principles and challenges in reward design, scalability, and cost-performance optimization, along with the curation of benchmarks and metrics to assess the effectiveness of post-training techniques.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决计算机视觉中视频理解的复杂性，特别是模型有效推理时空关系和多模态证据的需求。作者对视频大型多模态模型（Video-LMMs）的后训练方法进行了全面调查，重点关注监督微调、基于可验证目标的强化学习和测试时刻扩展。研究结果强调了该领域的关键设计原则和挑战，如时间定位和多模态集成，同时提供了结构化分类法和基本基准，以增强后训练有效性的评估。</div>
</details>
</div>
<div class="card">
<div class="title">VideoTG-R1: Boosting Video Temporal Grounding via Curriculum   Reinforcement Learning on Reflected Boundary Annotations</div>
<div class="meta-line">Authors: Lu Dong, Haiyu Zhang, Han Lin, Ziang Yan, Xiangyu Zeng, Hongjie Zhang, Yifei Huang, Yi Wang, Zhen-Hua Ling, Limin Wang, Yali Wang</div>
<div class="meta-line">First: 2025-10-27T14:55:38+00:00 · Latest: 2025-10-27T14:55:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23397v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23397v1">PDF</a> · <a href="https://github.com/ldong1111/VideoTG-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video temporal grounding (VTG) aims to locate precise segments in videos
based on language queries, which is a fundamental challenge in video
understanding. While recent Multimodal Large Language Models (MLLMs) have shown
promise in tackling VTG through reinforcement learning (RL), they overlook the
challenges arising from both the quality and difficulty of training samples.
(1) Partially annotated samples. Many samples contain relevant segments beyond
the annotated interval, introducing ambiguous supervision. (2) Hard-to-ground
samples. Samples with poor zero-shot performance produce consistently low and
indistinguishable rewards during RL training, exhibiting no clear preference
among multiple outputs and thus hindering learning efficiency. To address these
challenges, we propose VideoTG-R1, a novel curriculum RL framework with
reflected boundary annotations, enabling data-efficient training. Specifically,
we propose a Boundary Reflection Agent that utilizes MLLMs to predict
query-relevant timestamps outside the annotated intervals, allowing us to
identify and filter out partially annotated samples, thereby reducing
ambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assess
the training difficulty of each sample and design a curriculum RL strategy that
dynamically masks the videos of hard-to-ground samples according to the
training steps, easing the training difficulty and providing clearer
preference. Experiments on the VTG and grounded VideoQA tasks demonstrate the
effectiveness of our method. Remarkably, with only 10% of the training samples
and 21% of the computational budget, VideoTG-R1 outperforms full-data
counterparts under both group relative policy optimization (GRPO) and
supervised fine-tuning (SFT). The code is available at
https://github.com/ldong1111/VideoTG-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoTG-R1：通过反射边界注释的课程强化学习提升视频时间定位</div>
<div class="mono" style="margin-top:8px">视频时间定位（VTG）旨在根据语言查询定位视频中的精确片段，这是视频理解中的一个基本挑战。尽管最近的多模态大型语言模型（MLLMs）在通过强化学习（RL）解决VTG方面显示出潜力，但它们忽视了训练样本质量和难度带来的挑战。（1）部分注释样本。许多样本包含超出注释区间的相关片段，导致模糊的监督。（2）难以定位的样本。表现不佳的零-shot样本在RL训练期间产生持续低且不可区分的奖励，在多个输出之间没有明显偏好，从而阻碍学习效率。为了解决这些挑战，我们提出了VideoTG-R1，一种具有反射边界注释的新型课程RL框架，实现数据高效训练。具体而言，我们提出了一种边界反射代理，利用MLLMs预测注释区间外的查询相关时间戳，使我们能够识别和过滤部分注释样本，从而减少模糊性。此外，我们引入了一种难度评估代理，以评估每个样本的训练难度，并设计了一种课程RL策略，根据训练步骤动态屏蔽难以定位样本的视频，减轻训练难度并提供更清晰的偏好。在VTG和基础视频问答任务上的实验表明我们方法的有效性。值得注意的是，仅使用10%的训练样本和21%的计算预算，VideoTG-R1在组相对策略优化（GRPO）和监督微调（SFT）下均优于全数据对手。代码可在https://github.com/ldong1111/VideoTG-R1获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve video temporal grounding (VTG), which is essential for accurately locating segments in videos based on language queries, while addressing challenges related to the quality and difficulty of training samples. The authors propose a novel curriculum reinforcement learning framework called VideoTG-R1, which incorporates reflected boundary annotations to enhance data efficiency. Key experimental findings indicate that VideoTG-R1 significantly outperforms traditional methods, achieving better performance with only 10% of the training samples and 21% of the computational budget, as demonstrated in VTG and grounded VideoQA tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于改善视频时间定位（VTG），这对于根据语言查询准确定位视频片段至关重要，同时解决与训练样本质量和难度相关的挑战。作者提出了一种名为VideoTG-R1的新型课程强化学习框架，该框架结合了反射边界注释，并利用边界反射代理预测注释区间外的相关时间戳，从而过滤掉模糊的样本。实验结果表明，VideoTG-R1在VTG和基础视频问答任务中显著优于全数据对照组，仅使用10%的训练样本和21%的计算预算就实现了比传统方法更好的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluation of Vision-LLMs in Surveillance Video</div>
<div class="meta-line">Authors: Pascal Benschop, Cristian Meo, Justin Dauwels, Jelte P. Mense</div>
<div class="meta-line">Venue: NeurIPS 2025 poster</div>
<div class="meta-line">First: 2025-10-27T10:27:02+00:00 · Latest: 2025-10-27T10:27:02+00:00</div>
<div class="meta-line">Comments: Accepted as poster in the NeurIPS 2025 Workshop on Space in Vision,
  Language, and Embodied AI</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23190v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23190v1">PDF</a> · <a href="https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread use of cameras in our society has created an overwhelming
amount of video data, far exceeding the capacity for human monitoring. This
presents a critical challenge for public safety and security, as the timely
detection of anomalous or criminal events is crucial for effective response and
prevention. The ability for an embodied agent to recognize unexpected events is
fundamentally tied to its capacity for spatial reasoning. This paper
investigates the spatial reasoning of vision-language models (VLMs) by framing
anomalous action recognition as a zero-shot, language-grounded task, addressing
the embodied perception challenge of interpreting dynamic 3D scenes from sparse
2D video. Specifically, we investigate whether small, pre-trained vision--LLMs
can act as spatially-grounded, zero-shot anomaly detectors by converting video
into text descriptions and scoring labels via textual entailment. We evaluate
four open models on UCF-Crime and RWF-2000 under prompting and
privacy-preserving conditions. Few-shot exemplars can improve accuracy for some
models, but may increase false positives, and privacy filters -- especially
full-body GAN transforms -- introduce inconsistencies that degrade accuracy.
These results chart where current vision--LLMs succeed (simple, spatially
salient events) and where they falter (noisy spatial cues, identity
obfuscation). Looking forward, we outline concrete paths to strengthen spatial
grounding without task-specific training: structure-aware prompts, lightweight
spatial memory across clips, scene-graph or 3D-pose priors during description,
and privacy methods that preserve action-relevant geometry. This positions
zero-shot, language-grounded pipelines as adaptable building blocks for
embodied, real-world video understanding. Our implementation for evaluating
VLMs is publicly available at:
https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>监控视频中视觉-大语言模型的评估</div>
<div class="mono" style="margin-top:8px">我们社会中摄像头的广泛使用产生了大量视频数据，远超人类监控的能力。这对公共安全和安保提出了严峻挑战，因为及时检测异常或犯罪事件对于有效响应和预防至关重要。具身代理识别意外事件的能力与其空间推理能力密切相关。本文通过将异常行为识别框定为零-shot、语言基础任务，研究视觉-语言模型（VLMs）的空间推理，解决从稀疏2D视频中解释动态3D场景的具身感知挑战。具体而言，我们研究小型预训练视觉-大语言模型是否可以通过将视频转换为文本描述并通过文本蕴涵评分标签，作为空间基础的零-shot异常检测器。我们在UCF-Crime和RWF-2000上评估四个开放模型，采用提示和隐私保护条件。少量示例可以提高某些模型的准确性，但可能增加假阳性，而隐私过滤器——尤其是全身GAN变换——引入的不一致性会降低准确性。这些结果描绘了当前视觉-大语言模型成功的地方（简单、空间显著事件）和失败的地方（嘈杂的空间线索、身份模糊）。展望未来，我们概述了在没有特定任务训练的情况下加强空间基础的具体路径：结构感知提示、跨片段的轻量级空间记忆、描述期间的场景图或3D姿态先验，以及保留与动作相关几何形状的隐私方法。这使得零-shot、语言基础的管道成为适应性强的具身现实视频理解的构建模块。我们用于评估视觉-语言模型的实现已公开可用：
https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing prevalence of surveillance cameras has resulted in vast amounts of video data, posing challenges for timely detection of anomalous events critical for public safety. This study evaluates the spatial reasoning capabilities of vision-language models (VLMs) by framing anomalous action recognition as a zero-shot, language-grounded task, converting video into text descriptions and scoring labels through textual entailment. The evaluation of four open models on UCF-Crime and RWF-2000 reveals that while few-shot exemplars can enhance accuracy for some models, they may also lead to increased false positives, and privacy filters can introduce inconsistencies that reduce accuracy, highlighting the strengths and weaknesses of current VLMs in recognizing spatially salient events and dealing with noisy spatial cues.</div>
<div class="mono" style="margin-top:8px">监控视频数据的激增需要有效的方法来及时检测异常事件，以增强公共安全。本研究通过将异常行为识别视为零样本、基于语言的任务，评估视觉语言模型（VLM）的空间推理能力，其中视频被转换为文本描述，通过文本蕴含进行标签评分。在UCF-Crime和RWF-2000上对四个开放模型的评估显示，虽然少量示例可以提高某些模型的准确性，但也可能导致假阳性增加，而隐私保护方法可能引入不一致性，负面影响准确性，突显了当前VLM在识别空间显著事件与噪声线索和身份模糊方面的优缺点。</div>
</details>
</div>
<div class="card">
<div class="title">VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding</div>
<div class="meta-line">Authors: Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Fuxiao Liu, Tianyi Zhou, Dinesh Manocha, Jordan Lee Boyd-Graber</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-02T15:58:38+00:00 · Latest: 2025-10-26T04:54:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.01481v4">Abs</a> · <a href="http://arxiv.org/pdf/2505.01481v4">PDF</a> · <a href="https://github.com/zli12321/VideoHallu.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have achieved strong results in video
understanding, yet a key question remains: do they truly comprehend visual
content or only learn shallow correlations between vision and language? Real
visual understanding, especially of physics and common sense, is essential for
AI systems that interact with the physical world. Current evaluations mostly
use real-world videos similar to training data, so high benchmark scores may
not reflect real reasoning ability. To address this, we propose
negative-control tests using videos that depict physically impossible or
logically inconsistent events. We introduce VideoHallu, a synthetic dataset of
physics- and commonsense-violating scenes generated with Veo2, Sora, and Kling.
It includes expert-annotated question-answer pairs across four categories of
violations. Tests of leading VLMs (Qwen-2.5-VL, Video-R1, VideoChat-R1) show
that, despite strong results on benchmarks such as MVBench and MMVU, they often
miss these violations, exposing gaps in visual reasoning. Reinforcement
learning fine-tuning on VideoHallu improves recognition of such violations
without reducing standard benchmark performance. Our data is available at
https://github.com/zli12321/VideoHallu.git.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoHallu：评估和缓解合成视频理解中的多模态幻觉</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在视频理解方面取得了强劲的成果，但一个关键问题仍然存在：它们是否真正理解视觉内容，还是仅仅学习了视觉与语言之间的浅层关联？真正的视觉理解，特别是物理和常识的理解，对于与物理世界互动的人工智能系统至关重要。目前的评估大多使用与训练数据相似的真实世界视频，因此高基准分数可能无法反映真实的推理能力。为了解决这个问题，我们提出了使用描绘物理上不可能或逻辑上不一致事件的视频进行负控制测试。我们引入了VideoHallu，这是一个由Veo2、Sora和Kling生成的违反物理和常识场景的合成数据集。它包括跨四类违规的专家注释问答对。对领先的VLM（Qwen-2.5-VL、Video-R1、VideoChat-R1）的测试表明，尽管在MVBench和MMVU等基准上取得了强劲的结果，但它们往往会忽视这些违规，暴露出视觉推理的缺口。在VideoHallu上进行强化学习微调可以提高对这些违规的识别，而不会降低标准基准性能。我们的数据可在https://github.com/zli12321/VideoHallu.git获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to evaluate the true understanding of visual content by Vision-Language Models (VLMs), as current benchmarks may not accurately reflect their reasoning abilities. The authors introduce VideoHallu, a synthetic dataset designed to test VLMs on videos that depict physically impossible or logically inconsistent events, using negative-control tests. Experimental results reveal that leading VLMs, despite high scores on traditional benchmarks, often fail to recognize these violations, indicating significant gaps in their visual reasoning capabilities; however, fine-tuning with reinforcement learning on VideoHallu enhances their ability to identify such inconsistencies without compromising performance on standard benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于评估视觉语言模型（VLMs）对视觉内容的真实理解，特别是在物理和常识方面，因为现有基准可能无法准确反映它们的推理能力。作者开发了VideoHallu，一个合成数据集，包含展示物理不可能或逻辑不一致事件的视频，并对领先的VLMs进行了负控制测试。研究结果表明，这些模型尽管在传统基准上表现良好，但常常无法识别VideoHallu数据集中存在的违规情况，显示出它们在视觉推理能力方面的显著差距；然而，通过对该数据集进行强化学习微调，可以提高它们对这些违规情况的识别能力，而不影响它们在标准基准上的表现。</div>
</details>
</div>
<div class="card">
<div class="title">MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning</div>
<div class="meta-line">Authors: Tieyuan Chen, Huabin Liu, Yi Wang, Yihang Chen, Tianyao He, Chaofan Gan, Huanyu He, Weiyao Lin</div>
<div class="meta-line">First: 2025-01-13T11:28:49+00:00 · Latest: 2025-10-26T04:45:17+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE TPAMI (IEEE Transactions on Pattern Analysis and
  Machine Intelligence). arXiv admin note: substantial text overlap with
  arXiv:2409.17647</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2501.07227v4">Abs</a> · <a href="http://arxiv.org/pdf/2501.07227v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video causal reasoning aims to achieve a high-level understanding of videos
from a causal perspective. However, it exhibits limitations in its scope,
primarily executed in a question-answering paradigm and focusing on brief video
segments containing isolated events and basic causal relations, lacking
comprehensive and structured causality analysis for videos with multiple
interconnected events. To fill this gap, we introduce a new task and dataset,
Multi-Event Causal Discovery (MECD). It aims to uncover the causal relations
between events distributed chronologically across long videos. Given visual
segments and textual descriptions of events, MECD identifies the causal
associations between these events to derive a comprehensive and structured
event-level video causal graph explaining why and how the result event
occurred. To address the challenges of MECD, we devise a novel framework
inspired by the Granger Causality method, incorporating an efficient mask-based
event prediction model to perform an Event Granger Test. It estimates causality
by comparing the predicted result event when premise events are masked versus
unmasked. Furthermore, we integrate causal inference techniques such as
front-door adjustment and counterfactual inference to mitigate challenges in
MECD like causality confounding and illusory causality. Additionally, context
chain reasoning is introduced to conduct more robust and generalized reasoning.
Experiments validate the effectiveness of our framework in reasoning complete
causal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,
respectively. Further experiments demonstrate that causal relation graphs can
also contribute to downstream video understanding tasks such as video question
answering and video event prediction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MECD+: 解锁事件级因果图发现以进行视频推理</div>
<div class="mono" style="margin-top:8px">视频因果推理旨在从因果角度实现对视频的高层次理解。然而，它在范围上存在局限，主要在问答范式中执行，关注包含孤立事件和基本因果关系的简短视频片段，缺乏对包含多个相互关联事件的视频的全面和结构化因果分析。为填补这一空白，我们引入了一项新任务和数据集，多事件因果发现（MECD）。它旨在揭示在长视频中按时间顺序分布的事件之间的因果关系。给定事件的视觉片段和文本描述，MECD识别这些事件之间的因果关联，以推导出一个全面和结构化的事件级视频因果图，解释结果事件为何以及如何发生。为应对MECD的挑战，我们设计了一个新颖的框架，灵感来自Granger因果性方法，结合高效的基于掩码的事件预测模型来执行事件Granger测试。它通过比较掩码前提事件与未掩码时预测的结果事件来估计因果性。此外，我们整合了因果推断技术，如前门调整和反事实推断，以减轻MECD中的因果混淆和虚假因果性等挑战。此外，引入上下文链推理以进行更稳健和广泛的推理。实验验证了我们框架在推理完整因果关系方面的有效性，分别比GPT-4o和VideoChat2提高了5.77%和2.70%。进一步的实验表明，因果关系图也可以为下游视频理解任务（如视频问答和视频事件预测）做出贡献。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance video causal reasoning by addressing its limitations in analyzing complex events and their interrelations. The authors introduce a new task and dataset called Multi-Event Causal Discovery (MECD), which aims to uncover causal relationships among events in long videos using visual segments and textual descriptions. They propose a novel framework inspired by Granger Causality, employing a mask-based event prediction model to conduct an Event Granger Test, and incorporate causal inference techniques to address confounding issues. Experimental results show that their framework outperforms existing models like GPT-4o and VideoChat2 by 5.77% and 2.70%, respectively, and also enhances performance in downstream tasks such as video question answering and event prediction.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决分析长视频中复杂相互关联事件的局限性来增强视频因果推理。作者引入了一个新的任务和数据集，称为多事件因果发现（MECD），旨在识别时间上事件之间的因果关系。他们开发了一种受格兰杰因果关系启发的新框架，利用基于掩码的事件预测模型进行事件格兰杰检验，并结合因果推断技术以应对混淆等挑战。实验结果表明，他们的框架显著改善了因果推理，分别比现有模型GPT-4o和VideoChat2提高了5.77%和2.70%，并且在相关视频理解任务中也提升了性能。</div>
</details>
</div>
<div class="card">
<div class="title">Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video   Reasoning</div>
<div class="meta-line">Authors: Daeun Lee, Jaehong Yoon, Jaemin Cho, Mohit Bansal</div>
<div class="meta-line">First: 2025-06-04T03:18:01+00:00 · Latest: 2025-10-24T15:17:29+00:00</div>
<div class="meta-line">Comments: Project website: https://video-skill-cot.github.io/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.03525v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.03525v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://video-skill-cot.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Chain-of-Thought (CoT) reasoning have improved complex
video understanding, but existing methods often struggle to adapt to
domain-specific skills (e.g., event detection, spatial relation understanding,
emotion understanding) over various video content. To address this, we propose
Video-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs
and leverages skill-aware CoT supervisions for domain-adaptive video reasoning.
First, we construct skill-based CoT annotations: we extract domain-relevant
reasoning skills from training questions, cluster them into a shared skill
taxonomy, and create detailed multi-step CoT rationale tailored to each
video-question pair for training. Second, we introduce a skill-specific expert
learning framework. Each expert module specializes in a subset of reasoning
skills and is trained with lightweight adapters using the collected CoT
supervision. We demonstrate the effectiveness of the proposed approach on three
video understanding benchmarks, where Video-SKoT consistently outperforms
strong baselines. We also provide in-depth analyses on comparing different CoT
annotation pipelines and learned skills over multiple video domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频技能链思维：基于技能的领域自适应视频推理</div>
<div class="mono" style="margin-top:8px">最近在链思维（CoT）推理方面的进展改善了复杂视频理解，但现有方法往往难以适应特定领域的技能（例如，事件检测、空间关系理解、情感理解）在各种视频内容中的应用。为了解决这个问题，我们提出了视频技能链思维（Video-Skill-CoT，简称Video-SKoT），这是一个自动构建和利用技能感知的CoT监督以实现领域自适应视频推理的框架。首先，我们构建基于技能的CoT注释：从训练问题中提取与领域相关的推理技能，将其聚类为共享技能分类法，并为每个视频-问题对创建详细的多步骤CoT推理。其次，我们引入了一个特定技能的专家学习框架。每个专家模块专注于一组推理技能，并使用收集的CoT监督通过轻量级适配器进行训练。我们在三个视频理解基准上展示了所提方法的有效性，其中Video-SKoT始终优于强基线。我们还提供了对不同CoT注释管道和多个视频领域中学习技能的深入分析。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance domain-adaptive video reasoning by addressing the limitations of existing Chain-of-Thought (CoT) methods in adapting to specific reasoning skills across various video content. The authors propose Video-Skill-CoT, a framework that constructs skill-aware CoT supervisions by extracting relevant reasoning skills from training questions, clustering them into a shared taxonomy, and creating tailored multi-step CoT rationales for each video-question pair. Experimental results on three video understanding benchmarks show that Video-SKoT consistently outperforms strong baseline methods, and the study includes detailed analyses of different CoT annotation pipelines and learned skills across multiple video domains.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决现有链式思维（CoT）方法在适应视频理解所需特定技能方面的局限性，来增强领域自适应视频推理。作者提出了一个名为Video-Skill-CoT的框架，该框架通过从训练问题中提取相关推理技能并为每个视频-问题对创建量身定制的多步骤推理，构建基于技能的CoT注释。实验结果表明，Video-SKoT在三个视频理解基准测试中优于强基线，突显了技能特定专家学习框架的有效性以及基于技能的CoT监督在提高视频推理性能中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">MUVR: A Multi-Modal Untrimmed Video Retrieval Benchmark with Multi-Level   Visual Correspondence</div>
<div class="meta-line">Authors: Yue Feng, Jinwei Hu, Qijia Lu, Jiawei Niu, Li Tan, Shuo Yuan, Ziyi Yan, Yizhen Jia, Qingzhi He, Shiping Ge, Ethan Q. Chen, Wentong Li, Limin Wang, Jie Qin</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-24T12:50:02+00:00 · Latest: 2025-10-24T12:50:02+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025 D&amp;B Track</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.21406v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.21406v1">PDF</a> · <a href="https://github.com/debby-0527/MUVR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose the Multi-modal Untrimmed Video Retrieval task, along with a new
benchmark (MUVR) to advance video retrieval for long-video platforms. MUVR aims
to retrieve untrimmed videos containing relevant segments using multi-modal
queries. It has the following features: 1) Practical retrieval paradigm: MUVR
supports video-centric multi-modal queries, expressing fine-grained retrieval
needs through long text descriptions, video tag prompts, and mask prompts. It
adopts a one-to-many retrieval paradigm and focuses on untrimmed videos,
tailored for long-video platform applications. 2) Multi-level visual
correspondence: To cover common video categories (e.g., news, travel, dance)
and precisely define retrieval matching criteria, we construct multi-level
visual correspondence based on core video content (e.g., news events, travel
locations, dance moves) which users are interested in and want to retrieve. It
covers six levels: copy, event, scene, instance, action, and others. 3)
Comprehensive evaluation criteria: We develop 3 versions of MUVR (i.e., Base,
Filter, QA). MUVR-Base/Filter evaluates retrieval models, while MUVR-QA
assesses MLLMs in a question-answering format. We also propose a Reranking
Score to evaluate the reranking ability of MLLMs. MUVR consists of 53K
untrimmed videos from the video platform Bilibili, with 1,050 multi-modal
queries and 84K matches. Extensive evaluations of 3 state-of-the-art video
retrieval models, 6 image-based VLMs, and 10 MLLMs are conducted. MUVR reveals
the limitations of retrieval methods in processing untrimmed videos and
multi-modal queries, as well as MLLMs in multi-video understanding and
reranking. Our code and benchmark is available at
https://github.com/debby-0527/MUVR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MUVR：一个具有多级视觉对应的多模态未剪辑视频检索基准</div>
<div class="mono" style="margin-top:8px">我们提出了多模态未剪辑视频检索任务，以及一个新的基准（MUVR），以推动长视频平台的视频检索。MUVR旨在使用多模态查询检索包含相关片段的未剪辑视频。其特点包括：1）实用的检索范式：MUVR支持以视频为中心的多模态查询，通过长文本描述、视频标签提示和掩码提示表达细粒度的检索需求。它采用一对多的检索范式，专注于未剪辑视频，适用于长视频平台应用。2）多级视觉对应：为了覆盖常见视频类别（如新闻、旅行、舞蹈）并精确定义检索匹配标准，我们基于用户感兴趣并希望检索的核心视频内容（如新闻事件、旅行地点、舞蹈动作）构建了多级视觉对应。它涵盖六个级别：复制、事件、场景、实例、动作和其他。3）全面的评估标准：我们开发了3个版本的MUVR（即Base、Filter、QA）。MUVR-Base/Filter评估检索模型，而MUVR-QA以问答格式评估MLLMs。我们还提出了一个重新排序评分来评估MLLMs的重新排序能力。MUVR包含来自视频平台Bilibili的53K个未剪辑视频，具有1,050个多模态查询和84K个匹配。对3个最先进的视频检索模型、6个基于图像的VLM和10个MLLM进行了广泛评估。MUVR揭示了检索方法在处理未剪辑视频和多模态查询方面的局限性，以及MLLM在多视频理解和重新排序方面的不足。我们的代码和基准可在https://github.com/debby-0527/MUVR获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance video retrieval capabilities for long-video platforms by addressing the challenges of retrieving relevant segments from untrimmed videos using multi-modal queries. The authors introduce the Multi-modal Untrimmed Video Retrieval (MUVR) benchmark, which incorporates a practical retrieval paradigm that supports various query types, including long text descriptions and video tags, and establishes a multi-level visual correspondence framework to define retrieval criteria across different video categories. Experimental results demonstrate the limitations of existing retrieval methods in handling untrimmed videos and multi-modal queries, as well as the challenges faced by multi-modal large language models in understanding multiple videos and reranking results effectively.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决未剪辑视频和多模态查询相关的挑战，增强长视频平台的视频检索能力。作者提出了多模态未剪辑视频检索（MUVR）基准，支持一对多的检索范式，并在多个视频类别中融入多层次视觉对应关系。实验结果表明，现有的检索方法和多模态语言模型（MLLMs）在有效处理未剪辑视频和理解多视频上下文方面存在局限性，突显了该领域改进技术的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Two Causally Related Needles in a Video Haystack</div>
<div class="meta-line">Authors: Miaoyu Li, Qin Chao, Boyang Li</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-26T11:37:34+00:00 · Latest: 2025-10-24T09:46:52+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025 D&amp;B Track</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.19853v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.19853v2">PDF</a> · <a href="https://huggingface.co/datasets/causal2needles/Causal2Needles">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Properly evaluating the ability of Video-Language Models (VLMs) to understand
long videos remains a challenge. We propose a long-context video understanding
benchmark, Causal2Needles, that assesses two crucial abilities insufficiently
addressed by existing benchmarks: (1) extracting information from two separate
locations (two needles) in a long video and understanding them jointly, and (2)
modeling the world in terms of cause and effect in human behaviors.
Causal2Needles evaluates these abilities using noncausal one-needle, causal
one-needle, and causal two-needle questions. The most complex question type,
causal two-needle questions, require extracting information from both the cause
and effect events from a long video and the associated narration text. To
prevent textual bias, we introduce two complementary question formats: locating
the video clip containing the answer, and verbal description of a visual detail
from that video clip. Our experiments reveal that models excelling on existing
benchmarks struggle with causal 2-needle questions, and the model performance
is negatively correlated with the distance between the two needles. These
findings highlight critical limitations in current VLMs. The dataset is
available at: https://huggingface.co/datasets/causal2needles/Causal2Needles</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频干草堆中的两个因果相关的针</div>
<div class="mono" style="margin-top:8px">正确评估视频语言模型（VLMs）理解长视频的能力仍然是一个挑战。我们提出了一个长上下文视频理解基准，Causal2Needles，评估现有基准不足以解决的两个关键能力：（1）从长视频中的两个不同位置（两根针）提取信息并共同理解，以及（2）以因果关系建模人类行为。Causal2Needles通过非因果单针、因果单针和因果双针问题来评估这些能力。最复杂的问题类型，因果双针问题，需要从长视频及其相关叙述文本中提取因果事件的信息。为了防止文本偏见，我们引入了两种互补的问题格式：定位包含答案的视频片段，以及对该视频片段中视觉细节的口头描述。我们的实验表明，在现有基准上表现出色的模型在因果双针问题上表现不佳，模型性能与两根针之间的距离呈负相关。这些发现突显了当前VLMs的关键局限性。数据集可在以下网址获取：https://huggingface.co/datasets/causal2needles/Causal2Needles</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of evaluating Video-Language Models (VLMs) in understanding long videos, particularly their ability to extract information from multiple locations and model causal relationships in human behavior. The authors introduce a benchmark called Causal2Needles, which includes various question types to assess these abilities, specifically focusing on causal two-needle questions that require extracting information from both cause and effect events in a long video. Experimental results indicate that models performing well on existing benchmarks struggle with causal two-needle questions, and performance declines as the distance between the two information points increases, revealing significant limitations in current VLMs.</div>
<div class="mono" style="margin-top:8px">本研究旨在评估视频语言模型（VLMs）理解长视频的能力，特别是提取和共同理解来自两个不同位置的信息，同时建模人类行为中的因果关系。作者提出了一个新的基准，称为Causal2Needles，包含多种问题类型以评估这些能力，特别关注因果双针问题，这要求从长视频中的因果事件提取信息。实验结果表明，在现有基准上表现良好的模型在因果双针问题上表现不佳，并且随着两个信息源之间距离的增加，性能下降，这揭示了当前VLMs的重大局限性。</div>
</details>
</div>
<div class="card">
<div class="title">InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video   Understanding</div>
<div class="meta-line">Authors: Minsoo Kim, Kyuhong Shim, Jungwook Choi, Simyung Chang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-18T02:22:14+00:00 · Latest: 2025-10-24T05:39:03+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.15745v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.15745v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern multimodal large language models (MLLMs) can reason over hour-long
video, yet their key-value (KV) cache grows linearly with time-quickly
exceeding the fixed memory of phones, AR glasses, and edge robots. Prior
compression schemes either assume the whole video and user query are available
offline or must first build the full cache, so memory still scales with stream
length. InfiniPot-V is the first training-free, query-agnostic framework that
enforces a hard, length-independent memory cap for streaming video
understanding. During video encoding it monitors the cache and, once a user-set
threshold is reached, runs a lightweight compression pass that (i) removes
temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)
keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four
open-source MLLMs and four long-video and streaming-video benchmarks,
InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,
and matches or surpasses full-cache accuracy-even in multi-turn dialogues. By
dissolving the KV cache bottleneck without retraining or query knowledge,
InfiniPot-V closes the gap for on-device streaming video assistants.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InfiniPot-V：用于流视频理解的内存受限KV缓存压缩</div>
<div class="mono" style="margin-top:8px">现代多模态大型语言模型（MLLMs）能够对长达一小时的视频进行推理，但它们的键值（KV）缓存随着时间线性增长，迅速超过手机、AR眼镜和边缘机器人固定的内存。之前的压缩方案要么假设整个视频和用户查询在离线状态下可用，要么必须首先构建完整的缓存，因此内存仍然随着流长度而扩展。InfiniPot-V是第一个无训练、与查询无关的框架，为流视频理解强制实施一个硬的、与长度无关的内存上限。在视频编码过程中，它监控缓存，并在达到用户设定的阈值后，运行轻量级压缩过程，(i) 通过时间轴冗余（TaR）度量去除时间上冗余的标记，(ii) 通过值规范（VaN）排名保留语义上重要的标记。在四个开源MLLM和四个长视频及流视频基准测试中，InfiniPot-V将峰值GPU内存减少了多达94%，维持实时生成，并在多轮对话中匹配或超过完整缓存的准确性。通过在不重新训练或查询知识的情况下消除KV缓存瓶颈，InfiniPot-V缩小了设备上流视频助手的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the memory limitations of modern multimodal large language models (MLLMs) when processing long videos, as their key-value (KV) cache can quickly exceed the memory capacity of devices like phones and AR glasses. The authors propose InfiniPot-V, a training-free and query-agnostic framework that enforces a fixed memory cap for streaming video understanding by monitoring the cache during video encoding and applying a lightweight compression pass when a user-defined threshold is reached. Experimental results demonstrate that InfiniPot-V can reduce peak GPU memory usage by up to 94% across four open-source MLLMs and various long-video benchmarks while maintaining real-time generation and achieving comparable or superior accuracy to full-cache methods, even in multi-turn dialogues.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现代多模态大型语言模型（MLLMs）在处理长视频时的局限性，因为它们的键值（KV）缓存很快就会超过手机和增强现实眼镜等设备的内存容量。作者提出了InfiniPot-V，这是一种无训练且与查询无关的框架，能够对流视频理解施加固定的内存上限。实验结果表明，InfiniPot-V能够将峰值GPU内存使用减少多达94%，同时保持实时生成，并在各种基准测试中实现与全缓存方法相当或更优的准确性，有效缓解了设备上流视频助手的KV缓存瓶颈。</div>
</details>
</div>
<div class="card">
<div class="title">SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video   Understanding</div>
<div class="meta-line">Authors: Yuan Sheng, Yanbin Hao, Chenxu Li, Shuo Wang, Xiangnan He</div>
<div class="meta-line">First: 2025-10-23T14:55:28+00:00 · Latest: 2025-10-23T14:55:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20622v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20622v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long video understanding remains challenging due to its complex, diverse, and
temporally scattered content. Although video large language models (Video-LLMs)
can process videos lasting tens of minutes, applying them to truly long
sequences is computationally prohibitive and often leads to unfocused or
inconsistent reasoning. A promising solution is to select only the most
informative frames, yet existing approaches typically ignore temporal
dependencies or rely on unimodal evidence, limiting their ability to provide
complete and query-relevant context. We propose a Semantic-Visual Consensus
Evidence Selection (SeViCES) framework for effective and reliable long video
understanding. SeViCES is training-free and model-agnostic, and introduces two
key components. The Semantic-Visual Consensus Frame Selection (SVCFS) module
selects frames through (1) a temporal-aware semantic branch that leverages LLM
reasoning over captions, and (2) a cluster-guided visual branch that aligns
embeddings with semantic scores via mutual information. The Answer Consensus
Refinement (ACR) module further resolves inconsistencies between semantic- and
visual-based predictions by fusing evidence and constraining the answer space.
Extensive experiments on long video understanding benchmarks show that SeViCES
consistently outperforms state-of-the-art methods in both accuracy and
robustness, demonstrating the importance of consensus-driven evidence selection
for Video-LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SeViCES：统一语义-视觉证据共识以理解长视频</div>
<div class="mono" style="margin-top:8px">长视频理解仍然具有挑战性，因为其内容复杂、多样且时间上分散。尽管视频大型语言模型（Video-LLMs）可以处理持续数十分钟的视频，但将其应用于真正的长序列在计算上是不可行的，且往往导致推理不集中或不一致。一种有前景的解决方案是仅选择最具信息量的帧，但现有方法通常忽略时间依赖性或依赖单一模态证据，限制了其提供完整和查询相关上下文的能力。我们提出了一种语义-视觉共识证据选择（SeViCES）框架，以实现有效和可靠的长视频理解。SeViCES是无训练和模型无关的，并引入了两个关键组件。语义-视觉共识帧选择（SVCFS）模块通过（1）利用LLM对字幕进行推理的时间感知语义分支，以及（2）通过互信息对嵌入与语义分数进行对齐的聚类引导视觉分支来选择帧。答案共识精炼（ACR）模块通过融合证据和限制答案空间进一步解决语义和视觉预测之间的不一致性。在长视频理解基准上的广泛实验表明，SeViCES在准确性和鲁棒性方面始终优于最先进的方法，证明了共识驱动的证据选择对Video-LLMs的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of long video understanding, which is hindered by complex and temporally scattered content. The authors propose a training-free and model-agnostic framework called Semantic-Visual Consensus Evidence Selection (SeViCES), which includes a Semantic-Visual Consensus Frame Selection (SVCFS) module that utilizes both temporal-aware semantic reasoning and cluster-guided visual alignment, along with an Answer Consensus Refinement (ACR) module to resolve inconsistencies in predictions. Experimental results demonstrate that SeViCES significantly outperforms existing state-of-the-art methods in accuracy and robustness on long video understanding benchmarks, highlighting the effectiveness of consensus-driven evidence selection for Video-LLMs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决长视频理解中的挑战，这些挑战源于内容的复杂性和时间上的分散性。作者提出了语义-视觉共识证据选择（SeViCES）框架，该框架无需训练且与模型无关，包含两个主要组件：语义-视觉共识帧选择（SVCFS）模块，通过时间感知的语义分支和集群引导的视觉分支选择信息丰富的帧，以及答案共识精炼（ACR）模块，解决语义和视觉预测之间的不一致性。实验结果表明，SeViCES在长视频理解基准上在准确性和鲁棒性方面显著优于现有的最先进方法，突显了共识驱动的证据选择在视频大语言模型中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal   Evidence</div>
<div class="meta-line">Authors: Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, Zhuochen Wang</div>
<div class="meta-line">First: 2025-10-23T14:05:56+00:00 · Latest: 2025-10-23T14:05:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20579v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20579v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most video reasoning models only generate textual reasoning traces without
indicating when and where key evidence appears. Recent models such as OpenAI-o3
have sparked wide interest in evidence-centered reasoning for images, yet
extending this ability to videos is more challenging, as it requires joint
temporal tracking and spatial localization across dynamic scenes. We introduce
Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal
evidence into video reasoning, and carefully collect training data and design
training strategies to address the aforementioned challenges. The model
highlights key timestamps, objects, and bounding boxes alongside its answers,
allowing reasoning to be grounded in concrete visual observations. To enable
this functionality, we first curate and build two high-quality datasets,
STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed
temporal and spatial annotations, since most existing datasets offer either
temporal spans for videos or spatial boxes on images, lacking unified
spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start
reinforcement learning strategy with multiple specially designed rewards that
jointly encourage answer accuracy, temporal alignment, and spatial precision.
On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,
raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent
improvements are also observed on a broad range of video understanding
benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond
accuracy, the reasoning traces produced by Open-o3 Video also provide valuable
signals for test-time scaling, enabling confidence-aware verification and
improving answer reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Open-o3 视频：基于明确时空证据的扎根视频推理</div>
<div class="mono" style="margin-top:8px">大多数视频推理模型仅生成文本推理痕迹，而未指明关键证据出现的时间和地点。最近的模型如 OpenAI-o3 引发了对图像证据中心推理的广泛关注，但将这一能力扩展到视频更具挑战性，因为这需要在动态场景中进行联合时间跟踪和空间定位。我们介绍了 Open-o3 视频，一个非代理框架，将明确的时空证据整合到视频推理中，并仔细收集训练数据和设计训练策略以应对上述挑战。该模型在其答案旁边突出显示关键时间戳、对象和边界框，使推理能够扎根于具体的视觉观察。为了实现这一功能，我们首先策划并构建了两个高质量数据集，STGR-CoT-30k 用于 SFT，STGR-RL-36k 用于 RL，具有精心构建的时间和空间注释，因为大多数现有数据集仅提供视频的时间跨度或图像的空间框，缺乏统一的时空监督和推理痕迹。然后，我们采用冷启动强化学习策略，结合多个特别设计的奖励，联合鼓励答案准确性、时间对齐和空间精度。在 V-STAR 基准测试中，Open-o3 视频实现了最先进的性能，在 Qwen2.5-VL 基线上的 mAM 提高了 14.4%，mLGM 提高了 24.2%。在包括 VideoMME、WorldSense、VideoMMMU 和 TVGBench 在内的广泛视频理解基准上也观察到了持续的改进。除了准确性，Open-o3 视频生成的推理痕迹还提供了有价值的信号，用于测试时的扩展，支持基于信心的验证并提高答案的可靠性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance video reasoning models by integrating explicit spatio-temporal evidence, addressing the limitations of existing models that only produce textual reasoning without clear indications of when and where key evidence appears. The authors introduce Open-o3 Video, a non-agent framework that incorporates spatio-temporal evidence into video reasoning, supported by the creation of two high-quality datasets with comprehensive annotations and a cold-start reinforcement learning strategy that emphasizes accuracy, temporal alignment, and spatial precision. Experimental results demonstrate that Open-o3 Video achieves state-of-the-art performance on the V-STAR benchmark, improving mAM by 14.4% and mLGM by 24.2% compared to the Qwen2.5-VL baseline, while also showing consistent improvements across various video understanding benchmarks and providing valuable reasoning traces for enhanced answer reliability.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过整合明确的时空证据来增强视频推理模型，解决现有模型仅生成文本推理而未清晰指示关键证据出现时间和地点的局限性。作者提出了Open-o3 Video，一个非代理框架，将这些证据纳入视频推理，并通过创建两个具有全面时空注释的高质量数据集来支持这一点。实验结果表明，Open-o3 Video在V-STAR基准测试中实现了最先进的性能，相较于Qwen2.5-VL基线，mAM提高了14.4%，mLGM提高了24.2%，同时在各种视频理解基准上也显示出一致的改进，并提供了有价值的推理轨迹以增强答案的可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Conan: Progressive Learning to Reason Like a Detective over Multi-Scale   Visual Evidence</div>
<div class="meta-line">Authors: Kun Ouyang, Yuanxin Liu, Linli Yao, Yishuo Cai, Hao Zhou, Jie Zhou, Fandong Meng, Xu Sun</div>
<div class="meta-line">First: 2025-10-23T12:11:46+00:00 · Latest: 2025-10-23T12:11:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20470v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20470v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video reasoning, which requires multi-step deduction across frames, remains a
major challenge for multimodal large language models (MLLMs). While
reinforcement learning (RL)-based methods enhance reasoning capabilities, they
often rely on text-only chains that yield ungrounded or hallucinated
conclusions. Conversely, frame-retrieval approaches introduce visual grounding
but still struggle with inaccurate evidence localization. To address these
challenges, we present Conan, a framework for evidence-grounded multi-step
video reasoning. Conan identifies contextual and evidence frames, reasons over
cross-frame clues, and adaptively decides when to conclude or explore further.
To achieve this, we (1) construct Conan-91K, a large-scale dataset of
automatically generated reasoning traces that includes frame identification,
evidence reasoning, and action decision, and (2) design a multi-stage
progressive cold-start strategy combined with an
Identification-Reasoning-Action (AIR) RLVR training framework to jointly
enhance multi-step visual reasoning. Extensive experiments on six multi-step
reasoning benchmarks demonstrate that Conan surpasses the baseline
Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving
state-of-the-art performance. Furthermore, Conan generalizes effectively to
long-video understanding tasks, validating its strong scalability and
robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>柯南：在多尺度视觉证据上像侦探一样进行渐进学习推理</div>
<div class="mono" style="margin-top:8px">视频推理需要跨帧的多步骤推导，仍然是多模态大型语言模型（MLLMs）面临的主要挑战。虽然基于强化学习（RL）的方法增强了推理能力，但它们通常依赖于仅文本的链条，导致无根据或虚构的结论。相反，帧检索方法引入了视觉基础，但在证据定位不准确方面仍然存在困难。为了解决这些挑战，我们提出了柯南，一个基于证据的多步骤视频推理框架。柯南识别上下文和证据帧，跨帧线索进行推理，并自适应决定何时结束或进一步探索。为此，我们（1）构建了Conan-91K，一个大规模的数据集，包含自动生成的推理轨迹，包括帧识别、证据推理和行动决策，以及（2）设计了一种多阶段渐进冷启动策略，结合识别-推理-行动（AIR）RLVR训练框架，共同增强多步骤视觉推理。在六个多步骤推理基准上的广泛实验表明，柯南在准确性上平均超过基线Qwen2.5-VL-7B-Instruct超过10%，实现了最先进的性能。此外，柯南在长视频理解任务中有效泛化，验证了其强大的可扩展性和鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of video reasoning, which involves multi-step deduction across frames and is a significant hurdle for multimodal large language models. The authors introduce Conan, a framework designed for evidence-grounded multi-step video reasoning, which identifies contextual and evidence frames while reasoning over cross-frame clues. Through the construction of the Conan-91K dataset and the implementation of a multi-stage progressive cold-start strategy within an Identification-Reasoning-Action RLVR training framework, the experiments show that Conan outperforms the baseline model Qwen2.5-VL-7B-Instruct by over 10% in accuracy across six benchmarks, demonstrating its effectiveness and scalability in long-video understanding tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高多模态大语言模型在视频推理中的能力，这些模型在跨帧的多步骤推理中面临挑战。作者提出了Conan，一个增强证据基础的多步骤视频推理框架，通过识别上下文和证据帧、跨帧线索推理以及自适应决定何时结束或进一步探索来实现这一目标。实验结果表明，Conan在六个多步骤推理基准测试中比基线模型Qwen2.5-VL-7B-Instruct的准确率提高了超过10%，展示了其最先进的性能和对长视频理解任务的有效泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">PreFM: Online Audio-Visual Event Parsing via Predictive Future Modeling</div>
<div class="meta-line">Authors: Xiao Yu, Yan Fang, Xiaojie Jin, Yao Zhao, Yunchao Wei</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-29T06:46:19+00:00 · Latest: 2025-10-23T08:34:50+00:00</div>
<div class="meta-line">Comments: This paper is accepted by 39th Conference on Neural Information
  Processing Systems (NeurIPS 2025)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.23155v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.23155v2">PDF</a> · <a href="https://github.com/XiaoYu-1123/PreFM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio-visual event parsing plays a crucial role in understanding multimodal
video content, but existing methods typically rely on offline processing of
entire videos with huge model sizes, limiting their real-time applicability. We
introduce Online Audio-Visual Event Parsing (On-AVEP), a novel paradigm for
parsing audio, visual, and audio-visual events by sequentially analyzing
incoming video streams. The On-AVEP task necessitates models with two key
capabilities: (1) Accurate online inference, to effectively distinguish events
with unclear and limited context in online settings, and (2) Real-time
efficiency, to balance high performance with computational constraints. To
cultivate these, we propose the Predictive Future Modeling (PreFM) framework
featured by (a) predictive multimodal future modeling to infer and integrate
beneficial future audio-visual cues, thereby enhancing contextual understanding
and (b) modality-agnostic robust representation along with focal temporal
prioritization to improve precision and generalization. Extensive experiments
on the UnAV-100 and LLP datasets show PreFM significantly outperforms
state-of-the-art methods by a large margin with significantly fewer parameters,
offering an insightful approach for real-time multimodal video understanding.
Code is available at https://github.com/XiaoYu-1123/PreFM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PreFM：通过预测未来建模进行在线音视频事件解析</div>
<div class="mono" style="margin-top:8px">音视频事件解析在理解多模态视频内容中起着至关重要的作用，但现有方法通常依赖于对整个视频的离线处理，模型规模庞大，限制了其实时应用性。我们提出了在线音视频事件解析（On-AVEP），这是一种通过顺序分析输入视频流来解析音频、视觉和音视频事件的新范式。On-AVEP任务需要具备两个关键能力的模型：（1）准确的在线推理，以有效区分在线环境中上下文不清晰和有限的事件；（2）实时效率，以在高性能与计算约束之间取得平衡。为了培养这些能力，我们提出了预测未来建模（PreFM）框架，其特点是（a）预测多模态未来建模，以推断和整合有益的未来音视频线索，从而增强上下文理解；（b）模态无关的鲁棒表示以及焦点时间优先级，以提高精度和泛化能力。在UnAV-100和LLP数据集上的大量实验表明，PreFM在参数显著更少的情况下，显著超越了最先进的方法，为实时多模态视频理解提供了深刻的思路。代码可在https://github.com/XiaoYu-1123/PreFM获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the real-time applicability of audio-visual event parsing, which is essential for understanding multimodal video content but is hindered by existing methods that rely on offline processing and large model sizes. The authors introduce the Online Audio-Visual Event Parsing (On-AVEP) paradigm and propose the Predictive Future Modeling (PreFM) framework, which enables sequential analysis of incoming video streams through predictive multimodal future modeling and robust representation techniques. Experimental results on the UnAV-100 and LLP datasets demonstrate that PreFM significantly outperforms state-of-the-art methods with fewer parameters, thus providing a more efficient solution for real-time multimodal video understanding.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提高音视频事件解析的实时适用性，这对于理解多模态视频内容至关重要，但现有方法依赖于离线处理和大型模型，限制了其应用。作者提出了在线音视频事件解析（On-AVEP）范式，并提出了预测未来建模（PreFM）框架，该框架通过预测未来音视频线索和利用鲁棒表示来实现准确的在线推理和实时效率。在UnAV-100和LLP数据集上的实验结果表明，PreFM在参数更少的情况下显著超越了最先进的方法，表明其在实时多模态视频理解中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for   Egocentric Video Question Answering</div>
<div class="meta-line">Authors: Jiayi Zou, Chaofan Chen, Bing-Kun Bao, Changsheng Xu</div>
<div class="meta-line">First: 2025-10-23T07:15:18+00:00 · Latest: 2025-10-23T07:15:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20285v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20285v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Egocentric Video Question Answering (Egocentric VideoQA) plays an important
role in egocentric video understanding, which refers to answering questions
based on first-person videos. Although existing methods have made progress
through the paradigm of pre-training and fine-tuning, they ignore the unique
challenges posed by the first-person perspective, such as understanding
multiple events and recognizing hand-object interactions. To deal with these
challenges, we propose a Dual-Modal Counterfactual Contrastive Construction
(DMC$^3$) framework, which contains an egocentric videoqa baseline, a
counterfactual sample construction module and a counterfactual sample-involved
contrastive optimization. Specifically, We first develop a counterfactual
sample construction module to generate positive and negative samples for
textual and visual modalities through event description paraphrasing and core
interaction mining, respectively. Then, We feed these samples together with the
original samples into the baseline. Finally, in the counterfactual
sample-involved contrastive optimization module, we apply contrastive loss to
minimize the distance between the original sample features and the positive
sample features, while maximizing the distance from the negative samples.
Experiments show that our method achieve 52.51\% and 46.04\% on the
\textit{normal} and \textit{indirect} splits of EgoTaskQA, and 13.2\% on
QAEGO4D, both reaching the state-of-the-art performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DMC$^3$: 双模态反事实对比构建用于自我中心视频问答</div>
<div class="mono" style="margin-top:8px">自我中心视频问答（Egocentric VideoQA）在自我中心视频理解中扮演着重要角色，指的是基于第一人称视频回答问题。尽管现有方法通过预训练和微调的范式取得了进展，但它们忽视了第一人称视角带来的独特挑战，例如理解多个事件和识别手-物体交互。为应对这些挑战，我们提出了双模态反事实对比构建（DMC$^3$）框架，其中包含一个自我中心视频问答基线、一个反事实样本构建模块和一个涉及反事实样本的对比优化。具体而言，我们首先开发了一个反事实样本构建模块，通过事件描述释义和核心交互挖掘分别为文本和视觉模态生成正负样本。然后，我们将这些样本与原始样本一起输入基线。最后，在涉及反事实样本的对比优化模块中，我们应用对比损失来最小化原始样本特征与正样本特征之间的距离，同时最大化与负样本的距离。实验表明，我们的方法在EgoTaskQA的正常和间接分割上分别达到52.51\%和46.04\%，在QAEGO4D上达到13.2\%，均达到了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of Egocentric Video Question Answering (Egocentric VideoQA), which involves interpreting first-person videos to answer questions, a task complicated by the need to understand multiple events and hand-object interactions. The authors propose a Dual-Modal Counterfactual Contrastive Construction (DMC$^3$) framework that integrates a baseline for Egocentric VideoQA with a counterfactual sample construction module and a contrastive optimization approach. Experimental results demonstrate that the DMC$^3$ framework achieves state-of-the-art performance with scores of 52.51% and 46.04% on the normal and indirect splits of EgoTaskQA, respectively, and 13.2% on QAEGO4D.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决第一人称视频问答（Egocentric VideoQA）中独特的挑战，特别是在理解多个事件和手物体交互方面。作者提出了一种双模态反事实对比构建（DMC$^3$）框架，该框架包括Egocentric VideoQA的基线、一个用于构建反事实样本的模块，以及一个结合这些样本的对比优化方法。实验结果表明，DMC$^3$框架在EgoTaskQA的正常和间接分割上分别达到了52.51%和46.04%的最新性能，在QAEGO4D上达到了13.2%。</div>
</details>
</div>
<div class="card">
<div class="title">Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning   Segmentation</div>
<div class="meta-line">Authors: Su Ho Han, Jeongseok Hyun, Pilhyeon Lee, Minho Shim, Dongyoon Wee, Seon Joo Kim</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2025-10-22T13:42:59+00:00 · Latest: 2025-10-22T13:42:59+00:00</div>
<div class="meta-line">Comments: Project page: https://www.jshyun.me/projects/decaf</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19592v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19592v1">PDF</a> · <a href="https://github.com/HYUNJS/DecAF">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://www.jshyun.me/projects/decaf">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) demonstrate strong video
understanding by attending to visual tokens relevant to textual queries. To
directly adapt this for localization in a training-free manner, we cast video
reasoning segmentation as a video QA task and extract attention maps via
rollout mechanism. However, raw attention maps are noisy and poorly aligned
with object regions. We propose Decomposed Attention Fusion (DecAF), which
refines these maps through two mechanisms: (1) contrastive object-background
fusion and (2) complementary video-frame fusion. This method suppresses
irrelevant activations and enhances object-focused cues, enabling direct
conversion of attention maps into coarse segmentation masks. In addition, we
introduce attention-guided SAM2 prompting for obtaining fine-grained masks.
Unlike existing methods that jointly train MLLMs with SAM, our method operates
entirely without retraining. DecAF outperforms training-free methods and
achieves performance comparable to training-based methods on both referring and
reasoning VOS benchmarks. The code will be available at
https://github.com/HYUNJS/DecAF.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无训练视频推理分割中的分解注意力融合</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLMs）通过关注与文本查询相关的视觉标记，展现出强大的视频理解能力。为了以无训练的方式直接适应这一点，我们将视频推理分割视为视频问答任务，并通过回滚机制提取注意力图。然而，原始注意力图噪声较大且与物体区域对齐不佳。我们提出了分解注意力融合（DecAF），通过两种机制来精炼这些图： (1) 对比物体-背景融合和 (2) 互补视频帧融合。该方法抑制无关激活并增强以物体为中心的线索，使注意力图能够直接转换为粗略分割掩码。此外，我们引入了注意力引导的SAM2提示以获取细粒度掩码。与现有的将MLLMs与SAM联合训练的方法不同，我们的方法完全不需要重新训练。DecAF在无训练方法中表现优越，并在引用和推理VOS基准上达到了与基于训练的方法相当的性能。代码将发布在 https://github.com/HYUNJS/DecAF。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for effective video reasoning segmentation using multimodal large language models (MLLMs) without the requirement for retraining. The authors propose a method called Decomposed Attention Fusion (DecAF), which refines noisy attention maps through contrastive object-background fusion and complementary video-frame fusion, allowing for the direct conversion of these maps into segmentation masks. Experimental results indicate that DecAF surpasses existing training-free methods and achieves performance levels comparable to those of training-based approaches on referring and reasoning video object segmentation benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于利用多模态大语言模型（MLLMs）进行有效的视频推理分割，而无需重新训练。作者提出了一种名为分解注意力融合（DecAF）的方法，通过对比对象-背景融合和互补视频帧融合来精炼从回滚机制获得的噪声注意力图。实验结果表明，DecAF超越了现有的无训练方法，并在引用和推理视频对象分割基准测试中达到了与基于训练的方法相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">X-Ego: Acquiring Team-Level Tactical Situational Awareness via   Cross-Egocentric Contrastive Video Representation Learning</div>
<div class="meta-line">Authors: Yunzhe Wang, Soham Hans, Volkan Ustun</div>
<div class="meta-line">First: 2025-10-22T00:48:35+00:00 · Latest: 2025-10-22T00:48:35+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19150v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19150v1">PDF</a> · <a href="https://github.com/HATS-ICT/x-ego">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human team tactics emerge from each player&#x27;s individual perspective and their
ability to anticipate, interpret, and adapt to teammates&#x27; intentions. While
advances in video understanding have improved the modeling of team interactions
in sports, most existing work relies on third-person broadcast views and
overlooks the synchronous, egocentric nature of multi-agent learning. We
introduce X-Ego-CS, a benchmark dataset consisting of 124 hours of gameplay
footage from 45 professional-level matches of the popular e-sports game
Counter-Strike 2, designed to facilitate research on multi-agent
decision-making in complex 3D environments. X-Ego-CS provides cross-egocentric
video streams that synchronously capture all players&#x27; first-person perspectives
along with state-action trajectories. Building on this resource, we propose
Cross-Ego Contrastive Learning (CECL), which aligns teammates&#x27; egocentric
visual streams to foster team-level tactical situational awareness from an
individual&#x27;s perspective. We evaluate CECL on a teammate-opponent location
prediction task, demonstrating its effectiveness in enhancing an agent&#x27;s
ability to infer both teammate and opponent positions from a single
first-person view using state-of-the-art video encoders. Together, X-Ego-CS and
CECL establish a foundation for cross-egocentric multi-agent benchmarking in
esports. More broadly, our work positions gameplay understanding as a testbed
for multi-agent modeling and tactical learning, with implications for
spatiotemporal reasoning and human-AI teaming in both virtual and real-world
domains. Code and dataset are available at https://github.com/HATS-ICT/x-ego.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>X-Ego：通过跨自我中心对比视频表示学习获取团队级战术情境意识</div>
<div class="mono" style="margin-top:8px">人类团队战术源于每个玩家的个体视角以及他们预测、解释和适应队友意图的能力。尽管视频理解的进展改善了体育团队互动的建模，但大多数现有工作依赖于第三人称广播视角，忽视了多智能体学习的同步自我中心特性。我们引入了X-Ego-CS，这是一个基准数据集，包含来自45场职业级比赛的124小时游戏录像，旨在促进复杂3D环境中多智能体决策的研究。X-Ego-CS提供了跨自我中心的视频流，能够同步捕捉所有玩家的第一人称视角以及状态-动作轨迹。在此基础上，我们提出了跨自我对比学习（CECL），该方法将队友的自我中心视觉流对齐，以从个体的视角促进团队级战术情境意识。我们在队友-对手位置预测任务上评估了CECL，证明其在增强智能体从单一第一人称视角推断队友和对手位置的能力方面的有效性，使用了最先进的视频编码器。X-Ego-CS和CECL共同为电子竞技中的跨自我中心多智能体基准测试奠定了基础。更广泛地说，我们的工作将游戏理解定位为多智能体建模和战术学习的试验场，对虚拟和现实领域中的时空推理和人机团队合作具有重要意义。代码和数据集可在https://github.com/HATS-ICT/x-ego获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve team-level tactical situational awareness in multi-agent environments, particularly in esports, by addressing the limitations of existing models that rely on third-person perspectives. The authors introduce a new benchmark dataset, X-Ego-CS, which includes 124 hours of gameplay footage from 45 professional Counter-Strike 2 matches, capturing synchronized first-person perspectives of all players. They propose a method called Cross-Ego Contrastive Learning (CECL), which enhances an agent&#x27;s ability to predict teammate and opponent locations from a single first-person view, demonstrating its effectiveness through improved prediction accuracy using advanced video encoders.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决现有方法依赖第三人称视角的局限性，增强多智能体环境中团队级战术情境意识，特别是在电子竞技中。作者介绍了X-Ego-CS，一个包含45场职业《反恐精英2》比赛的124小时游戏录像的基准数据集，捕捉了所有玩家的同步第一人称视角。作者提出了一种名为交叉自我对比学习（CECL）的方法，以对齐这些自我中心的视频流，并在队友-对手位置预测任务中证明了其有效性，从而提高了智能体从单一第一人称视角推断位置的能力，使用了先进的视频编码器。</div>
</details>
</div>
<div class="card">
<div class="title">LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal   Models on Human-Centric Long-Video Understanding</div>
<div class="meta-line">Authors: ZhaoYang Han, Qihan Lin, Hao Liang, Bowen Chen, Zhou Liu, Wentao Zhang</div>
<div class="meta-line">First: 2025-10-20T08:49:10+00:00 · Latest: 2025-10-21T10:16:53+00:00</div>
<div class="meta-line">Comments: Submitted to ARR Rolling Review</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17305v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.17305v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models&#x27; ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LongInsightBench：评估人本长视频理解的全模态模型的综合基准</div>
<div class="mono" style="margin-top:8px">我们介绍了\textbf{LongInsightBench}，这是第一个旨在评估模型理解长视频能力的基准，重点关注人类语言、视角、动作和其他上下文元素，同时整合\textbf{视觉、音频和文本}模态。我们的基准在三个关键领域表现出色：\textbf{a) 长时长、信息密集的视频：} 我们从开源数据集FineVideo中精心选择了大约1000个视频，基于时长限制和视觉与音频模态的信息密度，重点关注讲座、访谈和视频日志等内容，这些内容包含丰富的语言元素。\textbf{b) 多样且具有挑战性的任务场景：} 我们设计了六个具有挑战性的任务场景，包括事件内和事件间任务。\textbf{c) 严谨且全面的质量保证流程：} 我们开发了一个三步的半自动化数据质量保证流程，以确保合成问题和答案选项的难度和有效性。基于LongInsightBench，我们设计了一系列实验。实验结果表明，全模态模型（OLMs）在需要精确时间定位（T-Loc）和长距离因果推理（CE-Caus）的任务中仍面临挑战。扩展实验揭示了OLMs在多模态融合中的信息损失和处理偏差。我们的数据集和代码可在https://anonymous.4open.science/r/LongInsightBench-910F/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to create a benchmark for evaluating models&#x27; capabilities in understanding long videos, particularly focusing on human-centric elements such as language and actions. The authors developed LongInsightBench, which includes approximately 1,000 carefully selected long-duration, information-dense videos from open-source datasets, and designed six challenging task scenarios to assess model performance. Experimental findings indicate that omni-modal models struggle with tasks requiring precise temporal localization and long-range causal inference, revealing issues related to information loss and processing bias during multi-modal fusion.</div>
<div class="mono" style="margin-top:8px">本研究的动机是创建一个基准，以评估模型在理解长视频方面的能力，特别关注人类中心的元素，如语言、动作和跨多种模态的上下文。作者开发了LongInsightBench，其中包括大约1000个精心挑选的长时长、信息密集的视频，并设计了六个具有挑战性的任务场景来评估模型性能。实验结果表明，通用模态模型在需要精确时间定位和长程因果推理的任务中面临困难，揭示了多模态融合过程中信息损失和处理偏差的问题。</div>
</details>
</div>
<div class="card">
<div class="title">Think With Videos For Agentic Long-Video Understanding</div>
<div class="meta-line">Authors: Huaying Yuan, Zheng Liu, Junjie Zhou, Hongjin Qian, Yan Shu, Nicu Sebe, Ji-Rong Wen, Zhicheng Dou</div>
<div class="meta-line">First: 2025-06-12T15:39:10+00:00 · Latest: 2025-10-21T08:58:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.10821v5">Abs</a> · <a href="http://arxiv.org/pdf/2506.10821v5">PDF</a> · <a href="https://github.com/yhy-2000/VideoDeepResearch">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-video understanding~(LVU) is a challenging problem in computer vision.
Existing methods either downsample frames for single-pass reasoning,
sacrificing fine-grained details, or depend on textual reasoning over
task-agnostic representations, hindering task-specific perception and
exploration. In this paper, we propose VideoExplorer, a framework grounded in
the principle of ``thinking with video&#x27;&#x27;, which naturally intertwines planning,
temporal grounding, and scalable perception into a coherent reasoning process.
Rather than reasoning over a static context, VideoExplorer iteratively
formulates sub-questions, locates relevant moments, and performs task-oriented,
temporally scalable video understanding until reaching the final answer,
enabling faithful, efficient, and interpretable reasoning. To address the lack
of LVU training resources, we construct a long-video reasoning dataset using
difficulty-adaptive sampling to ensure high-quality trajectories on complex
tasks. Building on this dataset, we design a two-stage training pipeline:
supervised trajectory initialization followed by trajectory-level preference
optimization, encouraging adaptive temporal grounding and iterative information
integration guided by downstream rewards. Extensive evaluations on popular
long-video understanding and reasoning benchmarks demonstrate VideoExplorer&#x27;s
significant advantage over existing baselines, highlighting its robustness,
adaptability, and efficiency. Our code is made publicly available in this
repository(https://github.com/yhy-2000/VideoDeepResearch).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过视频思考实现代理性长视频理解</div>
<div class="mono" style="margin-top:8px">长视频理解（LVU）是计算机视觉中的一个挑战性问题。现有方法要么通过下采样帧进行单次推理，牺牲细粒度细节，要么依赖于对任务无关表示的文本推理，阻碍了任务特定的感知和探索。本文提出了VideoExplorer，一个基于“通过视频思考”原则的框架，自然地将规划、时间基础和可扩展感知交织成一个连贯的推理过程。VideoExplorer不是在静态上下文中推理，而是迭代地制定子问题，定位相关时刻，并执行面向任务的、时间可扩展的视频理解，直到达到最终答案，从而实现忠实、高效和可解释的推理。为了解决LVU训练资源不足的问题，我们构建了一个长视频推理数据集，采用难度自适应采样，以确保在复杂任务上获得高质量轨迹。在此数据集的基础上，我们设计了一个两阶段的训练流程：监督轨迹初始化，随后进行轨迹级偏好优化，鼓励自适应时间基础和由下游奖励引导的迭代信息整合。在流行的长视频理解和推理基准上的广泛评估表明，VideoExplorer在现有基线中具有显著优势，突显了其鲁棒性、适应性和效率。我们的代码已在此仓库公开（https://github.com/yhy-2000/VideoDeepResearch）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges of long-video understanding (LVU) in computer vision, where existing methods either compromise on detail or rely on generic textual reasoning. The authors propose VideoExplorer, a framework that integrates planning, temporal grounding, and scalable perception into a coherent reasoning process by iteratively formulating sub-questions and locating relevant moments for task-oriented understanding. Experimental results show that VideoExplorer outperforms existing baselines on various long-video understanding benchmarks, demonstrating its robustness, adaptability, and efficiency in handling complex tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决计算机视觉中长视频理解（LVU）的挑战，现有方法要么损失细粒度细节，要么依赖于与任务无关的表示。作者提出了VideoExplorer框架，该框架将规划、时间定位和可扩展感知整合为一个连贯的推理过程，通过迭代地形成子问题和定位相关时刻来实现任务导向的视频理解。实验结果表明，VideoExplorer在流行的长视频理解基准上显著优于现有基线，展示了其在处理复杂任务时的鲁棒性、适应性和效率。</div>
</details>
</div>
<div class="card">
<div class="title">StreamingTOM: Streaming Token Compression for Efficient Video   Understanding</div>
<div class="meta-line">Authors: Xueyi Chen, Keda Tao, Kele Shao, Huan Wang</div>
<div class="meta-line">First: 2025-10-21T03:39:41+00:00 · Latest: 2025-10-21T03:39:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18269v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18269v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unlike offline processing, streaming video vision-language models face two
fundamental constraints: causality and accumulation. Causality prevents access
to future frames that offline methods exploit, while accumulation causes tokens
to grow unbounded, creating efficiency bottlenecks. However, existing
approaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill
unchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage
framework that addresses both pre-LLM and post-LLM bottlenecks with predictable
latency. Causal Temporal Reduction imposes a fixed per-frame budget and selects
tokens based on adjacent-frame changes and token saliency, drastically reducing
per-frame prefill cost by processing only a compact subset of visual tokens per
frame instead of all visual tokens. Online Quantized Memory stores tokens in
4-bit format, retrieves relevant groups on demand, and dequantizes them,
keeping the active kv-cache bounded regardless of stream length. Experiments
demonstrate our method achieves $15.7\times$ kv-cache compression, $1.2\times$
lower peak memory and $2\times$ faster TTFT compared to prior SOTA.
StreamingTOM maintains state-of-the-art accuracy among training-free methods
with an average of $63.8\%$ on offline benchmarks and $55.8\%/3.7$ on RVS.
These results highlight the practical benefits of our two-stage approach for
efficient streaming video understanding with bounded growth.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StreamingTOM：高效视频理解的流式令牌压缩</div>
<div class="mono" style="margin-top:8px">与离线处理不同，流式视频视觉语言模型面临两个基本限制：因果性和累积性。因果性阻止访问离线方法利用的未来帧，而累积性导致令牌无限增长，造成效率瓶颈。然而，现有方法仅调节后LLM的kv-cache，未改变成本高昂的前LLM预填充。我们提出了StreamingTOM，一个无训练、即插即用的两阶段框架，解决前LLM和后LLM的瓶颈，具有可预测的延迟。因果时间减少施加固定的每帧预算，并根据相邻帧变化和令牌显著性选择令牌，通过每帧仅处理紧凑的视觉令牌子集，显著降低每帧预填充成本。在线量化内存以4位格式存储令牌，根据需要检索相关组并进行反量化，无论流长度如何，保持活动kv-cache的界限。实验表明，我们的方法实现了$15.7\times$的kv-cache压缩，$1.2\times$的峰值内存降低和$2\times$的TTFT加速，相较于之前的SOTA。StreamingTOM在无训练方法中保持了最先进的准确性，在离线基准上平均为$63.8\%$，在RVS上为$55.8\%/3.7$。这些结果突显了我们两阶段方法在高效流式视频理解中的实际优势，且增长受限。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenges faced by streaming video vision-language models, specifically the constraints of causality and accumulation that hinder efficiency. The authors propose StreamingTOM, a two-stage framework that operates without the need for training, which effectively manages both pre-LLM and post-LLM bottlenecks by implementing Causal Temporal Reduction to limit per-frame token processing and utilizing Online Quantized Memory for efficient token storage and retrieval. Experimental results show that StreamingTOM achieves a 15.7 times reduction in kv-cache size, 1.2 times lower peak memory usage, and 2 times faster time-to-first-token (TTFT) compared to state-of-the-art methods, while maintaining competitive accuracy with an average of 63.8% on offline benchmarks and 55.8%/3.7 on RVS.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决流媒体视频视觉语言模型面临的因果性和累积性约束，这些约束阻碍了效率。作者提出了StreamingTOM，这是一个无需训练的两阶段框架，有效管理了LLM前后瓶颈。实验结果表明，StreamingTOM在kv-cache大小上实现了15.7倍的减少，峰值内存使用降低了1.2倍，首次令牌时间（TTFT）速度提高了两倍，同时在离线基准测试中保持了63.8%的竞争性准确率，在RVS上为55.8%。</div>
</details>
</div>
<div class="card">
<div class="title">MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating   Multimodal LLMs in Multi-Turn Dialogues</div>
<div class="meta-line">Authors: Yaning Pan, Zekun Wang, Qianqian Xie, Yongqian Wen, Yuanxing Zhang, Guohui Zhang, Haoxuan Hu, Zhiyu Pan, Yibing Huang, Zhidong Gan, Yonghong Lin, An Ping, Tianhao Peng, Jiaheng Liu</div>
<div class="meta-line">First: 2025-10-20T16:38:40+00:00 · Latest: 2025-10-20T16:38:40+00:00</div>
<div class="meta-line">Comments: Project Website: https://github.com/NJU-LINK/MT-Video-Bench</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17722v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.17722v1">PDF</a> · <a href="https://github.com/NJU-LINK/MT-Video-Bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI&#x27;s ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MT-Video-Bench：用于评估多模态大语言模型在多轮对话中的整体视频理解基准</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）的近期发展显著提升了人工智能理解视觉模态的能力。然而，现有的评估基准仍然局限于单轮问答，忽视了现实场景中多轮对话的复杂性。为填补这一空白，我们推出了MT-Video-Bench，这是一个用于评估MLLMs在多轮对话中的整体视频理解基准。具体而言，我们的MT-Video-Bench主要评估六项核心能力，专注于感知性和互动性，涵盖了来自不同领域的987个精心策划的多轮对话。这些能力与现实应用严格对齐，例如互动体育分析和基于视频的多轮智能辅导。通过MT-Video-Bench，我们广泛评估了各种最先进的开源和闭源MLLMs，揭示了它们在处理多轮视频对话时的显著性能差异和局限性。该基准将公开发布，以促进未来的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of existing evaluation benchmarks for Multimodal Large Language Models (MLLMs), which primarily focus on single-turn question answering and do not reflect the complexities of multi-turn dialogues. The authors introduce MT-Video-Bench, a comprehensive benchmark designed to evaluate MLLMs in multi-turn dialogues, assessing six core competencies related to perceptivity and interactivity through 987 curated dialogues from various domains. The experimental results demonstrate significant performance discrepancies among state-of-the-art MLLMs, highlighting their limitations in effectively managing multi-turn video dialogues, and the benchmark aims to support future research in this area.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决现有多模态大型语言模型（MLLM）评估基准的局限性，这些基准主要集中在单轮问答上，无法充分反映多轮对话的复杂性。作者提出了MT-Video-Bench，这是一个综合基准，旨在通过评估与感知和互动相关的六项核心能力，来评估MLLM在多轮对话中的表现，利用来自不同领域的987个精心策划的对话。实验结果表明，不同MLLM之间存在显著的性能差异，突显了它们在有效处理多轮视频对话方面的局限性，该基准旨在支持该领域的未来研究。</div>
</details>
</div>
<div class="card">
<div class="title">ActAlign: Zero-Shot Fine-Grained Video Classification via   Language-Guided Sequence Alignment</div>
<div class="meta-line">Authors: Amir Aghdam, Vincent Tao Hu, Björn Ommer</div>
<div class="meta-line">First: 2025-06-28T17:57:58+00:00 · Latest: 2025-10-19T23:08:33+00:00</div>
<div class="meta-line">Comments: Accepted to TMLR 2025 - Project page:
  https://amir-aghdam.github.io/act-align/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.22967v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.22967v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://amir-aghdam.github.io/act-align/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We address the task of zero-shot video classification for extremely
fine-grained actions (e.g., Windmill Dunk in basketball), where no video
examples or temporal annotations are available for unseen classes. While
image-language models (e.g., CLIP, SigLIP) show strong open-set recognition,
they lack temporal modeling needed for video understanding. We propose
ActAlign, a truly zero-shot, training-free method that formulates video
classification as a sequence alignment problem, preserving the generalization
strength of pretrained image-language models. For each class, a large language
model (LLM) generates an ordered sequence of sub-actions, which we align with
video frames using Dynamic Time Warping (DTW) in a shared embedding space.
Without any video-text supervision or fine-tuning, ActAlign achieves 30.5%
accuracy on ActionAtlas--the most diverse benchmark of fine-grained actions
across multiple sports--where human performance is only 61.6%. ActAlign
outperforms billion-parameter video-language models while using 8x fewer
parameters. Our approach is model-agnostic and domain-general, demonstrating
that structured language priors combined with classical alignment methods can
unlock the open-set recognition potential of image-language models for
fine-grained video understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ActAlign：通过语言引导的序列对齐实现零样本细粒度视频分类</div>
<div class="mono" style="margin-top:8px">我们解决了极细粒度动作（例如篮球中的风车扣篮）的零样本视频分类任务，其中没有可用于未见类别的视频示例或时间注释。尽管图像-语言模型（例如CLIP、SigLIP）在开放集识别中表现出色，但它们缺乏视频理解所需的时间建模。我们提出了ActAlign，这是一种真正的零样本、无训练的方法，将视频分类公式化为序列对齐问题，保留了预训练图像-语言模型的泛化能力。对于每个类别，一个大型语言模型（LLM）生成一个有序的子动作序列，我们使用动态时间规整（DTW）在共享嵌入空间中与视频帧对齐。在没有任何视频-文本监督或微调的情况下，ActAlign在ActionAtlas上实现了30.5%的准确率——这是跨多个运动的细粒度动作的最具多样性的基准，而人类表现仅为61.6%。ActAlign在使用8倍更少参数的情况下超越了十亿参数的视频-语言模型。我们的方法是模型无关和领域通用的，证明了结构化语言先验与经典对齐方法相结合可以释放图像-语言模型在细粒度视频理解中的开放集识别潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to tackle the challenge of zero-shot video classification for fine-grained actions, where no training examples or temporal annotations exist for unseen classes. The authors propose ActAlign, a novel method that treats video classification as a sequence alignment problem, leveraging the strengths of pretrained image-language models while incorporating temporal modeling through Dynamic Time Warping (DTW). Experimental results show that ActAlign achieves an accuracy of 30.5% on the ActionAtlas benchmark for fine-grained actions, significantly outperforming larger video-language models and demonstrating the effectiveness of combining structured language priors with classical alignment techniques for video understanding.</div>
<div class="mono" style="margin-top:8px">本研究解决了零样本视频分类中细粒度动作的挑战，在未见类别中没有示例或注释。作者提出了ActAlign方法，将视频分类视为序列对齐问题，利用大型语言模型生成有序的子动作序列，并通过动态时间规整在共享嵌入空间中与视频帧对齐。ActAlign在细粒度动作的ActionAtlas基准测试中取得了30.5%的准确率，显著优于现有的十亿参数视频语言模型，同时使用的参数少了八倍，从而证明了将结构化语言先验与经典对齐技术结合在视频理解中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Xiaoice: Training-Free Video Understanding via Self-Supervised   Spatio-Temporal Clustering of Semantic Features</div>
<div class="meta-line">Authors: Shihao Ji, Zihui Song</div>
<div class="meta-line">First: 2025-10-19T10:13:34+00:00 · Latest: 2025-10-19T10:13:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.16781v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.16781v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM&#x27;s generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>小冰：通过自监督时空聚类语义特征实现无训练视频理解</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（VLM）在静态图像上的显著零-shot推理能力尚未完全转化到视频领域。传统的视频理解模型通常依赖于在标注数据集上进行广泛的、特定任务的训练，这一过程既昂贵又有限于可扩展性。本文提出了一种新颖的无训练视频理解框架，通过将预训练VLM的丰富语义先验与经典机器学习算法相结合，避免了端到端训练。我们的核心思想是将视频理解重新构建为高维语义特征空间中的自监督时空聚类问题。所提出的流程首先使用预训练VLM的冻结视觉编码器将视频流转换为语义特征轨迹。随后，我们采用一种稳健的机器学习技术——核时间分割（KTS），将连续特征流划分为离散的、语义一致的事件片段。这些片段随后经过无监督的基于密度的聚类，以识别视频中的重复宏观场景和主题。通过从每个发现的聚类中选择代表性关键帧，并利用VLM的生成能力进行文本描述，我们的框架自动生成视频内容的结构化多模态摘要。这种方法为零-shot自动结构分析视频内容提供了一条有效、可解释且与模型无关的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of translating the zero-shot reasoning capabilities of large-scale Visual Language Models (VLMs) to video understanding, which traditionally relies on extensive training with annotated datasets. The authors propose a training-free framework that combines the semantic knowledge of pre-trained VLMs with classic machine learning techniques, specifically framing video understanding as a self-supervised spatio-temporal clustering problem. The experimental results demonstrate that the method effectively partitions video streams into semantically coherent segments and generates structured multi-modal summaries, showcasing its potential for automated video analysis without the need for extensive training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决传统视频理解模型需要在注释数据集上进行大量训练的局限性，这一过程既昂贵又不具可扩展性。作者提出了一种无训练框架，利用预训练的视觉语言模型（VLM）和经典机器学习技术，通过自监督的时空聚类语义特征来进行视频理解。实验结果表明，该框架有效地将视频流转换为语义特征轨迹，使用核时间分割将其划分为连贯的事件段，并在无需特定任务训练的情况下生成视频内容的结构化多模态摘要。</div>
</details>
</div>
<div class="card">
<div class="title">StretchySnake: Flexible SSM Training Unlocks Action Recognition Across   Spatio-Temporal Scales</div>
<div class="meta-line">Authors: Nyle Siddiqui, Rohit Gupta, Sirnam Swetha, Mubarak Shah</div>
<div class="meta-line">First: 2025-10-17T20:43:54+00:00 · Latest: 2025-10-17T20:43:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.16209v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.16209v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model&#x27;s ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StretchySnake：灵活的SSM训练解锁跨时空尺度的动作识别</div>
<div class="mono" style="margin-top:8px">状态空间模型（SSM）已成为各种任务中对变换器的竞争性替代方案。它们的线性复杂性和隐藏状态的递归使其在建模长序列时特别有吸引力，而注意力机制的计算成本则呈二次增长。然而，目前的视频理解训练方法主要针对变换器，未能充分利用SSM的独特属性。例如，视频模型通常在固定分辨率和视频长度下进行训练，以平衡注意力成本的二次扩展与性能。因此，这些模型在评估未在训练期间见过的空间和时间分辨率的视频时，性能会下降；我们称之为时空不灵活性。在动作识别的背景下，这严重限制了模型在短视频和长视频中的性能保持能力。因此，我们提出了一种灵活的训练方法，利用并改善SSM的固有适应性。我们的方法在训练期间以不同的时间和空间分辨率对视频进行采样，并动态插值模型权重，以适应任何时空尺度。这使我们的SSM（称为StretchySnake）具备时空灵活性，能够无缝处理从短小精悍的片段到长时间复杂活动的视频。我们介绍并比较了五种不同的灵活训练变体，并确定了视频SSM的最有效策略。在短动作（UCF-101，HMDB-51）和长动作（COIN，Breakfast）基准测试中，StretchySnake的表现比变换器和SSM基线高出多达28%，对细粒度动作（SSV2，Diving-48）具有强大的适应性。因此，我们的方法提供了一种简单的训练方案，使视频SSM在各种动作识别场景中更加稳健、与分辨率无关且高效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of current training methods for state space models (SSMs) in action recognition, particularly their spatio-temporal inflexibility when dealing with videos of varying resolutions. The authors propose a flexible training method for SSMs, named StretchySnake, which samples videos at different temporal and spatial resolutions and dynamically interpolates model weights to enhance adaptability. Experimental results demonstrate that StretchySnake outperforms both transformer and SSM baselines by up to 28% on various benchmarks, showcasing its effectiveness in handling both short and long action videos with improved robustness and efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决当前状态空间模型（SSMs）在动作识别中的训练方法的局限性，特别是在处理不同分辨率视频时的时空不灵活性。作者提出了一种灵活的训练方法，该方法在不同的时间和空间分辨率下对视频进行采样，并动态插值模型权重，从而得到了名为StretchySnake的模型。实验结果表明，StretchySnake在多个基准测试中显著优于变压器和SSM基线，提升幅度可达28%，展示了其在短期和长期动作识别任务中的强适应性。</div>
</details>
</div>
<div class="card">
<div class="title">SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal   Video Action Grounding</div>
<div class="meta-line">Authors: Tanveer Hannan, Shuaicong Wu, Mark Weber, Suprosanna Shit, Jindong Gu, Rajat Koner, Aljoša Ošep, Laura Leal-Taixé, Thomas Seidl</div>
<div class="meta-line">First: 2025-10-14T22:10:49+00:00 · Latest: 2025-10-16T15:16:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13016v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.13016v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding fine-grained actions and accurately localizing their
corresponding actors in space and time are fundamental capabilities for
advancing next-generation AI systems, including embodied agents, autonomous
platforms, and human-AI interaction frameworks. Despite recent progress in
video understanding, existing methods predominantly address either
coarse-grained action recognition or generic object tracking, thereby
overlooking the challenge of jointly detecting and tracking multiple objects
according to their actions while grounding them temporally. To address this
gap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel task
that requires models to simultaneously detect, track, and temporally localize
all referent objects in videos based on natural language descriptions of their
actions. To support this task, we construct SVAG-Bench, a large-scale benchmark
comprising 688 videos, 19,590 annotated records, and 903 unique verbs, covering
a diverse range of objects, actions, and real-world scenes. We further propose
SVAGFormer, a baseline framework that adapts state of the art vision language
models for joint spatial and temporal grounding, and introduce SVAGEval, a
standardized evaluation toolkit for fair and reproducible benchmarking.
Empirical results show that existing models perform poorly on SVAG,
particularly in dense or complex scenes, underscoring the need for more
advanced reasoning over fine-grained object-action interactions in long videos.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SVAG-Bench：多实例时空视频动作定位的大规模基准</div>
<div class="mono" style="margin-top:8px">理解细粒度动作并准确定位其对应的行为者在时空中的位置是推动下一代人工智能系统（包括具身代理、自主平台和人机交互框架）的基本能力。尽管视频理解方面取得了近期进展，现有方法主要解决粗粒度动作识别或通用物体跟踪的问题，从而忽视了根据动作同时检测和跟踪多个物体的挑战，同时进行时间定位。为了解决这一空白，我们引入了时空视频动作定位（SVAG），这是一项新任务，要求模型根据自然语言描述同时检测、跟踪和时间定位视频中的所有参考物体。为支持这一任务，我们构建了SVAG-Bench，这是一个大规模基准，包含688个视频、19,590条注释记录和903个独特动词，涵盖了多样的物体、动作和现实场景。我们进一步提出了SVAGFormer，一个基线框架，适应最先进的视觉语言模型以实现空间和时间的联合定位，并引入SVAGEval，一个标准化评估工具包，用于公平和可重复的基准测试。实证结果表明，现有模型在SVAG上的表现较差，特别是在密集或复杂场景中，强调了在长视频中对细粒度物体-动作交互进行更高级推理的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to enhance AI systems&#x27; capabilities in understanding fine-grained actions and accurately localizing actors in space and time, which is crucial for applications like embodied agents and human-AI interactions. The authors introduce a novel task called Spatio-temporal Video Action Grounding (SVAG) and construct a large-scale benchmark, SVAG-Bench, consisting of 688 videos and over 19,000 annotated records to facilitate this task. Experimental results reveal that existing models struggle with SVAG, especially in dense or complex scenes, highlighting the necessity for improved reasoning in fine-grained object-action interactions within lengthy videos.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于提升人工智能系统理解细粒度动作和准确定位空间与时间中参与者的能力，这对嵌入式代理和人机交互等应用至关重要。作者提出了一项新任务，称为时空视频动作定位（SVAG），并构建了一个大规模基准SVAG-Bench，包含688个视频、19,590个注释记录和903个独特动词，以支持该任务。实验结果表明，现有模型在SVAG任务上表现不佳，尤其是在密集或复杂场景中，突显了在长视频中改善细粒度物体-动作交互推理的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning</div>
<div class="meta-line">Authors: Jinglei Zhang, Yuanfan Guo, Rolandos Alexandros Potamias, Jiankang Deng, Hang Xu, Chao Ma</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-10-16T13:29:02+00:00 · Latest: 2025-10-16T13:29:02+00:00</div>
<div class="meta-line">Comments: Accepted by ICCV 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14672v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14672v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vtimecot.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, video question answering based on multimodal large language
models (MLLM) has garnered considerable attention, due to the benefits from the
substantial advancements in LLMs. However, these models have a notable
deficiency in the domains of video temporal grounding and reasoning, posing
challenges to the development of effective real-world video understanding
systems. Inspired by how humans use video players to interact with the progress
bar for video comprehension, we introduce VTimeCoT, a simple yet effective
training-free framework, designed for high-performance video grounding and
reasoning. The proposed framework incorporates two novel visual tools of the
progress bar: a plug-and-play progress bar integration tool and a
high-efficiency highlighting tool. In addition, to address the limitations of
conventional text-based chain-of-thought (CoT) approaches, we introduce a
visuotemporal CoT process that integrates cross-modality reasoning across both
video and text. Our approach demonstrates significant performance improvements
on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and
reasoning-based question answering. Finally, we showcase that the proposed
framework achieves a compositional and interpretable reasoning process. Project
page: https://vtimecot.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VTimeCoT：通过绘图进行视频时间定位和推理</div>
<div class="mono" style="margin-top:8px">近年来，基于多模态大型语言模型（MLLM）的视频问答引起了相当大的关注，这得益于LLM的重大进展。然而，这些模型在视频时间定位和推理领域存在显著不足，给有效的现实世界视频理解系统的发展带来了挑战。受到人类如何使用视频播放器与进度条互动以理解视频的启发，我们提出了VTimeCoT，这是一个简单而有效的无训练框架，旨在实现高性能的视频定位和推理。该框架结合了两种新颖的进度条视觉工具：即插即用的进度条集成工具和高效的高亮工具。此外，为了解决传统基于文本的思维链（CoT）方法的局限性，我们引入了一种视时序CoT过程，整合了视频和文本之间的跨模态推理。我们的方法在视频时间定位和基于推理的问题回答任务中，在Qwen2VL-7B和GPT4o基线上的性能显著提升。最后，我们展示了所提出的框架实现了组合性和可解释的推理过程。项目页面：https://vtimecot.github.io</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the limitations of multimodal large language models (MLLM) in video temporal grounding and reasoning, which are critical for effective video understanding systems. The authors propose VTimeCoT, a training-free framework that utilizes two innovative visual tools inspired by video player progress bars: a plug-and-play integration tool and a highlighting tool, alongside a visuotemporal chain-of-thought process that enhances cross-modality reasoning. Experimental results indicate that VTimeCoT significantly improves performance on video temporal grounding and reasoning tasks when compared to Qwen2VL-7B and GPT4o baselines, while also providing a compositional and interpretable reasoning process.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多模态大语言模型（MLLM）在视频时间定位和推理方面的局限性，这对有效的视频理解系统至关重要。作者提出了VTimeCoT，这是一种无训练的框架，利用受人类与视频播放器交互启发的新视觉工具，具体包括进度条集成工具和高效高亮工具，以及增强跨模态推理的视时链式思维过程。实验结果表明，VTimeCoT在视频时间定位和推理任务上显著提高了与Qwen2VL-7B和GPT4o基线的性能，同时还提供了可组合和可解释的推理过程。</div>
</details>
</div>
<div class="card">
<div class="title">Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long   Video Understanding</div>
<div class="meta-line">Authors: Xiaoqian Shen, Wenxuan Zhang, Jun Chen, Mohamed Elhoseiny</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-10-15T19:14:58+00:00 · Latest: 2025-10-15T19:14:58+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 (Spotlight). Webpage at
  https://xiaoqian-shen.github.io/Vgent</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14032v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14032v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xiaoqian-shen.github.io/Vgent">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and reasoning over long videos pose significant challenges for
large video language models (LVLMs) due to the difficulty in processing
intensive video tokens beyond context window and retaining long-term sequential
information. Retrieval-Augmented Generation (RAG) has demonstrated
effectiveness in processing long context for Large Language Models (LLMs);
however, applying RAG to long video faces challenges such as disrupted temporal
dependencies and inclusion of irrelevant information that can hinder accurate
reasoning. To address these limitations, we propose Vgent, a novel graph-based
retrieval-reasoning-augmented generation framework to enhance LVLMs for long
video understanding. Our approach introduces two key innovations: (i) It
represents videos by structured graphs with semantic relationships across video
clips preserved to improve retrieval effectiveness. (ii) It introduces an
intermediate reasoning step to mitigate the reasoning limitation of LVLMs,
which leverages structured verification to reduce retrieval noise and
facilitate the explicit aggregation of relevant information across clips,
resulting in more accurate and context-aware responses. We comprehensively
evaluate our framework with various open-source LVLMs on three long-video
understanding benchmarks. Our approach yielded an overall performance
improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed
state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available
at https://xiaoqian-shen.github.io/Vgent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vgent：基于图的检索推理增强生成用于长视频理解</div>
<div class="mono" style="margin-top:8px">理解和推理长视频对大型视频语言模型（LVLMs）提出了重大挑战，因为处理超出上下文窗口的密集视频标记和保留长期序列信息的难度。检索增强生成（RAG）已在处理大型语言模型（LLMs）的长上下文方面显示出有效性；然而，将RAG应用于长视频面临着时间依赖性中断和包含无关信息等挑战，这可能妨碍准确推理。为了解决这些局限性，我们提出了Vgent，一种新颖的基于图的检索推理增强生成框架，以增强LVLMs的长视频理解能力。我们的方法引入了两个关键创新：（i）通过结构化图表示视频，保留视频片段之间的语义关系，以提高检索效果。（ii）引入中间推理步骤，以减轻LVLMs的推理限制，利用结构化验证减少检索噪声，并促进跨片段相关信息的显式聚合，从而产生更准确和上下文感知的响应。我们在三个长视频理解基准上全面评估了我们的框架，结果显示我们的方案在MLVU上相较于基础模型整体性能提升了$3.0\%\sim 5.4\%$，并且超越了最先进的视频RAG方法$8.6\%$。我们的代码公开可用，网址为https://xiaoqian-shen.github.io/Vgent。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges faced by large video language models (LVLMs) in understanding and reasoning over long videos, particularly due to limitations in processing extensive video tokens and retaining sequential information. The authors propose Vgent, a graph-based retrieval-reasoning-augmented generation framework that enhances LVLMs by representing videos as structured graphs to improve retrieval effectiveness and introducing an intermediate reasoning step to reduce retrieval noise. Experimental results demonstrate that Vgent improves overall performance by 3.0% to 5.4% on the MLVU benchmark compared to base models and surpasses state-of-the-art video retrieval-augmented generation methods by 8.6%.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型视频语言模型（LVLMs）在理解和推理长视频时面临的挑战，特别是在处理大量视频标记和保持长期序列信息方面的局限性。作者提出了Vgent，一种基于图的检索-推理-增强生成框架，通过将视频表示为结构化图来保留语义关系，并引入中间推理步骤以提高检索准确性和减少噪声。实验结果表明，Vgent在MLVU基准上相比基础模型提高了3.0%至5.4%的性能，并且超越了最先进的视频检索增强生成方法8.6%。</div>
</details>
</div>
<div class="card">
<div class="title">InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn   Dialogue</div>
<div class="meta-line">Authors: Wenwen Tong, Hewei Guo, Dongchuan Ran, Jiangnan Chen, Jiefan Lu, Kaibin Wang, Keqiang Li, Xiaoxu Zhu, Jiakui Li, Kehan Li, Xueheng Li, Lumin Li, Chenxu Guo, Jiasheng Zhou, Jiandong Chen, Xianye Wu, Jiahao Wang, Silei Wu, Lei Chen, Hanming Deng, Yuxuan Song, Dinghao Zhou, Guiping Zhong, Ken Zheng, Shiyin Kang, Lewei Lu</div>
<div class="meta-line">First: 2025-10-15T16:52:48+00:00 · Latest: 2025-10-15T16:52:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13747v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13747v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce InteractiveOmni, a unified and open-source omni-modal large
language model for audio-visual multi-turn interaction, ranging from 4B to 8B
parameters, designed to lead the field of lightweight models by offering
comprehensive omni-modal understanding and speech generation capabilities. To
achieve this, we integrate the vision encoder, audio encoder, large language
model, and speech decoder into a unified model for understanding and generation
tasks. We design a multi-stage training strategy to ensure robust cross-modal
capabilities, including pre-training for omni-modal understanding, followed by
post-training with speech conversation and audio-visual interaction. To enable
human-like long-term conversational ability, we meticulously curate a
multi-turn training dataset that enhances the model&#x27;s ability to handle complex
and multi-turn interactions. To effectively evaluate the multi-turn memory and
speech interaction capabilities, we construct the multi-modal multi-turn memory
benchmark and the multi-turn speech interaction benchmark. Experiments
demonstrate that InteractiveOmni significantly outperforms leading open-source
models and provides a more intelligent multi-turn audio-visual experience,
particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B
is comparable to the much larger model like Qwen2.5-Omni-7B on general
benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B
while utilizing only 50% of the model size. Achieving state-of-the-art results
against similarly sized models across image, audio, video understanding, and
speech generation tasks, InteractiveOmni is an accessible, open-source
foundation for next-generation intelligent interactive systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InteractiveOmni：统一的音视频多模态模型用于多轮对话</div>
<div class="mono" style="margin-top:8px">我们介绍了InteractiveOmni，一个统一的开源音视频多模态大型语言模型，参数范围从40亿到80亿，旨在通过提供全面的多模态理解和语音生成能力，引领轻量级模型领域。为此，我们将视觉编码器、音频编码器、大型语言模型和语音解码器整合为一个统一模型，用于理解和生成任务。我们设计了多阶段训练策略，以确保强大的跨模态能力，包括针对多模态理解的预训练，随后进行语音对话和音视频交互的后训练。为了实现类人长期对话能力，我们精心策划了一个多轮训练数据集，以增强模型处理复杂和多轮交互的能力。为了有效评估多轮记忆和语音交互能力，我们构建了多模态多轮记忆基准和多轮语音交互基准。实验表明，InteractiveOmni显著优于领先的开源模型，提供更智能的多轮音视频体验，特别是在其长期记忆能力方面。值得注意的是，InteractiveOmni-4B在一般基准上与更大模型如Qwen2.5-Omni-7B相当，并且在仅使用50%模型大小的情况下，能够保留InteractiveOmni-8B的97%性能。在图像、音频、视频理解和语音生成任务中，InteractiveOmni在同等大小模型中取得了最先进的结果，是下一代智能交互系统的可访问开源基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind the development of InteractiveOmni is to create a unified omni-modal large language model that enhances audio-visual multi-turn interactions while maintaining lightweight characteristics. The method involves integrating various components such as a vision encoder, audio encoder, large language model, and speech decoder into a single framework, supported by a multi-stage training strategy that includes pre-training for omni-modal understanding and post-training for speech conversation. Experimental results indicate that InteractiveOmni significantly outperforms existing open-source models, demonstrating superior long-term memory capabilities and achieving state-of-the-art performance in image, audio, video understanding, and speech generation tasks, with the smaller InteractiveOmni-4B model showing comparable results to larger models like Qwen2.5-Omni-7B.</div>
<div class="mono" style="margin-top:8px">InteractiveOmni的开发动机是创建一个轻量级的统一全模态大语言模型，以在音频-视觉多轮对话交互中表现出色。该方法将视觉和音频编码器、大语言模型和语音解码器等多个组件集成到一个框架中，并通过多阶段训练策略增强跨模态能力。实验结果表明，InteractiveOmni在多轮交互中显著超越现有的开源模型，展现出强大的长期记忆能力，并在保持较小体积的同时，与更大模型的性能相当。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251102_1232.html">20251102_1232</a>
<a href="archive/20251102_1220.html">20251102_1220</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
