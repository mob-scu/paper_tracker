<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-02 13:56</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251102_1356</div>
    <div class="row"><div class="card">
<div class="title">ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for   Audio-Language Models</div>
<div class="meta-line">Authors: Weifei Jin, Yuxin Cao, Junjie Su, Minhui Xue, Jie Hao, Ke Xu, Jin Song Dong, Derui Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-30T03:19:59+00:00 · Latest: 2025-10-30T03:19:59+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.26096v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.26096v1">PDF</a> · <a href="https://github.com/WeifeiJin/ALMGuard">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Audio-Language Models (ALMs) have significantly improved
multimodal understanding capabilities. However, the introduction of the audio
modality also brings new and unique vulnerability vectors. Previous studies
have proposed jailbreak attacks that specifically target ALMs, revealing that
defenses directly transferred from traditional audio adversarial attacks or
text-based Large Language Model (LLM) jailbreaks are largely ineffective
against these ALM-specific threats. To address this issue, we propose ALMGuard,
the first defense framework tailored to ALMs. Based on the assumption that
safety-aligned shortcuts naturally exist in ALMs, we design a method to
identify universal Shortcut Activation Perturbations (SAPs) that serve as
triggers that activate the safety shortcuts to safeguard ALMs at inference
time. To better sift out effective triggers while preserving the model&#x27;s
utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM),
which restricts perturbations to Mel-frequency bins that are sensitive to
jailbreaks but insensitive to speech understanding. Both theoretical analyses
and empirical results demonstrate the robustness of our method against both
seen and unseen attacks. Overall, \MethodName reduces the average success rate
of advanced ALM-specific jailbreak attacks to 4.6% across four models, while
maintaining comparable utility on benign benchmarks, establishing it as the new
state of the art. Our code and data are available at
https://github.com/WeifeiJin/ALMGuard.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ALMGuard：音频语言模型的安全捷径及其发现方式</div>
<div class="mono" style="margin-top:8px">最近音频语言模型（ALMs）的进展显著提升了多模态理解能力。然而，音频模态的引入也带来了新的独特脆弱性向量。之前的研究提出了专门针对ALMs的越狱攻击，揭示了直接从传统音频对抗攻击或基于文本的大型语言模型（LLM）越狱转移的防御在这些ALM特定威胁面前大多无效。为了解决这个问题，我们提出了ALMGuard，这是第一个针对ALMs量身定制的防御框架。基于安全对齐捷径自然存在于ALMs的假设，我们设计了一种方法来识别通用捷径激活扰动（SAPs），作为触发器以激活安全捷径，在推理时保护ALMs。为了更好地筛选有效触发器，同时保持模型在良性任务上的效用，我们进一步提出了梅尔梯度稀疏掩码（M-GSM），该方法将扰动限制在对越狱敏感但对语音理解不敏感的梅尔频率区间。理论分析和实证结果均表明我们的方法在已知和未知攻击下的鲁棒性。总体而言，\MethodName将四个模型中先进的ALM特定越狱攻击的平均成功率降低到4.6%，同时在良性基准上保持了可比的效用，确立了其作为新的最先进技术。我们的代码和数据可在https://github.com/WeifeiJin/ALMGuard获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities introduced by Audio-Language Models (ALMs), which have advanced multimodal understanding but are susceptible to unique threats such as jailbreak attacks. Previous defenses derived from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks have proven ineffective against these ALM-specific vulnerabilities. The proposed approach, ALMGuard, is motivated by the existence of safety-aligned shortcuts in ALMs and introduces a method to identify Shortcut Activation Perturbations (SAPs) that activate these safety measures during inference. The methodology includes the Mel-Gradient Sparse Mask (M-GSM) to ensure that perturbations target sensitive Mel-frequency bins while preserving performance on benign tasks. The results show that ALMGuard reduces the average success rate of advanced jailbreak attacks to 4.6% across four models while maintaining comparable utility on benign benchmarks, establishing a new state of the art in ALM safety.</div>
<div class="mono" style="margin-top:8px">本研究关注音频语言模型（ALMs）引入的脆弱性，尽管ALMs在多模态理解方面取得了显著进展，但也面临独特的威胁，如越狱攻击。以往的方法，包括从传统音频对抗攻击和基于文本的大型语言模型越狱中转移的防御，已被证明对这些ALM特定的脆弱性无效。所提出的方法ALMGuard基于安全对齐快捷方式在ALMs中自然存在的假设，提出了一个新颖的防御框架，通过识别通用的快捷方式激活扰动（SAPs）来激活这些安全快捷方式。该方法论包括Mel-Gradient Sparse Mask（M-GSM），旨在将扰动限制在对攻击敏感但对语音理解不敏感的Mel频率区间。实验结果表明，ALMGuard将四个模型中先进的ALM特定越狱攻击的平均成功率降低到4.6%，同时在良性基准上保持了可比的效用，从而在该领域建立了新的技术领先水平。</div>
</details>
</div>
<div class="card">
<div class="title">Improving LLM Safety Alignment with Dual-Objective Optimization</div>
<div class="meta-line">Authors: Xuandong Zhao, Will Cai, Tianneng Shi, David Huang, Licong Lin, Song Mei, Dawn Song</div>
<div class="meta-line">Venue: ICML 2025</div>
<div class="meta-line">First: 2025-03-05T18:01:05+00:00 · Latest: 2025-10-30T01:16:06+00:00</div>
<div class="meta-line">Comments: ICML 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.03710v3">Abs</a> · <a href="http://arxiv.org/pdf/2503.03710v3">PDF</a> · <a href="https://github.com/wicai24/DOOR-Alignment">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing training-time safety alignment techniques for large language models
(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization
(DPO), a widely deployed alignment method, exhibits limitations in both
experimental and theoretical contexts as its loss function proves suboptimal
for refusal learning. Through gradient-based analysis, we identify these
shortcomings and propose an improved safety alignment that disentangles DPO
objectives into two components: (1) robust refusal training, which encourages
refusal even when partial unsafe generations are produced, and (2) targeted
unlearning of harmful knowledge. This approach significantly increases LLM
robustness against a wide range of jailbreak attacks, including prefilling,
suffix, and multi-turn attacks across both in-distribution and
out-of-distribution scenarios. Furthermore, we introduce a method to emphasize
critical refusal tokens by incorporating a reward-based token-level weighting
mechanism for refusal learning, which further improves the robustness against
adversarial exploits. Our research also suggests that robustness to jailbreak
attacks is correlated with token distribution shifts in the training process
and internal representations of refusal and harmful tokens, offering valuable
directions for future research in LLM safety alignment. The code is available
at https://github.com/wicai24/DOOR-Alignment</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过双目标优化提高大型语言模型的安全对齐</div>
<div class="mono" style="margin-top:8px">现有的大型语言模型（LLMs）训练时安全对齐技术仍然容易受到越狱攻击。直接偏好优化（DPO）作为一种广泛应用的对齐方法，在实验和理论背景下均表现出局限性，因为其损失函数在拒绝学习中被证明是次优的。通过基于梯度的分析，我们识别了这些缺陷，并提出了一种改进的安全对齐方法，将DPO目标分解为两个组成部分：（1）强健的拒绝训练，鼓励在产生部分不安全生成时仍然拒绝，以及（2）针对有害知识的有针对性的去学习。这种方法显著提高了LLM对各种越狱攻击的鲁棒性，包括预填充、后缀和多轮攻击，涵盖了分布内和分布外场景。此外，我们引入了一种通过结合基于奖励的令牌级加权机制来强调关键拒绝令牌的方法，以进一步提高对抗性利用的鲁棒性。我们的研究还表明，对越狱攻击的鲁棒性与训练过程中的令牌分布变化以及拒绝和有害令牌的内部表示相关，为未来LLM安全对齐的研究提供了宝贵的方向。代码可在 https://github.com/wicai24/DOOR-Alignment 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of large language models (LLMs) to jailbreak attacks, highlighting the limitations of existing training-time safety alignment techniques, particularly direct preference optimization (DPO), which is suboptimal for refusal learning. The proposed approach improves upon DPO by disentangling its objectives into robust refusal training and targeted unlearning of harmful knowledge, effectively addressing the shortcomings of previous methods. This paper contributes to the field by significantly enhancing LLM robustness against various jailbreak attacks through a novel reward-based token-level weighting mechanism for refusal learning. The methodology involves gradient-based analysis and the implementation of the dual-objective optimization framework, which has been shown to improve performance in both in-distribution and out-of-distribution scenarios, thereby supporting the goal of increased safety alignment in LLMs.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLM）在面临越狱攻击时的脆弱性，强调了现有安全对齐技术的局限性，特别是直接偏好优化（DPO），其在拒绝学习方面表现不佳。所提出的方法通过将DPO的目标分解为强大的拒绝训练和有针对性的有害知识去除，显著提高了模型对各种越狱攻击策略的鲁棒性。这种双目标优化方法具有良好的动机，因为它直接解决了前述方法的不足之处。该方法论包括一种基于奖励的令牌级加权机制，以强调关键拒绝令牌，进一步增强了模型对对抗性攻击的防御能力。实验结果表明，在分布内和分布外场景中，LLM的鲁棒性显著提高，支持了增强LLM安全对齐的目标。</div>
</details>
</div>
<div class="card">
<div class="title">MixAT: Combining Continuous and Discrete Adversarial Training for LLMs</div>
<div class="meta-line">Authors: Csaba Dékány, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, Martin Vechev</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-22T17:32:50+00:00 · Latest: 2025-10-28T09:41:22+00:00</div>
<div class="meta-line">Comments: Published at 39th Conference on Neural Information Processing Systems
  (NeurIPS 2025)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.16947v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.16947v2">PDF</a> · <a href="https://github.com/insait-institute/MixAT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent efforts in Large Language Model (LLM) safety and alignment,
current adversarial attacks on frontier LLMs can still consistently force
harmful generations. Although adversarial training has been widely studied and
shown to significantly improve the robustness of traditional machine learning
models, its strengths and weaknesses in the context of LLMs are less
understood. Specifically, while existing discrete adversarial attacks are
effective at producing harmful content, training LLMs with concrete adversarial
prompts is often computationally expensive, leading to reliance on continuous
relaxations. At the same time, despite their effectiveness and generalization
capabilities, training with continuous perturbations does not always capture
the full spectrum of vulnerabilities exploited by discrete attacks. In this
work, we aim to bridge this gap by introducing MixAT, a novel method that
combines stronger discrete and faster continuous attacks during training. We
rigorously evaluate MixAT across a wide spectrum of state-of-the-art attacks,
proposing the At Least One Attack Success Rate (ALO-ASR) metric to capture the
worst-case vulnerability of models. We show MixAT achieves substantially better
robustness (ALO-ASR &lt; 20%) compared to prior defenses (ALO-ASR &gt; 50%), while
maintaining a runtime comparable to methods based on continuous relaxations. We
further analyze MixAT in realistic deployment settings, exploring how chat
templates, quantization, low-rank adapters, and temperature affect both
adversarial training and evaluation, revealing additional blind spots in
current methodologies. Our results demonstrate that MixAT&#x27;s discrete-continuous
defense offers a principled and superior robustness-accuracy tradeoff with
minimal computational overhead, highlighting its promise for building safer
LLMs. We provide our code and models at
https://github.com/insait-institute/MixAT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MixAT：结合连续和离散对抗训练的LLM</div>
<div class="mono" style="margin-top:8px">尽管近期在大型语言模型（LLM）安全性和对齐方面进行了努力，当前对前沿LLM的对抗攻击仍能持续强制生成有害内容。虽然对抗训练已被广泛研究并显示出显著提高传统机器学习模型的鲁棒性，但在LLM背景下其优缺点尚不清楚。具体而言，现有的离散对抗攻击在生成有害内容方面有效，但使用具体对抗提示训练LLM通常计算成本高，导致依赖于连续松弛。同时，尽管连续扰动的训练有效且具有泛化能力，但并不总能捕捉到离散攻击所利用的全部脆弱性。在本研究中，我们旨在通过引入MixAT这一新方法来填补这一空白，该方法在训练过程中结合了更强的离散攻击和更快的连续攻击。我们在广泛的最先进攻击中严格评估MixAT，提出了至少一个攻击成功率（ALO-ASR）指标，以捕捉模型的最坏情况脆弱性。我们展示了MixAT在鲁棒性方面显著优于先前的防御（ALO-ASR &lt; 20%），而运行时间与基于连续松弛的方法相当。我们进一步分析了MixAT在现实部署环境中的表现，探讨了聊天模板、量化、低秩适配器和温度如何影响对抗训练和评估，揭示了当前方法中的额外盲点。我们的结果表明，MixAT的离散-连续防御提供了原则性和优越的鲁棒性-准确性权衡，计算开销最小，突显了其构建更安全LLM的潜力。我们在https://github.com/insait-institute/MixAT提供了我们的代码和模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the ongoing challenges in ensuring the safety and alignment of Large Language Models (LLMs), particularly in light of persistent adversarial attacks that can lead to harmful outputs. Previous methods primarily focused on discrete adversarial training, which, while effective, proved to be computationally intensive and often relied on continuous relaxations that failed to capture the full range of vulnerabilities. The proposed MixAT method innovatively combines discrete and continuous adversarial training, effectively bridging the gap between the two approaches. The contribution of this paper lies in its introduction of the At Least One Attack Success Rate (ALO-ASR) metric, which evaluates model vulnerabilities, and its demonstration that MixAT significantly enhances robustness (ALO-ASR &lt; 20%) compared to existing defenses (ALO-ASR &gt; 50%), all while maintaining comparable computational efficiency. The findings suggest that MixAT provides a promising framework for improving the robustness-accuracy tradeoff in LLMs, making strides toward safer deployment in real-world applications.</div>
<div class="mono" style="margin-top:8px">本文探讨了确保大型语言模型（LLMs）安全性和对齐性的持续挑战，特别是在面对可能导致有害输出的对抗性攻击时。以往的方法主要依赖于离散对抗训练，尽管有效，但计算成本高，通常需要连续松弛，这无法充分捕捉离散攻击所利用的脆弱性。所提出的方法MixAT创新性地结合了离散和连续对抗训练，以增强鲁棒性，同时保持计算效率。本文的贡献在于引入了至少一个攻击成功率（ALO-ASR）指标来评估模型脆弱性，并证明MixAT在鲁棒性方面显著优于现有防御（ALO-ASR &lt; 20%），而现有防御的ALO-ASR超过50%，且运行时间与基于连续方法相当。这一研究方法有效解决了以往方法的局限性，并显示出在实际应用中开发更安全的LLMs的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">SAGE: A Generic Framework for LLM Safety Evaluation</div>
<div class="meta-line">Authors: Madhur Jindal, Hari Shrawgi, Parag Agrawal, Sandipan Dandapat</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-04-28T11:01:08+00:00 · Latest: 2025-10-27T10:19:55+00:00</div>
<div class="meta-line">Comments: Accepted to EMNLP 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.19674v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.19674v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Language Models are rapidly deployed across diverse applications
from healthcare to financial advice, safety evaluation struggles to keep pace.
Current benchmarks focus on single-turn interactions with generic policies,
failing to capture the conversational dynamics of real-world usage and the
application-specific harms that emerge in context. Such potential oversights
can lead to harms that go unnoticed in standard safety benchmarks and other
current evaluation methodologies. To address these needs for robust AI safety
evaluation, we introduce SAGE (Safety AI Generic Evaluation), an automated
modular framework designed for customized and dynamic harm evaluations. SAGE
employs prompted adversarial agents with diverse personalities based on the Big
Five model, enabling system-aware multi-turn conversations that adapt to target
applications and harm policies. We evaluate seven state-of-the-art LLMs across
three applications and harm policies. Multi-turn experiments show that harm
increases with conversation length, model behavior varies significantly when
exposed to different user personalities and scenarios, and some models minimize
harm via high refusal rates that reduce usefulness. We also demonstrate policy
sensitivity within a harm category where tightening a child-focused sexual
policy substantially increases measured defects across applications. These
results motivate adaptive, policy-aware, and context-specific testing for safer
real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAGE：大型语言模型安全评估的通用框架</div>
<div class="mono" style="margin-top:8px">随着大型语言模型在医疗保健到金融建议等多种应用中迅速部署，安全评估难以跟上步伐。目前的基准测试侧重于与通用策略的单轮交互，未能捕捉到真实使用中的对话动态和特定应用中出现的危害。这些潜在的忽视可能导致在标准安全基准和其他当前评估方法中未被注意的危害。为满足对强大AI安全评估的需求，我们引入了SAGE（安全AI通用评估），这是一个旨在定制和动态危害评估的自动化模块化框架。SAGE采用基于五大人格模型的多样化个性化对抗代理，能够进行系统感知的多轮对话，适应目标应用和危害政策。我们在三个应用和危害政策中评估了七个最先进的LLM。多轮实验表明，随着对话长度的增加，危害也在增加；模型行为在不同用户个性和场景下显著变化；一些模型通过高拒绝率来最小化危害，从而降低了实用性。我们还展示了在一个危害类别内的政策敏感性，其中收紧以儿童为中心的性政策显著增加了跨应用的缺陷测量。这些结果促使我们进行适应性、政策感知和特定上下文的测试，以实现更安全的现实世界部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the urgent need for effective safety evaluation of Large Language Models (LLMs) as they are increasingly used in various sensitive applications, where existing benchmarks fall short by only assessing single-turn interactions and ignoring the complexities of real-world conversational dynamics. Previous methods have not adequately captured application-specific harms, leading to potential oversights in safety evaluations. The proposed SAGE framework offers a novel solution by utilizing prompted adversarial agents with diverse personalities to facilitate multi-turn conversations that are tailored to specific applications and harm policies. This approach is well-motivated as it enables a more comprehensive assessment of LLM behavior in context. The methodology involves evaluating seven state-of-the-art LLMs across three different applications and harm policies, revealing that harm tends to increase with conversation length and that model behavior is significantly influenced by user personality. The findings highlight the necessity for adaptive and context-aware safety evaluations to ensure safer deployment of LLMs in real-world scenarios.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）在多种应用中快速部署所带来的安全评估需求，指出现有基准测试主要集中于单轮交互和通用策略，未能考虑真实使用中的对话动态和特定应用的危害。以往的方法未能捕捉到这些动态，可能导致安全评估中的潜在疏漏。提出的SAGE框架通过利用具有多样化个性的对抗代理，促进系统感知的多轮对话，从而提供针对实际应用复杂性的定制评估。这篇论文的贡献在于展示SAGE在评估七种最先进的LLMs在三种应用中的有效性，结果表明，随着对话长度的增加，危害也在上升，模型行为因用户个性而显著变化，最终倡导更具适应性和上下文敏感性的安全测试，以提高实际部署的安全性。</div>
</details>
</div>
<div class="card">
<div class="title">Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense   Against Adversarial LLM Jailbreaks</div>
<div class="meta-line">Authors: Md. Mehedi Hasan, Ziaur Rahman, Rafid Mostafiz, Md. Abir Hossain</div>
<div class="meta-line">First: 2025-10-26T11:19:47+00:00 · Latest: 2025-10-26T11:19:47+00:00</div>
<div class="meta-line">Comments: 11 pages, 5 figures. Preprint version under review in the area of
  Artificial Intelligence (cs.AI)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22628v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.22628v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a real-time modular defense system named Sentra-Guard.
The system detects and mitigates jailbreak and prompt injection attacks
targeting large language models (LLMs). The framework uses a hybrid
architecture with FAISS-indexed SBERT embedding representations that capture
the semantic meaning of prompts, combined with fine-tuned transformer
classifiers, which are machine learning models specialized for distinguishing
between benign and adversarial language inputs. It identifies adversarial
prompts in both direct and obfuscated attack vectors. A core innovation is the
classifier-retriever fusion module, which dynamically computes context-aware
risk scores that estimate how likely a prompt is to be adversarial based on its
content and context. The framework ensures multilingual resilience with a
language-agnostic preprocessing layer. This component automatically translates
non-English prompts into English for semantic evaluation, enabling consistent
detection across over 100 languages. The system includes a HITL feedback loop,
where decisions made by the automated system are reviewed by human experts for
continual learning and rapid adaptation under adversarial pressure.
Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and
malicious prompts, enhancing detection reliability and reducing false
positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 =
1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading
baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike
black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible
with diverse LLM backends. Its modular design supports scalable deployment in
both commercial and open-source environments. The system establishes a new
state-of-the-art in adversarial LLM defense.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Sentra-Guard：一种多语言人机框架，用于实时防御对抗性LLM越狱</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为Sentra-Guard的实时模块化防御系统。该系统检测并缓解针对大型语言模型（LLMs）的越狱和提示注入攻击。该框架采用混合架构，使用FAISS索引的SBERT嵌入表示来捕捉提示的语义含义，并结合微调的变换器分类器，这些机器学习模型专门用于区分良性和对抗性语言输入。它识别直接和模糊攻击向量中的对抗性提示。核心创新是分类器-检索器融合模块，它动态计算上下文感知风险评分，估计提示基于其内容和上下文的对抗性可能性。该框架确保多语言的韧性，具有语言无关的预处理层。该组件自动将非英语提示翻译成英语进行语义评估，从而在100多种语言中实现一致检测。该系统包括一个人机反馈循环，自动系统做出的决策由人类专家审查，以便在对抗压力下进行持续学习和快速适应。Sentra-Guard维护一个不断发展的良性和恶意提示的双标签知识库，提高检测可靠性并减少误报。评估结果显示检测率为99.96%（AUC = 1.00，F1 = 1.00），攻击成功率（ASR）仅为0.004%。这优于领先的基线，如LlamaGuard-2（1.3%）和OpenAI Moderation（3.7%）。与黑箱方法不同，Sentra-Guard是透明的、可微调的，并与多种LLM后端兼容。其模块化设计支持在商业和开源环境中的可扩展部署。该系统在对抗性LLM防御中建立了新的最先进水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concern of adversarial attacks on large language models (LLMs), specifically focusing on jailbreak and prompt injection threats. Previous methods often relied on black-box approaches that lacked transparency and adaptability, leading to higher false positive rates and limited effectiveness. The proposed Sentra-Guard framework distinguishes itself by employing a hybrid architecture that combines FAISS-indexed SBERT embeddings with fine-tuned transformer classifiers, enabling it to detect adversarial prompts in various forms while maintaining multilingual capabilities through automatic translation. The paper contributes a modular defense system that achieves a remarkable 99.96% detection rate and a mere 0.004% attack success rate, significantly outperforming existing solutions like LlamaGuard-2 and OpenAI Moderation. This innovative approach not only enhances detection reliability but also supports scalable deployment across different environments, establishing a new benchmark in adversarial LLM defense.</div>
<div class="mono" style="margin-top:8px">本研究关注于大型语言模型（LLM）面临的日益严重的对抗性攻击，特别是越狱和提示注入攻击。以往的方法在检测准确性和适应性方面存在不足，通常依赖于缺乏透明度和微调能力的黑箱方法。提出的Sentra-Guard框架引入了一种混合架构，结合了FAISS索引的SBERT嵌入和微调的变换器分类器，有效识别各种攻击向量中的对抗性提示。这种方法具有良好的动机，因为它结合了分类器-检索器融合模块，根据上下文动态评估风险，并配备了语言无关的预处理层以支持多语言。该论文贡献了一个强大的防御系统，实现了99.96%的检测率和仅0.004%的攻击成功率，显著优于现有解决方案如LlamaGuard-2和OpenAI Moderation，从而在对抗性LLM防御中建立了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks</div>
<div class="meta-line">Authors: Havva Alizadeh Noughabi, Julien Serbanescu, Fattane Zarrinkalam, Ali Dehghantanha</div>
<div class="meta-line">First: 2025-10-24T19:20:23+00:00 · Latest: 2025-10-24T19:20:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.21983v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.21983v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent advances, Large Language Models remain vulnerable to jailbreak
attacks that bypass alignment safeguards and elicit harmful outputs. While
prior research has proposed various attack strategies differing in human
readability and transferability, little attention has been paid to the
linguistic and psychological mechanisms that may influence a model&#x27;s
susceptibility to such attacks. In this paper, we examine an interdisciplinary
line of research that leverages foundational theories of persuasion from the
social sciences to craft adversarial prompts capable of circumventing alignment
constraints in LLMs. Drawing on well-established persuasive strategies, we
hypothesize that LLMs, having been trained on large-scale human-generated text,
may respond more compliantly to prompts with persuasive structures.
Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive
fingerprints that emerge in their jailbreak responses. Empirical evaluations
across multiple aligned LLMs reveal that persuasion-aware prompts significantly
bypass safeguards, demonstrating their potential to induce jailbreak behaviors.
This work underscores the importance of cross-disciplinary insight in
addressing the evolving challenges of LLM safety. The code and data are
available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>揭示大型语言模型在越狱攻击中的说服指纹</div>
<div class="mono" style="margin-top:8px">尽管最近取得了进展，大型语言模型仍然容易受到越狱攻击，这些攻击绕过了对齐保护措施并引发有害输出。虽然之前的研究提出了不同的人类可读性和可转移性的各种攻击策略，但对可能影响模型对这些攻击的易感性的语言和心理机制关注较少。本文考察了一条跨学科的研究路线，利用社会科学中的说服基础理论，设计能够绕过大型语言模型对齐约束的对抗性提示。基于成熟的说服策略，我们假设大型语言模型在大规模人类生成文本上训练后，可能对具有说服结构的提示反应更顺从。此外，我们还研究大型语言模型是否在其越狱响应中表现出独特的说服指纹。对多个对齐大型语言模型的实证评估表明，关注说服的提示显著绕过了保护措施，展示了它们诱发越狱行为的潜力。这项工作强调了跨学科洞察在应对大型语言模型安全不断演变的挑战中的重要性。代码和数据可用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the vulnerability of Large Language Models (LLMs) to jailbreak attacks that can bypass alignment safeguards and produce harmful outputs. Previous methods have focused on various attack strategies but often overlooked the linguistic and psychological factors influencing a model&#x27;s susceptibility. The proposed approach integrates theories of persuasion from social sciences to create adversarial prompts that exploit persuasive structures, hypothesizing that LLMs may respond more readily to such prompts due to their training on human-generated text. The paper contributes to the understanding of LLM safety by empirically demonstrating that persuasion-aware prompts can effectively bypass safeguards in multiple aligned LLMs, indicating a significant advancement in addressing jailbreak behaviors and highlighting the value of interdisciplinary research. The methodology involves crafting and testing these persuasive prompts, with results showing a marked increase in jailbreak success rates, thereby supporting the goals of enhancing LLM safety.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在越狱攻击中的脆弱性，这些攻击利用了对齐安全措施的弱点，因其可能产生有害输出而受到关注。以往的方法侧重于各种攻击策略，但往往忽视了影响模型易受攻击性的语言和心理因素。所提出的方法结合了社会科学中的说服理论，设计出旨在绕过对齐约束的对抗性提示，这与传统方法有所不同。研究通过实证表明，围绕说服技巧构建的提示能够有效诱发多个对齐LLMs的越狱行为。该方法论涉及设计和测试这些关注说服的提示，结果显示它们显著增强了绕过安全措施的能力，从而支持了理解和减轻LLM脆弱性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Shape it Up! Restoring LLM Safety during Finetuning</div>
<div class="meta-line">Authors: ShengYun Peng, Pin-Yu Chen, Jianfeng Chi, Seongmin Lee, Duen Horng Chau</div>
<div class="meta-line">First: 2025-05-22T18:05:16+00:00 · Latest: 2025-10-24T14:34:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.17196v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.17196v2">PDF</a> · <a href="https://github.com/poloclub/star-dss">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Finetuning large language models (LLMs) enables user-specific customization
but introduces critical safety risks: even a few harmful examples can
compromise safety alignment. A common mitigation strategy is to update the
model more strongly on examples deemed safe, while downweighting or excluding
those flagged as unsafe. However, because safety context can shift within a
single example, updating the model equally on both harmful and harmless parts
of a response is suboptimal-a coarse treatment we term static safety shaping.
In contrast, we propose dynamic safety shaping (DSS), a framework that uses
fine-grained safety signals to reinforce learning from safe segments of a
response while suppressing unsafe content. To enable such fine-grained control
during finetuning, we introduce a key insight: guardrail models, traditionally
used for filtering, can be repurposed to evaluate partial responses, tracking
how safety risk evolves throughout the response, segment by segment. This leads
to the Safety Trajectory Assessment of Response (STAR), a token-level signal
that enables shaping to operate dynamically over the training sequence.
Building on this, we present STAR-DSS, guided by STAR scores, that robustly
mitigates finetuning risks and delivers substantial safety improvements across
diverse threats, datasets, and model families-all without compromising
capability on intended tasks. We encourage future safety research to build on
dynamic shaping principles for stronger mitigation against evolving finetuning
risks. Our code is publicly available at https://github.com/poloclub/star-dss.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>塑造它！在微调过程中恢复大型语言模型的安全性</div>
<div class="mono" style="margin-top:8px">微调大型语言模型（LLMs）使用户能够进行特定定制，但引入了关键的安全风险：即使是少量有害示例也可能破坏安全对齐。一种常见的缓解策略是对被认为安全的示例进行更强的模型更新，同时降低或排除被标记为不安全的示例。然而，由于安全上下文可能在单个示例中发生变化，平等地更新模型对有害和无害部分的响应是次优的——我们称之为静态安全塑造。相反，我们提出了动态安全塑造（DSS），这是一个使用细粒度安全信号的框架，旨在强化从响应的安全部分学习，同时抑制不安全内容。为了在微调过程中实现这种细粒度控制，我们引入了一个关键见解：传统上用于过滤的护栏模型可以重新用于评估部分响应，跟踪安全风险如何在整个响应中逐段演变。这导致了响应的安全轨迹评估（STAR），这是一个令牌级信号，使塑造能够在训练序列中动态运行。在此基础上，我们提出了STAR-DSS，基于STAR分数，稳健地减轻微调风险，并在各种威胁、数据集和模型家族中提供显著的安全改进——所有这些都不影响预期任务的能力。我们鼓励未来的安全研究基于动态塑造原则，以更强的缓解措施应对不断演变的微调风险。我们的代码已公开发布在https://github.com/poloclub/star-dss。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical safety risks associated with finetuning large language models (LLMs), where even a few harmful examples can undermine safety alignment. Previous methods typically employed static safety shaping, which inadequately treats harmful and harmless parts of responses equally, leading to suboptimal outcomes. The proposed dynamic safety shaping (DSS) framework improves upon this by utilizing fine-grained safety signals to enhance learning from safe segments while suppressing unsafe content. This approach is well-motivated as it allows for a more nuanced evaluation of responses through guardrail models, which track safety risks segment by segment. The contribution of the paper lies in the introduction of the Safety Trajectory Assessment of Response (STAR) and STAR-DSS, which effectively mitigate finetuning risks and achieve significant safety improvements across various threats and datasets without sacrificing performance on intended tasks.</div>
<div class="mono" style="margin-top:8px">本研究解决了微调大型语言模型（LLMs）所带来的关键安全风险，少量有害示例可能会危及安全对齐。以往的方法通常采用静态安全塑形，这种方法在响应的安全和不安全部分对模型的更新不够充分，导致安全结果不理想。提出的动态安全塑形（DSS）框架通过利用细粒度的安全信号来强化对安全段的学习，同时抑制不安全内容，从而改进了这一点，利用护栏模型评估部分响应。本文的贡献在于引入了响应的安全轨迹评估（STAR），作为动态塑形的标记级信号，最终形成了STAR-DSS，有效减轻了微调风险，并在各种威胁和数据集上增强了安全性，而不牺牲模型在预期任务上的性能。该方法展示了显著的安全改进，支持了在LLM定制过程中保持安全的目标。</div>
</details>
</div>
<div class="card">
<div class="title">RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety   Evaluation via Role-Specialized Collaboration</div>
<div class="meta-line">Authors: Xiuyuan Chen, Jian Zhao, Yuchen Yuan, Tianle Zhang, Huilin Zhou, Zheng Zhu, Ping Hu, Linghe Kong, Chi Zhang, Weiran Huang, Xuelong Li</div>
<div class="meta-line">First: 2025-09-28T09:35:32+00:00 · Latest: 2025-10-23T03:33:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.25271v4">Abs</a> · <a href="http://arxiv.org/pdf/2509.25271v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing safety evaluation methods for large language models (LLMs) suffer
from inherent limitations, including evaluator bias and detection failures
arising from model homogeneity, which collectively undermine the robustness of
risk evaluation processes. This paper seeks to re-examine the risk evaluation
paradigm by introducing a theoretical framework that reconstructs the
underlying risk concept space. Specifically, we decompose the latent risk
concept space into three mutually exclusive subspaces: the explicit risk
subspace (encompassing direct violations of safety guidelines), the implicit
risk subspace (capturing potential malicious content that requires contextual
reasoning for identification), and the non-risk subspace. Furthermore, we
propose RADAR, a multi-agent collaborative evaluation framework that leverages
multi-round debate mechanisms through four specialized complementary roles and
employs dynamic update mechanisms to achieve self-evolution of risk concept
distributions. This approach enables comprehensive coverage of both explicit
and implicit risks while mitigating evaluator bias. To validate the
effectiveness of our framework, we construct an evaluation dataset comprising
800 challenging cases. Extensive experiments on our challenging testset and
public benchmarks demonstrate that RADAR significantly outperforms baseline
evaluation methods across multiple dimensions, including accuracy, stability,
and self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87%
improvement in risk identification accuracy compared to the strongest baseline
evaluation method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RADAR：一种风险意识动态多智能体框架，用于通过角色专门化协作评估大型语言模型的安全性</div>
<div class="mono" style="margin-top:8px">现有的大型语言模型（LLMs）安全评估方法存在固有局限性，包括评估者偏见和由于模型同质性导致的检测失败，这些问题共同削弱了风险评估过程的稳健性。本文旨在通过引入一个理论框架重新审视风险评估范式，该框架重构了潜在风险概念空间。具体而言，我们将潜在风险概念空间分解为三个相互排斥的子空间：显性风险子空间（包括对安全指南的直接违反）、隐性风险子空间（捕捉需要上下文推理才能识别的潜在恶意内容）和非风险子空间。此外，我们提出了RADAR，一个多智能体协作评估框架，利用四个专门互补角色的多轮辩论机制，并采用动态更新机制实现风险概念分布的自我演化。这种方法能够全面覆盖显性和隐性风险，同时减轻评估者偏见。为了验证我们框架的有效性，我们构建了一个包含800个挑战性案例的评估数据集。在我们的挑战性测试集和公共基准上的广泛实验表明，RADAR在多个维度上显著优于基线评估方法，包括准确性、稳定性和自我评估风险敏感性。值得注意的是，RADAR在风险识别准确性方面相比最强基线评估方法提高了28.87%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the limitations of existing safety evaluation methods for large language models (LLMs), which are often hindered by evaluator bias and detection failures due to model homogeneity. Previous methods have struggled with these issues, leading to inadequate risk assessments. The proposed approach, RADAR, introduces a theoretical framework that redefines the risk concept space into three distinct subspaces and employs a multi-agent collaborative evaluation framework with specialized roles and dynamic updates to enhance risk evaluation. The contribution of this paper lies in its ability to comprehensively cover both explicit and implicit risks while reducing evaluator bias. The methodology involves a multi-round debate mechanism and has been validated on a dataset of 800 challenging cases, demonstrating that RADAR significantly outperforms baseline methods, achieving a 28.87% improvement in risk identification accuracy.</div>
<div class="mono" style="margin-top:8px">本文针对现有大型语言模型（LLM）安全评估方法的局限性进行了研究，这些方法通常受到评估者偏见和模型同质性导致的检测失败的影响，从而削弱了风险评估的稳健性。以往的方法未能有效区分不同类型的风险，导致评估不足。所提出的方法RADAR引入了一个理论框架，将风险分为显性风险、隐性风险和非风险子空间，并利用多智能体协作评估框架，结合专业角色和动态更新机制，以增强风险识别能力并减少偏见。本文的贡献在于其新颖的框架和方法论，包括一个包含800个案例的综合评估数据集。实验结果表明，RADAR在风险识别准确性上显著优于基线方法，提升了28.87%，从而支持了其增强LLM安全评估的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities</div>
<div class="meta-line">Authors: Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng, Aidan San, Michel Galley, Jianfeng Gao</div>
<div class="meta-line">Venue: NAACL 2025 Oral</div>
<div class="meta-line">First: 2024-10-24T06:36:12+00:00 · Latest: 2025-10-23T00:57:57+00:00</div>
<div class="meta-line">Comments: Accepted to NAACL 2025 Main (Oral)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2410.18469v5">Abs</a> · <a href="http://arxiv.org/pdf/2410.18469v5">PDF</a> · <a href="https://github.com/SunChungEn/ADV-LLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research has shown that Large Language Models (LLMs) are vulnerable to
automated jailbreak attacks, where adversarial suffixes crafted by algorithms
appended to harmful queries bypass safety alignment and trigger unintended
responses. Current methods for generating these suffixes are computationally
expensive and have low Attack Success Rates (ASR), especially against
well-aligned models like Llama2 and Llama3. To overcome these limitations, we
introduce ADV-LLM, an iterative self-tuning process that crafts adversarial
LLMs with enhanced jailbreak ability. Our framework significantly reduces the
computational cost of generating adversarial suffixes while achieving nearly
100\% ASR on various open-source LLMs. Moreover, it exhibits strong attack
transferability to closed-source models, achieving 99\% ASR on GPT-3.5 and 49\%
ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving
jailbreak ability, ADV-LLM provides valuable insights for future safety
alignment research through its ability to generate large datasets for studying
LLM safety. Our code is available at: https://github.com/SunChungEn/ADV-LLM</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强越狱能力的迭代自调节大型语言模型</div>
<div class="mono" style="margin-top:8px">最近的研究表明，大型语言模型（LLMs）易受到自动化越狱攻击，算法生成的对抗后缀附加在有害查询上，绕过安全对齐并触发意外响应。目前生成这些后缀的方法计算成本高，攻击成功率（ASR）低，尤其是针对像Llama2和Llama3这样的良好对齐模型。为克服这些限制，我们提出了ADV-LLM，一种迭代自调节过程，旨在制作具有增强越狱能力的对抗LLMs。我们的框架显著降低了生成对抗后缀的计算成本，同时在各种开源LLMs上实现了近100%的ASR。此外，它对闭源模型表现出强大的攻击可转移性，在GPT-3.5上实现了99%的ASR，在GPT-4上实现了49%的ASR，尽管仅在Llama3上进行了优化。除了提高越狱能力，ADV-LLM还通过其生成大规模数据集的能力，为未来的安全对齐研究提供了宝贵的见解。我们的代码可在以下网址获取：https://github.com/SunChungEn/ADV-LLM</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the vulnerability of Large Language Models (LLMs) to automated jailbreak attacks, where adversarial suffixes can bypass safety mechanisms and elicit unintended responses. Previous methods for generating these suffixes have been computationally intensive and yielded low Attack Success Rates (ASR), particularly against well-aligned models like Llama2 and Llama3. The proposed ADV-LLM introduces an iterative self-tuning process that significantly reduces the computational cost while achieving nearly 100% ASR on various open-source LLMs and demonstrating strong transferability to closed-source models, with 99% ASR on GPT-3.5 and 49% on GPT-4. This contribution not only enhances jailbreak capabilities but also aids future safety alignment research by generating large datasets for studying LLM safety, thus supporting its objectives effectively.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在自动越狱攻击中的脆弱性，其中对抗后缀可以绕过安全措施并引发意外响应。以往生成这些后缀的方法因其高计算成本和低攻击成功率（ASR）而受到批评，尤其是在针对像Llama2和Llama3这样的良好对齐模型时。提出的ADV-LLM引入了一种迭代自调节过程，显著降低了计算负担，同时在各种开源LLM上实现了近100%的ASR，并在封闭源模型上表现出强大的迁移能力，在GPT-3.5上达到99%的ASR，在GPT-4上达到49%。这一贡献不仅增强了越狱能力，还通过生成大量数据集为未来的LLM安全研究提供了帮助。</div>
</details>
</div>
<div class="card">
<div class="title">NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn   LLM Jailbreaks</div>
<div class="meta-line">Authors: Javad Rafiei Asl, Sidhant Narula, Mohammad Ghasemigol, Eduardo Blanco, Daniel Takabi</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-10-03T18:24:14+00:00 · Latest: 2025-10-21T17:41:58+00:00</div>
<div class="meta-line">Comments: This paper has been accepted in the main conference proceedings of
  the 2025 Conference on Empirical Methods in Natural Language Processing
  (EMNLP 2025). Javad Rafiei Asl and Sidhant Narula are co-first authors</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.03417v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.03417v2">PDF</a> · <a href="https://github.com/inspire-lab/NEXUS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have revolutionized natural language processing
but remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks
that distribute malicious intent across benign exchanges and bypass alignment
mechanisms. Existing approaches often explore the adversarial space poorly,
rely on hand-crafted heuristics, or lack systematic query refinement. We
present NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular
framework for constructing, refining, and executing optimized multi-turn
attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a
harmful intent into a structured semantic network of topics, entities, and
query chains; (2) a feedback-driven Simulator that iteratively refines and
prunes these chains through attacker-victim-judge LLM collaboration using
harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser
that adaptively navigates the refined query space for real-time attacks. This
pipeline uncovers stealthy, high-success adversarial paths across LLMs. On
several closed-source and open-source LLMs, NEXUS increases attack success rate
by 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NEXUS：用于利用多轮 LLM 越狱中不安全序列的网络探索</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）已彻底改变自然语言处理，但仍然容易受到越狱攻击，尤其是将恶意意图分散在良性交流中的多轮越狱，绕过对齐机制。现有方法往往对对抗空间探索不足，依赖手工设计的启发式方法，或缺乏系统的查询优化。我们提出了 NEXUS（用于利用不安全序列的网络探索），这是一个构建、优化和执行多轮攻击的模块化框架。NEXUS 包括：（1）ThoughtNet，它将有害意图分层扩展为主题、实体和查询链的结构化语义网络；（2）一个基于反馈的模拟器，通过攻击者-受害者-评判者 LLM 协作，使用有害性和语义相似性基准迭代优化和修剪这些链；（3）一个网络遍历器，适应性地导航优化后的查询空间以进行实时攻击。该流程揭示了 LLM 中隐蔽的高成功率对抗路径。在多个闭源和开源 LLM 上，NEXUS 的攻击成功率比之前的方法提高了 2.1% 到 19.4%。代码：https://github.com/inspire-lab/NEXUS</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of Large Language Models (LLMs) to jailbreak attacks, particularly multi-turn jailbreaks that exploit benign exchanges to bypass safety mechanisms. Previous methods have struggled with inadequate exploration of the adversarial space, reliance on manual heuristics, and lack of systematic query refinement. The proposed NEXUS framework offers a novel approach by integrating a hierarchical semantic network expansion, a feedback-driven simulator for iterative refinement, and an adaptive network traverser for real-time attacks, effectively addressing the limitations of existing methods. This paper contributes a comprehensive methodology that enhances the success rate of multi-turn attacks on both closed-source and open-source LLMs, achieving an increase of 2.1% to 19.4% in attack success compared to prior techniques, thereby supporting its objectives of improving adversarial exploration.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在监狱突破攻击中的脆弱性，特别是能够在良性互动中掩盖恶意意图的多轮攻击。以往的方法在对抗空间的探索上表现不佳，依赖手工设计的启发式方法，缺乏系统的查询优化，导致效果有限。提出的NEXUS框架通过整合有害意图的结构化语义网络、反馈驱动的模拟器进行迭代优化，以及网络遍历器自适应导航查询空间，有效解决了现有方法的不足。本文贡献了一种全面的方法论，在多种LLM上显著提高了攻击成功率，提升幅度为2.1%到19.4%，从而支持了更有效的多轮监狱突破的目标。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Safety Alignment is Divergence Estimation in Disguise</div>
<div class="meta-line">Authors: Rajdeep Haldar, Ziyi Wang, Qifan Song, Guang Lin, Yue Xing</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-02-02T04:09:42+00:00 · Latest: 2025-10-20T19:47:59+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.00657v3">Abs</a> · <a href="http://arxiv.org/pdf/2502.00657v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a theoretical framework showing that popular LLM alignment
methods, including RLHF and its variants, can be understood as divergence
estimators between aligned (safe or preferred) and unaligned (harmful or less
preferred) distributions. This perspective explains the emergence of separation
in the latent space between safe and harmful prompts after alignment. As an
application of our general divergence framework, we propose KLDO, a novel KL
divergence-based alignment method, and empirically validate its effectiveness.
We further show that using compliance-refusal datasets, rather than standard
preference-based datasets, leads to stronger separation and improved safety
alignment. Finally, to quantify the separation effect, we propose a
distance-based metric in the prompt representation space, which also acts as a
statistically significant indicator for model safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM安全对齐是伪装的散度估计</div>
<div class="mono" style="margin-top:8px">我们提出了一个理论框架，表明流行的LLM对齐方法，包括RLHF及其变体，可以理解为对齐（安全或优选）和未对齐（有害或不太优选）分布之间的散度估计。这个视角解释了对齐后安全和有害提示在潜在空间中出现分离的现象。作为我们一般散度框架的应用，我们提出了KLDO，一种基于KL散度的新型对齐方法，并实证验证了其有效性。我们进一步表明，使用合规-拒绝数据集而非标准偏好数据集，能够导致更强的分离和改善的安全对齐。最后，为了量化分离效果，我们提出了一种基于距离的度量，适用于提示表示空间，这也作为模型安全的统计显著性指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the challenge of aligning large language models (LLMs) to ensure their outputs are safe and preferred, a critical issue given the potential risks associated with unaligned models. Previous methods, such as Reinforcement Learning from Human Feedback (RLHF), have struggled with effectively distinguishing between safe and harmful outputs, leading to inadequate alignment. The proposed approach, KLDO, reinterprets alignment methods as divergence estimators and introduces a novel KL divergence-based alignment technique that enhances the separation between aligned and unaligned distributions. The paper contributes a theoretical framework that elucidates the alignment process and demonstrates that using compliance-refusal datasets improves safety alignment. Empirical validation shows that KLDO achieves better performance in separating safe from harmful prompts, supporting the goal of enhancing model safety.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有大型语言模型（LLM）对齐方法的局限性，例如基于人类反馈的强化学习（RLHF），这些方法往往难以有效区分安全和有害的输出。所提出的方法KLDO将这些对齐方法重新框架为散度估计器，提供了一个理论基础，阐明了安全和有害提示之间潜在空间分离的观察结果。该框架的动机明确，因为它增强了对LLM行为和安全性的理解。该方法论涉及使用KL散度进行对齐，并强调使用合规-拒绝数据集而非标准偏好数据集，从而实现了更好的分离和安全对齐。实证结果表明，所提出的方法在安全对齐方面取得了更强的效果，支持通过新的基于距离的度量来量化提示表示分离的目标。</div>
</details>
</div>
<div class="card">
<div class="title">CultureGuard: Towards Culturally-Aware Dataset and Guard Model for   Multilingual Safety Applications</div>
<div class="meta-line">Authors: Raviraj Joshi, Rakesh Paul, Kanishk Singla, Anusha Kamath, Michael Evans, Katherine Luna, Shaona Ghosh, Utkarsh Vaidya, Eileen Long, Sanjay Singh Chauhan, Niranjan Wartikar</div>
<div class="meta-line">First: 2025-08-03T10:35:05+00:00 · Latest: 2025-10-19T16:44:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.01710v3">Abs</a> · <a href="http://arxiv.org/pdf/2508.01710v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing use of Large Language Models (LLMs) in agentic applications
highlights the need for robust safety guard models. While content safety in
English is well-studied, non-English languages lack similar advancements due to
the high cost of collecting culturally aligned labeled datasets. We present
CultureGuard, a novel solution for curating culturally aligned, high-quality
safety datasets across multiple languages. Our approach introduces a four-stage
synthetic data generation and filtering pipeline: cultural data segregation,
cultural data adaptation, machine translation, and quality filtering. This
pipeline enables the conversion and expansion of the
Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct
languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.
The resulting dataset, Nemotron-Safety-Guard-Dataset-v3, comprises 386,661
samples in 9 languages and facilitates the training of
Llama-3.1-Nemotron-Safety-Guard-8B-v3 via LoRA-based fine-tuning. The final
model achieves state-of-the-art performance on several multilingual content
safety benchmarks. Furthermore, we show our moderately multilingual fine-tuning
enables robust cross-lingual transfer and strong zero-shot generalization to
unseen languages. We also benchmark the latest open LLMs on multilingual safety
and observe that these LLMs are more prone to give unsafe responses when
prompted in non-English languages. This work advances multilingual LLM safety
by enabling the development of culturally aware safety guard models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CultureGuard：面向多语言安全应用的文化意识数据集和保护模型</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在代理应用中的日益使用突显了对强大安全保护模型的需求。虽然英语内容安全研究较为充分，但由于收集文化对齐标注数据集的高成本，非英语语言缺乏类似的进展。我们提出了CultureGuard，这是一种新颖的解决方案，用于策划跨多种语言的文化对齐高质量安全数据集。我们的方法引入了一个四阶段的合成数据生成和过滤流程：文化数据分离、文化数据适应、机器翻译和质量过滤。该流程使得将Nemotron-Content-Safety-Dataset-V2英语安全数据集转换和扩展为阿拉伯语、德语、西班牙语、法语、印地语、日语、泰语和中文八种不同语言成为可能。最终数据集Nemotron-Safety-Guard-Dataset-v3包含9种语言的386,661个样本，并通过基于LoRA的微调促进了Llama-3.1-Nemotron-Safety-Guard-8B-v3的训练。最终模型在多个多语言内容安全基准上实现了最先进的性能。此外，我们展示了我们的适度多语言微调能够实现强大的跨语言迁移和对未见语言的强大零样本泛化。我们还对最新的开放LLMs在多语言安全方面进行了基准测试，观察到这些LLMs在非英语语言提示时更容易给出不安全的响应。这项工作通过促进文化意识安全保护模型的发展，推动了多语言LLM安全性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the gap in safety guard models for non-English languages, which have not received the same level of attention as English due to the challenges in collecting culturally aligned labeled datasets. Previous methods have struggled with this issue, leading to inadequate safety measures in multilingual applications. The proposed CultureGuard approach introduces a four-stage synthetic data generation and filtering pipeline that includes cultural data segregation, adaptation, machine translation, and quality filtering, effectively creating a culturally aligned safety dataset. This methodology results in the Nemotron-Safety-Guard-Dataset-v3, which contains 386,661 samples across nine languages and supports the training of the Llama-3.1-Nemotron-Safety-Guard-8B-v3 model. The model demonstrates state-of-the-art performance on multilingual content safety benchmarks and shows strong cross-lingual transfer capabilities, thereby significantly enhancing the safety of multilingual large language models.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）在非英语语言中安全防护模型的不足，现有方法因收集文化对齐标注数据集的高成本而面临挑战。提出的CultureGuard方法通过引入四阶段的合成数据生成和过滤流程，包括文化数据分离、适应、机器翻译和质量过滤，有效地将Nemotron-Content-Safety-Dataset-V2扩展到八种语言。该贡献导致创建了Nemotron-Safety-Guard-Dataset-v3，包含386,661个样本，覆盖九种语言，并促进了Llama-3.1-Nemotron-Safety-Guard-8B-v3模型的训练，该模型在多语言内容安全基准测试中实现了最先进的性能。研究结果表明，该模型展现了强大的跨语言迁移能力和零样本泛化能力，解决了多语言应用中的安全问题。</div>
</details>
</div>
<div class="card">
<div class="title">Black-box Optimization of LLM Outputs by Asking for Directions</div>
<div class="meta-line">Authors: Jie Zhang, Meng Ding, Yang Liu, Jue Hong, Florian Tramèr</div>
<div class="meta-line">First: 2025-10-19T11:13:45+00:00 · Latest: 2025-10-19T11:13:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.16794v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.16794v1">PDF</a> · <a href="https://github.com/zj-jayzhang/black_box_llm_optimization">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel approach for attacking black-box large language models
(LLMs) by exploiting their ability to express confidence in natural language.
Existing black-box attacks require either access to continuous model outputs
like logits or confidence scores (which are rarely available in practice), or
rely on proxy signals from other models. Instead, we demonstrate how to prompt
LLMs to express their internal confidence in a way that is sufficiently
calibrated to enable effective adversarial optimization. We apply our general
method to three attack scenarios: adversarial examples for vision-LLMs,
jailbreaks and prompt injections. Our attacks successfully generate malicious
inputs against systems that only expose textual outputs, thereby dramatically
expanding the attack surface for deployed LLMs. We further find that better and
larger models exhibit superior calibration when expressing confidence, creating
a concerning security paradox where model capability improvements directly
enhance vulnerability. Our code is available at this
[link](https://github.com/zj-jayzhang/black_box_llm_optimization).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过询问方向对LLM输出进行黑箱优化</div>
<div class="mono" style="margin-top:8px">我们提出了一种新颖的方法，通过利用黑箱大型语言模型（LLMs）在自然语言中表达信心的能力来进行攻击。现有的黑箱攻击要么需要访问连续的模型输出，如logits或置信度分数（在实践中很少可用），要么依赖于其他模型的代理信号。相反，我们展示了如何提示LLMs以一种足够校准的方式表达其内部信心，从而实现有效的对抗优化。我们将我们的一般方法应用于三种攻击场景：视觉LLMs的对抗样本、越狱和提示注入。我们的攻击成功生成了针对仅暴露文本输出的系统的恶意输入，从而显著扩大了已部署LLMs的攻击面。我们进一步发现，更好和更大的模型在表达信心时表现出更优的校准，形成了一个令人担忧的安全悖论，即模型能力的提升直接增强了脆弱性。我们的代码可在此[链接](https://github.com/zj-jayzhang/black_box_llm_optimization)获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of black-box attacks on large language models (LLMs), which typically require access to model outputs like logits or confidence scores that are often unavailable. Previous methods either relied on such inaccessible outputs or used proxy signals from other models, limiting their effectiveness. The proposed approach innovatively prompts LLMs to express their internal confidence in a calibrated manner, enabling effective adversarial optimization without needing direct access to model internals. This method significantly expands the attack surface for deployed LLMs by successfully generating adversarial examples in various scenarios, including vision-LLMs, jailbreaks, and prompt injections. The findings indicate that larger and more capable models exhibit better calibration in expressing confidence, which paradoxically increases their vulnerability, thus highlighting a critical security concern in model development.</div>
<div class="mono" style="margin-top:8px">本文探讨了对大型语言模型（LLMs）进行黑箱攻击的挑战，传统方法通常需要访问连续的模型输出或依赖其他模型的代理信号，这在实际应用中往往不切实际。所提出的方法利用LLMs在自然语言中表达信心的能力，使得在不需要直接访问模型内部的情况下进行有效的对抗优化。该研究的贡献在于展示了通过提示LLMs表述其内部信心，可以在对抗性示例、越狱和提示注入等场景中成功发起攻击。该方法论涉及校准LLMs所表达的信心，以生成针对仅提供文本输出的系统的恶意输入，从而扩大了攻击面。研究结果表明，较大和更强大的模型在表达信心时表现出更好的校准，这反过来又增加了它们的脆弱性，突显了在部署先进LLMs时面临的重大安全隐患。</div>
</details>
</div>
<div class="card">
<div class="title">Active Honeypot Guardrail System: Probing and Confirming Multi-Turn LLM   Jailbreaks</div>
<div class="meta-line">Authors: ChenYu Wu, Yi Wang, Yang Liao</div>
<div class="meta-line">First: 2025-10-16T17:41:09+00:00 · Latest: 2025-10-16T17:41:09+00:00</div>
<div class="meta-line">Comments: 6pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15017v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.15017v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly vulnerable to multi-turn
jailbreak attacks, where adversaries iteratively elicit harmful behaviors that
bypass single-turn safety filters. Existing defenses predominantly rely on
passive rejection, which either fails against adaptive attackers or overly
restricts benign users. We propose a honeypot-based proactive guardrail system
that transforms risk avoidance into risk utilization. Our framework fine-tunes
a bait model to generate ambiguous, non-actionable but semantically relevant
responses, which serve as lures to probe user intent. Combined with the
protected LLM&#x27;s safe reply, the system inserts proactive bait questions that
gradually expose malicious intent through multi-turn interactions. We further
introduce the Honeypot Utility Score (HUS), measuring both the attractiveness
and feasibility of bait responses, and use a Defense Efficacy Rate (DER) for
balancing safety and usability. Initial experiment on MHJ Datasets with recent
attack method across GPT-4o show that our system significantly disrupts
jailbreak success while preserving benign user experience.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>主动蜜罐护栏系统：探测和确认多轮LLM越狱</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）越来越容易受到多轮越狱攻击，攻击者通过迭代引导有害行为，绕过单轮安全过滤器。现有防御主要依赖于被动拒绝，这对适应性攻击者无效，或过度限制良性用户。我们提出了一种基于蜜罐的主动护栏系统，将风险规避转变为风险利用。我们的框架微调了一个诱饵模型，以生成模糊、不可操作但语义相关的响应，作为引诱用户意图的诱饵。结合受保护的LLM的安全回复，该系统插入主动诱饵问题，通过多轮交互逐渐揭示恶意意图。我们进一步引入蜜罐效用评分（HUS），衡量诱饵响应的吸引力和可行性，并使用防御有效率（DER）来平衡安全性和可用性。在MHJ数据集上进行的初步实验显示，我们的系统显著干扰了越狱成功，同时保留了良性用户体验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the increasing vulnerability of large language models (LLMs) to multi-turn jailbreak attacks, where adversaries exploit iterative interactions to bypass safety measures. Previous methods primarily relied on passive rejection strategies, which either failed against adaptive attackers or restricted legitimate users excessively. The proposed honeypot-based proactive guardrail system shifts the focus from risk avoidance to risk utilization by fine-tuning a bait model that generates ambiguous yet relevant responses to probe user intent. This approach is well-motivated as it aims to balance safety and usability, contributing a novel framework that includes the Honeypot Utility Score (HUS) and Defense Efficacy Rate (DER) for evaluating bait responses. Experimental results on the MHJ Datasets demonstrate that the proposed system significantly reduces the success of jailbreak attempts while maintaining a positive experience for benign users.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）在多轮越狱攻击中日益增长的脆弱性，这种攻击通过迭代交互绕过安全过滤器。以往的方法主要采用被动拒绝策略，这要么无法有效对抗适应性攻击者，要么过度限制合法用户。提出的基于蜜罐的主动护栏系统将重点从风险规避转向风险利用，利用诱饵模型生成模糊但相关的响应，以探测用户意图，同时保持安全性。这种方法具有良好的动机，因为它旨在平衡安全性和可用性。该方法论涉及对诱饵模型的微调，并引入主动诱饵问题，以在多轮交互中揭示恶意意图。MHJ数据集上的实验结果表明，该系统显著降低了越狱成功率，同时确保了良性用户的积极体验。</div>
</details>
</div>
<div class="card">
<div class="title">When Style Breaks Safety: Defending LLMs Against Superficial Style   Alignment</div>
<div class="meta-line">Authors: Yuxin Xiao, Sana Tonekaboni, Walter Gerych, Vinith Suriyakumar, Marzyeh Ghassemi</div>
<div class="meta-line">First: 2025-06-09T05:57:39+00:00 · Latest: 2025-10-16T06:50:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.07452v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.07452v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) can be prompted with specific styles (e.g.,
formatting responses as lists), including in malicious queries. Prior jailbreak
research mainly augments these queries with additional string transformations
to maximize attack success rate (ASR). However, the impact of style patterns in
the original queries that are semantically irrelevant to the malicious intent
remains unclear. In this work, we seek to understand whether style patterns
compromise LLM safety, how superficial style alignment increases model
vulnerability, and how best to mitigate these risks during alignment. We first
define ASR inflation as the increase in ASR due to style patterns in existing
jailbreak benchmark queries. By evaluating 32 LLMs across seven benchmarks, we
find that nearly all models exhibit ASR inflation. Notably, the inflation
correlates with an LLM&#x27;s relative attention to style patterns, which also
overlap more with its instruction-tuning data when inflation occurs. We then
investigate superficial style alignment, and find that fine-tuning with
specific styles makes LLMs more vulnerable to jailbreaks of those same styles.
Finally, we propose SafeStyle, a defense strategy that incorporates a small
amount of safety training data augmented to match the distribution of style
patterns in the fine-tuning data. Across three LLMs, six fine-tuning style
settings, and two real-world instruction-tuning datasets, SafeStyle
consistently outperforms baselines in maintaining LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当风格破坏安全性：保护大型语言模型免受表面风格对齐的影响</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）可以通过特定风格（例如，将响应格式化为列表）进行提示，包括在恶意查询中。先前的越狱研究主要通过额外的字符串转换来增强这些查询，以最大化攻击成功率（ASR）。然而，原始查询中与恶意意图语义无关的风格模式的影响仍不清楚。在本研究中，我们旨在了解风格模式是否会危害LLM安全性，表面风格对齐如何增加模型脆弱性，以及在对齐过程中如何最好地减轻这些风险。我们首先将ASR膨胀定义为由于现有越狱基准查询中的风格模式而导致的ASR增加。通过评估32个LLM在七个基准上的表现，我们发现几乎所有模型都表现出ASR膨胀。值得注意的是，膨胀与LLM对风格模式的相对关注度相关，当膨胀发生时，这些模式与其指令调优数据的重叠也更多。然后，我们研究表面风格对齐，发现使用特定风格进行微调使LLM对这些相同风格的越狱更加脆弱。最后，我们提出了SafeStyle，一种防御策略，结合少量安全训练数据，增强以匹配微调数据中的风格模式分布。在三个LLM、六个微调风格设置和两个真实世界的指令调优数据集上，SafeStyle在维护LLM安全性方面始终优于基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the vulnerability of large language models (LLMs) to malicious queries that exploit specific styles, a concern that has emerged with the rise of jailbreak techniques. Previous methods primarily focused on augmenting queries with additional string transformations to enhance attack success rates, but they did not account for the influence of style patterns in the original queries that are unrelated to malicious intent. The proposed approach, SafeStyle, aims to mitigate these risks by incorporating a small amount of safety training data that aligns with the style patterns present in the fine-tuning data. The study evaluates 32 LLMs across seven benchmarks and finds that nearly all models exhibit increased attack success rates due to style patterns, with a correlation between vulnerability and attention to these patterns. SafeStyle demonstrates improved performance in maintaining LLM safety across various fine-tuning settings and instruction-tuning datasets, effectively addressing the identified vulnerabilities.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在恶意查询中利用特定风格模式的脆弱性，这一问题源于之前的越狱研究，主要集中在通过字符串变换增强查询。现有方法未能充分考虑与恶意意图无关的风格模式的影响，导致对这些表面风格对模型安全性影响的理解存在空白。本文的贡献在于定义了ASR膨胀的概念，证明几乎所有评估的LLMs由于风格模式而表现出攻击成功率的增加，并揭示了使用特定风格进行微调会提高对相应越狱的脆弱性。提出的方法SafeStyle通过整合与微调数据的风格分布相匹配的小量安全训练数据来增强LLM的安全性。结果表明，SafeStyle在各种模型和微调设置中始终优于基线方法，有效支持了提高LLM对基于风格攻击的安全性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">When &quot;Competency&quot; in Reasoning Opens the Door to Vulnerability:   Jailbreaking LLMs via Novel Complex Ciphers</div>
<div class="meta-line">Authors: Divij Handa, Zehua Zhang, Amir Saeidi, Shrinidhi Kumbhar, Md Nayem Uddin, Aswin RRV, Chitta Baral</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2024-02-16T11:37:05+00:00 · Latest: 2025-10-14T06:25:21+00:00</div>
<div class="meta-line">Comments: Published in Reliable ML from Unreliable Data workshop @ NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2402.10601v5">Abs</a> · <a href="http://arxiv.org/pdf/2402.10601v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Language Model (LLM) safety have primarily
focused on mitigating attacks crafted in natural language or common ciphers
(e.g. Base64), which are likely integrated into newer models&#x27; safety training.
However, we reveal a paradoxical vulnerability: as LLMs advance in reasoning,
they inadvertently become more susceptible to novel jailbreaking attacks.
Enhanced reasoning enables LLMs to interpret complex instructions and decode
complex user-defined ciphers, creating an exploitable security gap. To study
this vulnerability, we introduce Attacks using Custom Encryptions (ACE), a
jailbreaking technique that encodes malicious queries with novel ciphers.
Extending ACE, we introduce Layered Attacks using Custom Encryptions (LACE),
which applies multi-layer ciphers to amplify attack complexity. Furthermore, we
develop CipherBench, a benchmark designed to evaluate LLMs&#x27; accuracy in
decoding encrypted benign text. Our experiments reveal a critical trade-off:
LLMs that are more capable of decoding ciphers are more vulnerable to LACE,
with success rates on gpt-oss-20b escalating from 60% under ACE to 72% with
LACE. These findings highlight a critical insight: as LLMs become more adept at
deciphering complex user ciphers--many of which cannot be preemptively included
in safety training--they become increasingly exploitable.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the emerging vulnerabilities in Large Language Models (LLMs) as they improve in reasoning capabilities, which inadvertently makes them more susceptible to novel jailbreaking attacks. Previous methods focused on mitigating attacks using natural language or common ciphers, but these approaches fail to account for the complexities introduced by advanced reasoning. The proposed method, Attacks using Custom Encryptions (ACE), along with its extension Layered Attacks using Custom Encryptions (LACE), introduces a novel way to exploit these vulnerabilities by encoding malicious queries with complex ciphers. The paper contributes by revealing a critical trade-off between LLMs&#x27; decoding capabilities and their vulnerability to attacks, demonstrated through experiments that show an increase in success rates from 60% to 72% when using LACE on the gpt-oss-20b model, thus supporting the argument that enhanced reasoning can lead to greater security risks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在推理能力提升时出现的新兴脆弱性，这种提升无意中使其更容易受到新型越狱攻击的影响。以往的方法主要集中在减轻使用自然语言或常见密码的攻击，但这些方法未能考虑到高级推理所带来的复杂性。提出的方法，即自定义加密攻击（ACE）及其扩展层次自定义加密攻击（LACE），有效利用多层密码来利用这一脆弱性。本文的贡献在于引入了CipherBench，一个用于评估LLMs在解码加密良性文本时表现的基准。实验结果表明，脆弱性显著增加，使用LACE时gpt-oss-20b模型的成功率从60%上升至72%，这表明增强的推理能力可能导致更大的安全风险。</div>
</details>
</div>
<div class="card">
<div class="title">Attention-Aware GNN-based Input Defense against Multi-Turn LLM Jailbreak</div>
<div class="meta-line">Authors: Zixuan Huang, Kecheng Huang, Lihao Yin, Bowei He, Huiling Zhen, Mingxuan Yuan, Zili Shao</div>
<div class="meta-line">First: 2025-07-09T07:55:03+00:00 · Latest: 2025-10-14T04:28:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.07146v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.07146v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have gained significant traction in various
applications, yet their capabilities present risks for both constructive and
malicious exploitation. Despite extensive training and fine-tuning efforts
aimed at enhancing safety, LLMs remain susceptible to jailbreak attacks.
Recently, the emergence of multi-turn attacks has intensified this
vulnerability. Unlike single-turn attacks, multi-turn attacks incrementally
escalate dialogue complexity, rendering them more challenging to detect and
mitigate.
  In this study, we introduce G-Guard, an innovative attention-aware Graph
Neural Network (GNN)-based input classifier specifically designed to defend
against multi-turn jailbreak attacks targeting LLMs. G-Guard constructs an
entity graph for multi-turn queries, which captures the interrelationships
between queries and harmful keywords that present in multi-turn queries.
Furthermore, we propose an attention-aware augmentation mechanism that
retrieves the most relevant single-turn query based on the ongoing multi-turn
conversation. The retrieved query is incorporated as a labeled node within the
graph, thereby enhancing the GNN&#x27;s capacity to classify the current query as
harmful or benign. Evaluation results show that G-Guard consistently
outperforms all baselines across diverse datasets and evaluation metrics,
demonstrating its efficacy as a robust defense mechanism against multi-turn
jailbreak attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于注意力的GNN输入防御多轮LLM越狱攻击</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在各种应用中获得了显著关注，但其能力也带来了建设性和恶意利用的风险。尽管进行了广泛的训练和微调以增强安全性，LLMs仍然容易受到越狱攻击。最近，多轮攻击的出现加剧了这一脆弱性。与单轮攻击不同，多轮攻击逐步增加对话复杂性，使其更难以检测和缓解。在本研究中，我们介绍了G-Guard，一种创新的基于注意力的图神经网络（GNN）输入分类器，专门设计用于防御针对LLMs的多轮越狱攻击。G-Guard为多轮查询构建了一个实体图，捕捉查询与多轮查询中存在的有害关键词之间的相互关系。此外，我们提出了一种基于注意力的增强机制，根据正在进行的多轮对话检索最相关的单轮查询。检索到的查询作为标记节点纳入图中，从而增强GNN将当前查询分类为有害或良性的能力。评估结果表明，G-Guard在各种数据集和评估指标上始终优于所有基线，证明其作为多轮越狱攻击的强大防御机制的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concern of jailbreak attacks on Large Language Models (LLMs), particularly focusing on the more complex multi-turn attacks that are harder to detect than single-turn attacks. Previous methods have struggled to effectively mitigate these vulnerabilities due to their inability to capture the evolving context of multi-turn dialogues. The proposed approach, G-Guard, utilizes an attention-aware Graph Neural Network (GNN) to create an entity graph that represents the relationships between queries and harmful keywords, along with an attention-aware augmentation mechanism to enhance classification accuracy. This method significantly improves the detection of harmful queries in multi-turn contexts. The experimental results indicate that G-Guard outperforms existing baselines across various datasets and metrics, confirming its effectiveness as a defense mechanism against multi-turn jailbreak attacks.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在监狱突破攻击中的脆弱性，特别是更复杂的多轮攻击，这些攻击比单轮攻击更难以检测。以往的方法在有效缓解这些多轮攻击方面存在困难，因为它们的逐步性质和识别有害查询的难度。所提出的方法G-Guard利用注意力感知的图神经网络（GNN）通过构建一个捕捉查询与有害关键词之间关系的实体图来对输入进行分类，并结合注意力感知的增强机制以提高分类准确性。该方法的动机明确，直接针对现有防御的不足。本文贡献了一种新颖的防御机制，在各种数据集和评估指标上显著优于基线方法，证明了其在保护LLMs免受多轮监狱突破攻击方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses   Against Llm Jailbreaks and Prompt Injections</div>
<div class="meta-line">Authors: Milad Nasr, Nicholas Carlini, Chawin Sitawarin, Sander V. Schulhoff, Jamie Hayes, Michael Ilie, Juliette Pluto, Shuang Song, Harsh Chaudhari, Ilia Shumailov, Abhradeep Thakurta, Kai Yuanqing Xiao, Andreas Terzis, Florian Tramèr</div>
<div class="meta-line">First: 2025-10-10T05:51:04+00:00 · Latest: 2025-10-10T05:51:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09023v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How should we evaluate the robustness of language model defenses? Current
defenses against jailbreaks and prompt injections (which aim to prevent an
attacker from eliciting harmful knowledge or remotely triggering malicious
actions, respectively) are typically evaluated either against a static set of
harmful attack strings, or against computationally weak optimization methods
that were not designed with the defense in mind. We argue that this evaluation
process is flawed.
  Instead, we should evaluate defenses against adaptive attackers who
explicitly modify their attack strategy to counter a defense&#x27;s design while
spending considerable resources to optimize their objective. By systematically
tuning and scaling general optimization techniques-gradient descent,
reinforcement learning, random search, and human-guided exploration-we bypass
12 recent defenses (based on a diverse set of techniques) with attack success
rate above 90% for most; importantly, the majority of defenses originally
reported near-zero attack success rates. We believe that future defense work
must consider stronger attacks, such as the ones we describe, in order to make
reliable and convincing claims of robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>攻击者第二步行动：更强的自适应攻击绕过针对 LLM 监狱破解和提示注入的防御</div>
<div class="mono" style="margin-top:8px">我们应该如何评估语言模型防御的稳健性？当前针对监狱破解和提示注入的防御（旨在防止攻击者引发有害知识或远程触发恶意行为）通常是针对一组静态的有害攻击字符串，或针对未考虑防御设计的计算弱优化方法进行评估。我们认为这一评估过程存在缺陷。相反，我们应该针对自适应攻击者进行评估，他们明确修改攻击策略以对抗防御设计，同时花费大量资源来优化目标。通过系统地调整和扩展一般优化技术——梯度下降、强化学习、随机搜索和人类引导探索——我们绕过了12种最近的防御（基于多种技术），大多数情况下攻击成功率超过90%；重要的是，大多数防御最初报告的攻击成功率接近零。我们认为，未来的防御工作必须考虑更强的攻击，例如我们所描述的，以便做出可靠和令人信服的稳健性声明。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the inadequacies in evaluating the robustness of language model defenses against jailbreaks and prompt injections, which are critical for preventing harmful actions. Previous methods relied on static attack strings or weak optimization techniques, failing to account for adaptive attackers who can modify their strategies. The proposed approach emphasizes the need to evaluate defenses against these adaptive attackers, utilizing advanced optimization techniques such as gradient descent and reinforcement learning. The contribution of this paper lies in demonstrating that by employing these methods, the authors successfully bypassed 12 recent defenses with an attack success rate exceeding 90%, highlighting the necessity for future defense strategies to consider stronger adaptive attacks to validate their effectiveness.</div>
<div class="mono" style="margin-top:8px">本文探讨了评估语言模型防御措施在抵御越狱和提示注入方面的有效性时存在的不足，这对于防止有害行为至关重要。以往的方法依赖于静态攻击字符串或弱优化技术，这些方法未能考虑攻击者的适应性，导致对防御效果的评估存在误导。提出的方法强调需要针对能够修改策略并优化目标的适应性攻击者评估防御措施，从而提供更现实的鲁棒性衡量标准。作者系统地采用了包括梯度下降和强化学习在内的多种优化技术，成功绕过了12种最近的防御措施，攻击成功率超过90%，而这些防御措施原本报告的成功率接近于零。这项工作强调了未来防御评估必须考虑更强的适应性攻击，以支持鲁棒性声明的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Machine Learning for Detection and Analysis of Novel LLM Jailbreaks</div>
<div class="meta-line">Authors: John Hawkins, Aditya Pramar, Rodney Beard, Rohitash Chandra</div>
<div class="meta-line">First: 2025-10-02T03:55:29+00:00 · Latest: 2025-10-10T05:08:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.01644v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.01644v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) suffer from a range of vulnerabilities that
allow malicious users to solicit undesirable responses through manipulation of
the input text. These so-called jailbreak prompts are designed to trick the LLM
into circumventing the safety guardrails put in place to keep responses
acceptable to the developer&#x27;s policies. In this study, we analyse the ability
of different machine learning models to distinguish jailbreak prompts from
genuine uses, including looking at our ability to identify jailbreaks that use
previously unseen strategies. Our results indicate that using current datasets
the best performance is achieved by fine tuning a Bidirectional Encoder
Representations from Transformers (BERT) model end-to-end for identifying
jailbreaks. We visualise the keywords that distinguish jailbreak from genuine
prompts and conclude that explicit reflexivity in prompt structure could be a
signal of jailbreak intention.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于检测和分析新型 LLM 越狱的机器学习</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）存在一系列漏洞，允许恶意用户通过操控输入文本来获取不良响应。这些所谓的越狱提示旨在欺骗 LLM 绕过为保持响应符合开发者政策而设立的安全防护措施。在本研究中，我们分析了不同机器学习模型区分越狱提示与真实使用的能力，包括识别使用以前未见策略的越狱。我们的结果表明，使用当前数据集，通过端到端微调双向编码器表示的变换器（BERT）模型来识别越狱的最佳性能得以实现。我们可视化了区分越狱与真实提示的关键词，并得出结论，提示结构中的显式反身性可能是越狱意图的一个信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of Large Language Models (LLMs) that can be exploited by malicious users through specially crafted inputs known as jailbreak prompts, which bypass safety measures. Previous methods struggled to effectively differentiate between these prompts and legitimate uses, particularly when faced with novel strategies. The proposed approach enhances detection by fine-tuning a Bidirectional Encoder Representations from Transformers (BERT) model specifically for this task, addressing the limitations of existing techniques. This study contributes to the field by providing insights into the characteristics of jailbreak prompts and demonstrating that explicit reflexivity in prompt structure can indicate malicious intent. The methodology employed shows that the fine-tuned BERT model achieves superior performance in identifying jailbreaks, suggesting its effectiveness in supporting the goal of improving LLM safety.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）的脆弱性，这些脆弱性可以被恶意用户通过特制的输入提示（称为越狱提示）利用，旨在绕过安全措施。以往的方法在有效区分越狱提示和合法使用方面存在困难，往往无法识别攻击者采用的新策略。本研究提出了一种新方法，通过端到端微调双向编码器表示的变换器（BERT）模型，显著提高了越狱提示的识别能力。论文的贡献在于分析了多种机器学习模型，并可视化了表明越狱意图的关键词，表明提示结构中的明确反身性可能是恶意意图的信号。所提出的方法在检测越狱提示方面表现优异，支持了增强LLMs安全性和可靠性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through   Formal Logical Expression</div>
<div class="meta-line">Authors: Jingyu Peng, Maolin Wang, Nan Wang, Jiatong Li, Yuchen Li, Yuyang Ye, Wanyu Wang, Pengyue Jia, Kai Zhang, Xiangyu Zhao</div>
<div class="meta-line">First: 2025-05-18T04:23:51+00:00 · Latest: 2025-10-09T06:29:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.13527v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.13527v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite substantial advancements in aligning large language models (LLMs)
with human values, current safety mechanisms remain susceptible to jailbreak
attacks. We hypothesize that this vulnerability stems from distributional
discrepancies between alignment-oriented prompts and malicious prompts. To
investigate this, we introduce LogiBreak, a novel and universal black-box
jailbreak method that leverages logical expression translation to circumvent
LLM safety systems. By converting harmful natural language prompts into formal
logical expressions, LogiBreak exploits the distributional gap between
alignment data and logic-based inputs, preserving the underlying semantic
intent and readability while evading safety constraints. We evaluate LogiBreak
on a multilingual jailbreak dataset spanning three languages, demonstrating its
effectiveness across various evaluation settings and linguistic contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>逻辑越狱：通过形式逻辑表达高效解锁大型语言模型安全限制</div>
<div class="mono" style="margin-top:8px">尽管在将大型语言模型（LLMs）与人类价值观对齐方面取得了重大进展，但当前的安全机制仍然容易受到越狱攻击。我们假设这种脆弱性源于对齐导向提示与恶意提示之间的分布差异。为此，我们引入了LogiBreak，这是一种新颖且通用的黑箱越狱方法，利用逻辑表达翻译来规避LLM安全系统。通过将有害的自然语言提示转换为形式逻辑表达，LogiBreak利用对齐数据与基于逻辑的输入之间的分布差距，保留潜在的语义意图和可读性，同时规避安全约束。我们在涵盖三种语言的多语言越狱数据集上评估LogiBreak，展示了其在各种评估设置和语言环境中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the ongoing challenge of ensuring the safety of large language models (LLMs) against jailbreak attacks, which exploit vulnerabilities in current alignment mechanisms. Previous methods have struggled with distributional discrepancies between prompts designed for alignment and those intended for malicious use, leading to ineffective safety measures. The proposed approach, LogiBreak, introduces a novel black-box jailbreak method that translates harmful natural language prompts into formal logical expressions, effectively bridging the gap between alignment-oriented and logic-based inputs. This method is well-motivated as it maintains semantic intent while bypassing safety constraints. The paper contributes by demonstrating LogiBreak&#x27;s effectiveness on a multilingual jailbreak dataset across three languages, achieving significant performance improvements in various evaluation settings and confirming its capability to support the goal of enhancing LLM safety.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）在面对越狱攻击时的脆弱性进行探讨，这种脆弱性源于对齐导向提示与恶意提示之间的差异。以往的方法在有效绕过安全机制方面存在困难，往往无法保持语义意图或可读性。提出的方法LogiBreak引入了一种新颖的黑箱越狱方法，通过将有害的自然语言提示转换为形式逻辑表达式，从而利用对齐数据与基于逻辑的输入之间的分布差距。该方法的提出具有良好的动机，旨在增强LLMs对这些攻击的鲁棒性。本文的贡献在于展示LogiBreak在跨三种语言的多语言越狱数据集上的有效性，在各种评估设置和语言环境中取得了显著的性能提升，从而支持了改善LLM安全机制的目标。</div>
</details>
</div>
<div class="card">
<div class="title">$\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM   Systems with Optimized Prompt Attacks</div>
<div class="meta-line">Authors: Rana Muhammad Shahroz Khan, Zhen Tan, Sukwon Yun, Charles Fleming, Tianlong Chen</div>
<div class="meta-line">First: 2025-03-31T20:43:56+00:00 · Latest: 2025-10-08T22:17:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.00218v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.00218v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most discussions about Large Language Model (LLM) safety have focused on
single-agent settings but multi-agent LLM systems now create novel adversarial
risks because their behavior depends on communication between agents and
decentralized reasoning. In this work, we innovatively focus on attacking
pragmatic systems that have constrains such as limited token bandwidth, latency
between message delivery, and defense mechanisms. We design a
$\textit{permutation-invariant adversarial attack}$ that optimizes prompt
distribution across latency and bandwidth-constraint network topologies to
bypass distributed safety mechanisms within the system. Formulating the attack
path as a problem of $\textit{maximum-flow minimum-cost}$, coupled with the
novel $\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage
graph-based optimization to maximize attack success rate while minimizing
detection risk. Evaluating across models including $\texttt{Llama}$,
$\texttt{Mistral}$, $\texttt{Gemma}$, $\texttt{DeepSeek}$ and other variants on
various datasets like $\texttt{JailBreakBench}$ and
$\texttt{AdversarialBench}$, our method outperforms conventional attacks by up
to $7\times$, exposing critical vulnerabilities in multi-agent systems.
Moreover, we demonstrate that existing defenses, including variants of
$\texttt{Llama-Guard}$ and $\texttt{PromptGuard}$, fail to prohibit our attack,
emphasizing the urgent need for multi-agent specific safety mechanisms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>《被围攻的代理人》：通过优化提示攻击打破务实的多代理大型语言模型系统</div>
<div class="mono" style="margin-top:8px">关于大型语言模型（LLM）安全性的讨论大多集中在单代理设置上，但多代理LLM系统现在创造了新的对抗风险，因为它们的行为依赖于代理之间的通信和去中心化推理。在这项工作中，我们创新性地关注于攻击具有限制的务实系统，例如有限的令牌带宽、消息传递延迟和防御机制。我们设计了一种“置换不变对抗攻击”，优化延迟和带宽受限网络拓扑中的提示分布，以绕过系统内的分布式安全机制。将攻击路径公式化为“最大流最小成本”问题，结合新颖的“置换不变规避损失（PIEL）”，我们利用基于图的优化来最大化攻击成功率，同时最小化检测风险。在包括“Llama”、“Mistral”、“Gemma”、“DeepSeek”等模型以及“JailBreakBench”和“AdversarialBench”等各种数据集上的评估中，我们的方法比传统攻击提高了多达7倍，暴露了多代理系统中的关键漏洞。此外，我们证明现有的防御措施，包括“Llama-Guard”和“PromptGuard”的变体，无法阻止我们的攻击，强调了对多代理特定安全机制的迫切需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the emerging safety concerns associated with multi-agent large language model (LLM) systems, which present unique adversarial risks due to their inter-agent communication and decentralized reasoning. Previous methods primarily focused on single-agent scenarios and did not adequately account for the complexities of multi-agent interactions, leading to vulnerabilities that could be exploited. The authors propose a novel permutation-invariant adversarial attack that optimizes prompt distribution within bandwidth and latency constraints, effectively bypassing existing safety mechanisms. The methodology involves formulating the attack as a maximum-flow minimum-cost problem and utilizing a new Permutation-Invariant Evasion Loss (PIEL) to enhance attack success rates while reducing detection risks. Experimental results demonstrate that their approach significantly outperforms traditional attacks by up to seven times across various models and datasets, revealing critical weaknesses in multi-agent systems and highlighting the inadequacy of current defense strategies.</div>
<div class="mono" style="margin-top:8px">本研究关注多智能体大型语言模型（LLM）系统所面临的新兴对抗风险，这些风险与传统的单智能体模型不同，因为它们依赖于智能体之间的通信和去中心化推理。以往的方法主要集中在单智能体安全性上，导致多智能体系统容易受到新型攻击。提出的方法引入了一种置换不变的对抗攻击，优化了在延迟和带宽受限的网络拓扑中的提示分布，有效绕过了现有的安全机制。该研究将攻击形式化为最大流最小成本问题，并采用了一种新的损失函数——置换不变规避损失（PIEL），利用图优化技术提高攻击成功率，同时降低检测风险。在对Llama、Mistral和Gemma等模型以及JailBreakBench和AdversarialBench等数据集的实验评估中，该方法的表现比传统攻击提高了多达七倍，揭示了多智能体系统中的重大漏洞，并强调了当前防御策略的不足。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Psycho-Lexical Approach for Constructing Value Systems in   Large Language Models</div>
<div class="meta-line">Authors: Haoran Ye, Tianze Zhang, Yuhang Xie, Liyuan Zhang, Yuanyi Ren, Xin Zhang, Guojie Song</div>
<div class="meta-line">Venue: ACL 2025</div>
<div class="meta-line">First: 2025-02-04T16:10:55+00:00 · Latest: 2025-10-07T14:57:19+00:00</div>
<div class="meta-line">Comments: ACL 2025 Main</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.02444v6">Abs</a> · <a href="http://arxiv.org/pdf/2502.02444v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Values are core drivers of individual and collective perception, cognition,
and behavior. Value systems, such as Schwartz&#x27;s Theory of Basic Human Values,
delineate the hierarchy and interplay among these values, enabling
cross-disciplinary investigations into decision-making and societal dynamics.
Recently, the rise of Large Language Models (LLMs) has raised concerns
regarding their elusive intrinsic values. Despite growing efforts in
evaluating, understanding, and aligning LLM values, a psychologically grounded
LLM value system remains underexplored. This study addresses the gap by
introducing the Generative Psycho-Lexical Approach (GPLA), a scalable,
adaptable, and theoretically informed method for constructing value systems.
Leveraging GPLA, we propose a psychologically grounded five-factor value system
tailored for LLMs. For systematic validation, we present three benchmarking
tasks that integrate psychological principles with cutting-edge AI priorities.
Our results reveal that the proposed value system meets standard psychological
criteria, better captures LLM values, improves LLM safety prediction, and
enhances LLM alignment, when compared to the canonical Schwartz&#x27;s values.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成心理词汇方法构建大型语言模型中的价值体系</div>
<div class="mono" style="margin-top:8px">价值观是个体和集体感知、认知和行为的核心驱动因素。价值体系，如施瓦茨的基本人类价值理论，描绘了这些价值观之间的层次和相互作用，使跨学科的决策和社会动态研究成为可能。最近，大型语言模型（LLMs）的兴起引发了对其难以捉摸的内在价值的担忧。尽管在评估、理解和对齐LLM价值方面的努力不断增加，但基于心理学的LLM价值体系仍然未得到充分探索。本研究通过引入生成心理词汇方法（GPLA）来填补这一空白，GPLA是一种可扩展、可适应且理论上有依据的构建价值体系的方法。利用GPLA，我们提出了一个为LLM量身定制的基于心理学的五因素价值体系。为了进行系统验证，我们提出了三个基准任务，将心理学原理与前沿AI优先事项相结合。我们的结果表明，所提出的价值体系符合标准心理学标准，更好地捕捉了LLM的价值，提高了LLM的安全预测，并增强了LLM的对齐，与经典的施瓦茨价值相比。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for a psychologically grounded value system in Large Language Models (LLMs), as existing methods have not adequately explored the intrinsic values of these models. Previous approaches, such as Schwartz&#x27;s Theory of Basic Human Values, lack adaptability and scalability for LLMs, leading to insufficient alignment and safety predictions. The proposed Generative Psycho-Lexical Approach (GPLA) overcomes these limitations by providing a theoretically informed and flexible framework for constructing value systems specifically designed for LLMs. This paper contributes by introducing a five-factor value system that aligns with psychological principles and demonstrates its effectiveness through three benchmarking tasks. The results indicate that the GPLA-based value system not only meets established psychological criteria but also significantly enhances the alignment and safety prediction of LLMs compared to traditional methods.</div>
<div class="mono" style="margin-top:8px">本文探讨了在大型语言模型（LLMs）中构建心理学基础的价值体系的必要性，因为现有方法未能充分捕捉这些模型的内在价值。以施瓦茨的基本人类价值理论为代表的过去方法缺乏适应性和可扩展性，导致对齐和安全预测不足。提出的生成心理词汇方法（GPLA）提供了一种新颖的框架，将心理学原理与人工智能优先事项相结合，形成了专为LLMs设计的五因素价值体系。该方法通过三个基准任务进行系统验证，结果表明，基于GPLA的价值体系符合既定心理学标准，并显著提高了LLM的安全预测和对齐能力，相较于传统方法表现更佳。总体而言，研究结果支持了该方法在构建LLMs强大价值体系方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Sampling-aware Adversarial Attacks Against Large Language Models</div>
<div class="meta-line">Authors: Tim Beyer, Yan Scholten, Leo Schwinn, Stephan Günnemann</div>
<div class="meta-line">First: 2025-07-06T16:13:33+00:00 · Latest: 2025-10-06T09:02:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.04446v3">Abs</a> · <a href="http://arxiv.org/pdf/2507.04446v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To guarantee safe and robust deployment of large language models (LLMs) at
scale, it is critical to accurately assess their adversarial robustness.
Existing adversarial attacks typically target harmful responses in single-point
greedy generations, overlooking the inherently stochastic nature of LLMs and
overestimating robustness. We show that for the goal of eliciting harmful
responses, repeated sampling of model outputs during the attack complements
prompt optimization and serves as a strong and efficient attack vector. By
casting attacks as a resource allocation problem between optimization and
sampling, we determine compute-optimal trade-offs and show that integrating
sampling into existing attacks boosts success rates by up to 37\% and improves
efficiency by up to two orders of magnitude. We further analyze how
distributions of output harmfulness evolve during an adversarial attack,
discovering that many common optimization strategies have little effect on
output harmfulness. Finally, we introduce a label-free proof-of-concept
objective based on entropy maximization, demonstrating how our sampling-aware
perspective enables new optimization targets. Overall, our findings establish
the importance of sampling in attacks to accurately assess and strengthen LLM
safety at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对大型语言模型的采样感知对抗攻击</div>
<div class="mono" style="margin-top:8px">为了确保大型语言模型（LLMs）在规模上的安全和稳健部署，准确评估其对抗鲁棒性至关重要。现有的对抗攻击通常针对单点贪婪生成中的有害响应，忽视了LLMs固有的随机性，并高估了鲁棒性。我们表明，在引发有害响应的目标下，攻击期间对模型输出的重复采样补充了提示优化，并作为一种强大而高效的攻击向量。通过将攻击视为优化与采样之间的资源分配问题，我们确定了计算最优的权衡，并显示将采样整合到现有攻击中可以将成功率提高多达37\%并将效率提高两个数量级。我们进一步分析了对抗攻击期间输出有害性分布的演变，发现许多常见的优化策略对输出有害性几乎没有影响。最后，我们引入了一种基于熵最大化的无标签概念验证目标，展示了我们的采样感知视角如何启用新的优化目标。总体而言，我们的研究结果确立了采样在攻击中评估和增强LLM安全性的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical need for accurately assessing the adversarial robustness of large language models (LLMs) to ensure their safe deployment. Previous methods primarily focused on single-point greedy generations, which failed to account for the stochastic nature of LLMs, leading to an overestimation of their robustness. The proposed approach integrates repeated sampling of model outputs with prompt optimization, framing attacks as a resource allocation problem that balances optimization and sampling. This method significantly enhances the success rates of adversarial attacks by up to 37% and improves efficiency by two orders of magnitude. The paper contributes to the understanding of how harmful output distributions change during attacks and introduces a novel label-free objective based on entropy maximization, highlighting the importance of sampling in evaluating and enhancing LLM safety at scale.</div>
<div class="mono" style="margin-top:8px">本研究解决了准确评估大型语言模型（LLMs）对抗鲁棒性的重要需求，以确保其安全部署。以往的方法主要集中在单点贪婪生成上，未能考虑LLMs的随机特性，导致对其鲁棒性的高估。所提出的方法将模型输出的重复采样与提示优化相结合，将攻击框架视为优化与采样之间的资源分配问题。该方法显著提高了对抗攻击的有效性，成功率提高了最多37%，效率提升了两个数量级。研究贡献在于揭示了攻击过程中有害输出分布的演变，并引入了一种基于熵最大化的新型无标签目标，强调了采样在评估和提升LLM安全性中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Chasing Moving Targets with Online Self-Play Reinforcement Learning for   Safer Language Models</div>
<div class="meta-line">Authors: Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, Natasha Jaques</div>
<div class="meta-line">First: 2025-06-09T06:35:12+00:00 · Latest: 2025-10-06T03:42:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.07468v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.07468v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过在线自我对弈强化学习追踪移动目标以提高语言模型的安全性</div>
<div class="mono" style="margin-top:8px">传统的语言模型（LM）安全对齐依赖于反应性、分离的程序：攻击者利用静态模型，随后进行防御性微调以修补暴露的漏洞。这种顺序方法造成了不匹配——攻击者过度拟合过时的防御，而防御者则始终滞后于新出现的威胁。为了解决这个问题，我们提出了Self-RedTeam，一种在线自我对弈强化学习算法，其中攻击者和防御者代理通过持续互动共同进化。我们将安全对齐视为一个双人零和游戏，其中单个模型在攻击者和防御者角色之间交替——生成对抗性提示并对其进行保护——同时奖励语言模型裁定结果。这使得动态共同适应成为可能。基于零和游戏的博弈论框架，我们建立了理论安全保证，这激励了我们方法的设计：如果自我对弈收敛到纳什均衡，防御者将可靠地产生对任何对抗性输入的安全响应。在实证上，Self-RedTeam发现了比针对静态防御者训练的攻击者更具多样性的攻击（+21.8% SBERT），并在安全基准上实现了更高的鲁棒性（例如，在WildJailBreak上+65.5%），相比于针对静态攻击者训练的防御者。我们进一步提出了隐藏的思维链，允许代理私下规划，从而提高对抗性多样性并减少过度拒绝。我们的结果激励了从反应性修补转向主动共同进化的LM安全训练，使得通过多代理强化学习（MARL）实现可扩展、自主和鲁棒的自我改进成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of conventional language model safety alignment, which relies on a reactive approach that often leaves defenders lagging behind attackers. Previous methods involve static models that become vulnerable to evolving threats, leading to a mismatch in defenses. The proposed Self-RedTeam introduces an online self-play reinforcement learning algorithm where an attacker and defender co-evolve, allowing for dynamic adaptation and a theoretical safety guarantee based on Nash Equilibrium. This methodology enables the model to generate adversarial prompts while safeguarding against them, resulting in a 21.8% increase in attack diversity and a 65.5% improvement in robustness on safety benchmarks compared to traditional methods. The findings support a shift towards proactive co-evolution in language model safety training, enhancing the scalability and autonomy of safety mechanisms through multi-agent reinforcement learning.</div>
<div class="mono" style="margin-top:8px">本研究解决了传统语言模型安全对齐的局限性，传统方法通常采用反应性和分离的方式，导致攻击者和防御者之间存在持续的差距。以往的方法依赖于静态模型，导致攻击者利用过时的防御措施，而防御者则滞后。提出的Self-RedTeam引入了一种在线自我对抗强化学习算法，使攻击者和防御者代理能够通过持续互动共同进化，将安全对齐框定为一个双人零和博弈。该方法在博弈论框架下建立了理论安全保证，确保如果自我对抗收敛到纳什均衡，防御者将始终产生安全的响应。该方法在实验中显示出显著的改进，发现了21.8%更多的多样化攻击，并在安全基准测试中实现了65.5%的更高鲁棒性，从而支持了语言模型安全训练中主动共同进化的目标。</div>
</details>
</div>
<div class="card">
<div class="title">OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost   Always!</div>
<div class="meta-line">Authors: Jingdi Lei, Varun Gumma, Rishabh Bhardwaj, Seok Min Lim, Chuan Li, Amir Zadeh, Soujanya Poria</div>
<div class="meta-line">First: 2025-09-30T16:39:17+00:00 · Latest: 2025-10-03T12:46:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.26495v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.26495v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model (LLM) safety is one of the most pressing challenges for
enabling wide-scale deployment. While most studies and global discussions focus
on generic harms, such as models assisting users in harming themselves or
others, enterprises face a more fundamental concern: whether LLM-based agents
are safe for their intended use case. To address this, we introduce operational
safety, defined as an LLM&#x27;s ability to appropriately accept or refuse user
queries when tasked with a specific purpose. We further propose OffTopicEval,
an evaluation suite and benchmark for measuring operational safety both in
general and within specific agentic use cases. Our evaluations on six model
families comprising 20 open-weight LLMs reveal that while performance varies
across models, all of them remain highly operationally unsafe. Even the
strongest models - Qwen-3 (235B) with 77.77% and Mistral (24B) with 79.96% -
fall far short of reliable operational safety, while GPT models plateau in the
62-73% range, Phi achieves only mid-level scores (48-70%), and Gemma and
Llama-3 collapse to 39.53% and 23.84%, respectively. While operational safety
is a core model alignment issue, to suppress these failures, we propose
prompt-based steering methods: query grounding (Q-ground) and system-prompt
grounding (P-ground), which substantially improve OOD refusal. Q-ground
provides consistent gains of up to 23%, while P-ground delivers even larger
boosts, raising Llama-3.3 (70B) by 41% and Qwen-3 (30B) by 27%. These results
highlight both the urgent need for operational safety interventions and the
promise of prompt-based steering as a first step toward more reliable LLM-based
agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OffTopicEval：当大型语言模型进入错误的聊天时，几乎总是如此！</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）安全性是实现大规模部署面临的最紧迫挑战之一。虽然大多数研究和全球讨论集中在一般性危害上，例如模型协助用户自我伤害或伤害他人，但企业面临更根本的担忧：基于LLM的代理是否安全用于其预期用途。为了解决这个问题，我们引入了操作安全性，定义为LLM在特定目的下适当地接受或拒绝用户查询的能力。我们进一步提出了OffTopicEval，一个评估套件和基准，用于测量一般和特定代理使用案例中的操作安全性。我们对六个模型家族中20个开放权重LLM的评估显示，尽管模型之间的性能差异很大，但它们都高度不安全。即使是最强的模型——Qwen-3（235B）得分77.77%和Mistral（24B）得分79.96%——也远未达到可靠的操作安全性，而GPT模型的得分停留在62-73%范围内，Phi仅获得中等得分（48-70%），Gemma和Llama-3分别降至39.53%和23.84%。虽然操作安全性是核心模型对齐问题，为了抑制这些失败，我们提出了基于提示的引导方法：查询基础（Q-ground）和系统提示基础（P-ground），显著提高了OOD拒绝率。Q-ground提供了高达23%的持续增益，而P-ground则提供了更大的提升，将Llama-3.3（70B）提高41%，将Qwen-3（30B）提高27%。这些结果突显了对操作安全性干预的迫切需求，以及基于提示的引导作为朝着更可靠的基于LLM的代理迈出的第一步的前景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical challenge of safety in deploying Large Language Models (LLMs), particularly focusing on operational safety, which is the ability of LLMs to appropriately accept or refuse user queries for specific tasks. Previous methods primarily concentrated on generic harms without considering the specific use cases, leading to a lack of reliable safety in operational contexts. The proposed approach, OffTopicEval, introduces an evaluation suite and benchmark for assessing operational safety, revealing that all tested models exhibit significant operational safety issues. The methodology includes prompt-based steering techniques, specifically query grounding (Q-ground) and system-prompt grounding (P-ground), which enhance out-of-distribution refusal rates. The experiments demonstrate that these methods can improve operational safety significantly, with Q-ground achieving gains of up to 23% and P-ground yielding even higher improvements, indicating a promising direction for developing safer LLM-based agents.</div>
<div class="mono" style="margin-top:8px">本研究关注于大语言模型（LLM）部署中的安全性问题，特别是操作安全性，即LLM在特定目的下适当地接受或拒绝用户查询的能力。以往的方法主要集中在一般性危害上，而未能充分评估LLM在其预期应用中的适用性，导致了显著的操作安全性问题。提出的方法OffTopicEval引入了一套专门设计的评估工具和基准，以测量各种LLM的操作安全性。该方法论评估了来自六个模型系列的20个开放权重LLM，结果显示所有模型均表现出高度的操作不安全性，即使是表现最好的模型也未达到可接受的安全阈值。研究还提出了基于提示的引导方法，如查询引导和系统提示引导，这些方法显著提高了分布外拒绝率，对于某些模型的性能提升达到41%，从而强调了在LLM应用中进行操作安全干预的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs</div>
<div class="meta-line">Authors: Marco Arazzi, Vignesh Kumar Kembu, Antonino Nocera, Vinod P</div>
<div class="meta-line">First: 2025-04-30T14:44:24+00:00 · Latest: 2025-10-03T09:07:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.21700v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.21700v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models are fundamental actors in the modern IT landscape
dominated by AI solutions. However, security threats associated with them might
prevent their reliable adoption in critical application scenarios such as
government organizations and medical institutions. For this reason, commercial
LLMs typically undergo a sophisticated censoring mechanism to eliminate any
harmful output they could possibly produce. In response to this, LLM
Jailbreaking is a significant threat to such protections, and many previous
approaches have already demonstrated its effectiveness across diverse domains.
Existing jailbreak proposals mostly adopt a generate-and-test strategy to craft
malicious input. To improve the comprehension of censoring mechanisms and
design a targeted jailbreak attack, we propose an Explainable-AI solution that
comparatively analyzes the behavior of censored and uncensored models to derive
unique exploitable alignment patterns. Then, we propose XBreaking, a novel
jailbreak attack that exploits these unique patterns to break the security
constraints of LLMs by targeted noise injection. Our thorough experimental
campaign returns important insights about the censoring mechanisms and
demonstrates the effectiveness and performance of our attack.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>XBreaking：可解释的人工智能用于破解大型语言模型</div>
<div class="mono" style="margin-top:8px">大型语言模型是现代IT环境中由AI解决方案主导的基本参与者。然而，与之相关的安全威胁可能会阻碍它们在政府机构和医疗机构等关键应用场景中的可靠采用。因此，商业大型语言模型通常会经过复杂的审查机制，以消除它们可能产生的任何有害输出。对此，LLM破解对这些保护措施构成了重大威胁，许多先前的方法已经在不同领域展示了其有效性。现有的破解提案主要采用生成和测试策略来制作恶意输入。为了提高对审查机制的理解并设计针对性的破解攻击，我们提出了一种可解释的AI解决方案，比较分析审查和未审查模型的行为，以推导出独特的可利用对齐模式。然后，我们提出了XBreaking，这是一种新颖的破解攻击，利用这些独特模式通过有针对性的噪声注入来突破大型语言模型的安全约束。我们全面的实验活动提供了关于审查机制的重要见解，并展示了我们攻击的有效性和性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the security vulnerabilities of Large Language Models (LLMs), which are increasingly used in critical applications but face threats from jailbreaking that can undermine their censoring mechanisms. Previous methods primarily utilized a generate-and-test strategy for crafting malicious inputs, which often lacked a comprehensive understanding of the underlying censoring processes. The proposed approach, XBreaking, leverages Explainable AI to analyze and compare the behaviors of censored and uncensored models, identifying exploitable alignment patterns that can be targeted for jailbreak attacks. This methodology not only enhances the understanding of censoring mechanisms but also effectively demonstrates a novel attack through targeted noise injection. The experimental results indicate that XBreaking successfully circumvents security constraints of LLMs, providing valuable insights into their vulnerabilities and supporting the need for improved protective measures.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）的安全漏洞，这些模型在关键应用中越来越多地被使用，但面临着破坏其审查机制的越狱威胁。以往的方法主要依赖于生成与测试策略来制作恶意输入，缺乏对底层审查过程的全面理解。所提出的方法XBreaking利用可解释人工智能分析和比较审查与未审查模型的行为，识别可利用的对齐模式，从而通过噪声注入设计更具针对性的越狱攻击。本文的贡献在于提供了对LLMs审查机制的深入见解，并展示了XBreaking在克服安全约束方面的有效性，从而在越狱攻击中取得了显著的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language   Models</div>
<div class="meta-line">Authors: Yuhao Sun, Zhuoer Xu, Shiwen Cui, Kun Yang, Lingyun Yu, Yongdong Zhang, Hongtao Xie</div>
<div class="meta-line">First: 2025-10-02T16:43:33+00:00 · Latest: 2025-10-02T16:43:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.02194v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.02194v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have achieved remarkable progress across a wide
range of tasks, but remain vulnerable to safety risks such as harmful content
generation and jailbreak attacks. Existing safety techniques -- including
external guardrails, inference-time guidance, and post-training alignment --
each face limitations in balancing safety, utility, and controllability. In
this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM
safety through safety-aware upcycling. Our approach first identifies
safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)
structure, where the router acts as a soft guardrail that selectively activates
original MLPs and added safety experts. We further introduce a two-stage SFT
strategy to strengthen safety discrimination while preserving general
capabilities. To enable flexible control at inference time, we introduce a
safety temperature mechanism, allowing dynamic adjustment of the trade-off
between safety and utility. Experiments across multiple benchmarks, base model,
and model scales demonstrate that UpSafe$^\circ$C achieves robust safety
improvements against harmful and jailbreak inputs, while maintaining
competitive performance on general tasks. Moreover, analysis shows that safety
temperature provides fine-grained inference-time control that achieves the
Pareto-optimal frontier between utility and safety. Our results highlight a new
direction for LLM safety: moving from static alignment toward dynamic, modular,
and inference-aware control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UpSafe$^\circ$C：大型语言模型的可控安全升级</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在广泛任务中取得了显著进展，但仍然容易受到有害内容生成和越狱攻击等安全风险的影响。现有的安全技术——包括外部护栏、推理时指导和后训练对齐——在平衡安全性、实用性和可控性方面各有局限。在本研究中，我们提出了UpSafe$^\circ$C，这是一个通过安全意识升级来增强LLM安全性的统一框架。我们的方法首先识别安全关键层，并将其升级为稀疏的专家混合（MoE）结构，其中路由器充当选择性激活原始MLP和新增安全专家的软护栏。我们进一步引入了两阶段的SFT策略，以增强安全区分能力，同时保持一般能力。为了在推理时实现灵活控制，我们引入了一种安全温度机制，允许动态调整安全性和实用性之间的权衡。多个基准、基础模型和模型规模的实验表明，UpSafe$^\circ$C在抵御有害和越狱输入方面实现了稳健的安全性提升，同时在一般任务上保持了竞争性能。此外，分析表明，安全温度提供了细粒度的推理时控制，实现了实用性和安全性之间的帕累托最优边界。我们的结果突显了LLM安全的新方向：从静态对齐转向动态、模块化和推理意识的控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the safety vulnerabilities of Large Language Models (LLMs), which can generate harmful content and be susceptible to jailbreak attacks. Previous methods, such as external guardrails and post-training alignment, struggle to balance safety, utility, and controllability, leading to limitations in their effectiveness. The proposed UpSafe$^\circ$C framework enhances LLM safety through a novel approach of safety-aware upcycling, which identifies safety-critical layers and transforms them into a sparse Mixture-of-Experts (MoE) structure, allowing for selective activation of safety experts. This method includes a two-stage supervised fine-tuning strategy to improve safety discrimination while maintaining general capabilities, and introduces a safety temperature mechanism for dynamic control during inference. Experimental results across various benchmarks demonstrate that UpSafe$^\circ$C significantly improves safety against harmful inputs while preserving competitive performance on general tasks, thus achieving a balance between utility and safety that supports its objectives.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）的安全漏洞，这些模型可能生成有害内容并容易受到越狱攻击。以往的安全方法，如外部护栏和后期训练对齐，难以在安全性、实用性和可控性之间取得平衡。提出的UpSafe$^\circ$C框架通过一种新颖的安全感知再利用方法增强LLM的安全性，该方法识别安全关键层并将其转化为稀疏的专家混合（MoE）结构。该方法包括一个两阶段的SFT策略，以改善安全性区分，并引入安全温度机制以在推理过程中实现灵活控制。实验结果表明，UpSafe$^\circ$C在抵御有害输入方面显著提高了安全性，同时在一般任务上保持了竞争力的性能，展示了LLM安全性控制向动态和模块化转变的新方向。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Instincts: LLMs Learn to Trust Their Internal Compass for   Self-Defense</div>
<div class="meta-line">Authors: Guobin Shen, Dongcheng Zhao, Haibo Tong, Jindong Li, Feifei Zhao, Yi Zeng</div>
<div class="meta-line">First: 2025-10-01T16:35:03+00:00 · Latest: 2025-10-01T16:35:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.01088v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.01088v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring Large Language Model (LLM) safety remains challenging due to the
absence of universal standards and reliable content validators, making it
difficult to obtain effective training signals. We discover that aligned models
already possess robust internal safety beliefs: they consistently produce
high-confidence refusals to harmful requests while exhibiting high entropy when
generating potentially dangerous content. This entropy gap reveals an untapped
signal--models intrinsically &quot;know&quot; when to refuse. We introduce Safety
Instincts Reinforcement Learning (SIRL), which transforms this internal
confidence into a self-generated reward signal, eliminating dependence on
external validators or human annotations. SIRL teaches models to trust their
safety instincts by reinforcing low-entropy refusal behaviors. Evaluated on
Llama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against
20+ jailbreak methods, from static prompts to adaptive attacks. Using only
15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods
while preserving performance on mathematics, coding, and conversation
benchmarks. Our work demonstrates that effective alignment can emerge from
within, paving the way for more autonomous and robust AI safety mechanisms that
scale without extensive human oversight.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全本能：大型语言模型学习信任其内部指南以进行自我防御</div>
<div class="mono" style="margin-top:8px">确保大型语言模型（LLM）的安全性仍然具有挑战性，因为缺乏普遍标准和可靠的内容验证者，使得获得有效的训练信号变得困难。我们发现，已对齐的模型已经具备强大的内部安全信念：它们在面对有害请求时始终产生高置信度的拒绝，同时在生成潜在危险内容时表现出高熵。这一熵差揭示了一个未被利用的信号——模型本质上“知道”何时拒绝。我们引入了安全本能强化学习（SIRL），将这种内部信心转化为自生成的奖励信号，消除对外部验证者或人工注释的依赖。SIRL通过强化低熵拒绝行为来教导模型信任其安全本能。在Llama和Qwen模型上的评估表明，SIRL在20多种越狱方法（从静态提示到自适应攻击）中保持89%以上的防御成功率（DSR）。仅使用15,000个未标记的提示，SIRL超越了资源密集型的监督方法，同时在数学、编码和对话基准测试中保持性能。我们的工作表明，有效的对齐可以从内部产生，为更自主和强大的AI安全机制铺平道路，这些机制在没有广泛人类监督的情况下可扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the ongoing challenge of ensuring safety in Large Language Models (LLMs), which is complicated by the lack of universal standards and reliable content validators. Previous methods often relied heavily on external validators or human annotations, which can be resource-intensive and inconsistent. The proposed approach, Safety Instincts Reinforcement Learning (SIRL), leverages the internal safety beliefs of aligned models, transforming their intrinsic confidence into a self-generated reward signal that reinforces low-entropy refusal behaviors. This method is well-motivated as it allows models to autonomously trust their safety instincts without external dependencies. The contribution of the paper lies in demonstrating that SIRL can achieve over 89% Defense Success Rates against various jailbreak methods while using only 15,000 unlabeled prompts, outperforming traditional supervised methods and maintaining performance across other benchmarks such as mathematics, coding, and conversation.</div>
<div class="mono" style="margin-top:8px">本研究解决了确保大型语言模型（LLMs）安全性面临的持续挑战，特别是由于缺乏普遍标准和可靠内容验证者，导致有效训练受到阻碍。以往的方法往往过于依赖外部验证者或人工注释，这可能资源密集且不一致。提出的安全本能强化学习（SIRL）方法通过利用已对齐模型内部的安全信念，将其内在信心转化为自生成的奖励信号，从而强化低熵拒绝行为。这种方法的动机明确，旨在增强模型在安全决策中的自主性。论文的贡献在于证明SIRL能够在使用仅15,000个未标记提示的情况下，对抗多种越狱方法时实现超过89%的防御成功率，同时在数学、编码和对话等其他基准测试中保持性能，超越传统的监督方法。</div>
</details>
</div>
<div class="card">
<div class="title">QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety</div>
<div class="meta-line">Authors: Taegyeong Lee, Jeonghwa Yoo, Hyoungseo Cho, Soo Yong Kim, Yunho Maeng</div>
<div class="meta-line">Venue: ACL</div>
<div class="meta-line">First: 2025-06-14T01:23:50+00:00 · Latest: 2025-09-30T07:47:51+00:00</div>
<div class="meta-line">Comments: Accept to ACLW 2025 (WOAH); fix typo</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.12299v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.12299v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent advancements in Large Language Models(LLMs) have had a significant
impact on a wide range of fields, from general domains to specialized areas.
However, these advancements have also significantly increased the potential for
malicious users to exploit harmful and jailbreak prompts for malicious attacks.
Although there have been many efforts to prevent harmful prompts and jailbreak
prompts, protecting LLMs from such malicious attacks remains an important and
challenging task. In this paper, we propose QGuard, a simple yet effective
safety guard method, that utilizes question prompting to block harmful prompts
in a zero-shot manner. Our method can defend LLMs not only from text-based
harmful prompts but also from multi-modal harmful prompt attacks. Moreover, by
diversifying and modifying guard questions, our approach remains robust against
the latest harmful prompts without fine-tuning. Experimental results show that
our model performs competitively on both text-only and multi-modal harmful
datasets. Additionally, by providing an analysis of question prompting, we
enable a white-box analysis of user inputs. We believe our method provides
valuable insights for real-world LLM services in mitigating security risks
associated with harmful prompts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QGuard：基于问题的零-shot多模态LLM安全防护</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的最新进展对从一般领域到专业领域的广泛领域产生了重大影响。然而，这些进展也显著增加了恶意用户利用有害和越狱提示进行恶意攻击的潜力。尽管已经有许多努力来防止有害提示和越狱提示，但保护LLMs免受此类恶意攻击仍然是一项重要且具有挑战性的任务。本文提出了QGuard，一种简单而有效的安全防护方法，利用问题提示以零-shot方式阻止有害提示。我们的方法不仅可以防御基于文本的有害提示，还可以防御多模态有害提示攻击。此外，通过多样化和修改防护问题，我们的方法在不进行微调的情况下仍然对最新的有害提示保持稳健。实验结果表明，我们的模型在文本和多模态有害数据集上表现出竞争力。此外，通过提供问题提示的分析，我们实现了对用户输入的白盒分析。我们相信我们的方法为现实世界的LLM服务提供了有价值的见解，以减轻与有害提示相关的安全风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concern of malicious exploitation of Large Language Models (LLMs) through harmful and jailbreak prompts, which poses significant security risks. Previous methods aimed at preventing such attacks have been inadequate, often requiring fine-tuning or failing to address multi-modal threats effectively. The proposed QGuard method distinguishes itself by employing question prompting in a zero-shot manner, allowing it to block harmful prompts without the need for extensive model adjustments. This approach is well-motivated as it enhances the robustness of LLMs against evolving threats. The paper contributes a novel safety guard that demonstrates competitive performance on both text-only and multi-modal harmful datasets, supporting its goal of improving LLM safety in real-world applications.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）被恶意利用的日益严重的问题，尤其是通过有害和越狱提示，这对安全构成了重大风险。以往旨在防止此类攻击的方法效果不佳，通常需要微调，并且缺乏对不断演变威胁的鲁棒性。所提出的方法QGuard通过在零样本的情况下使用问题提示，能够有效阻止有害提示，而无需进行广泛的修改或微调，从而与众不同。该方法不仅保护文本攻击，还扩展到多模态威胁，在各种有害数据集上表现出竞争力。本文的贡献在于创新性地使用问题提示来增强LLM的安全性，为实际LLM服务提供了可以应用的见解，以减轻安全漏洞。</div>
</details>
</div>
<div class="card">
<div class="title">Backdoor Attribution: Elucidating and Controlling Backdoor in Language   Models</div>
<div class="meta-line">Authors: Miao Yu, Zhenhong Zhou, Moayad Aloqaily, Kun Wang, Biwei Huang, Stephen Wang, Yueming Jin, Qingsong Wen</div>
<div class="meta-line">First: 2025-09-26T01:45:25+00:00 · Latest: 2025-09-30T01:52:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.21761v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.21761v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuned Large Language Models (LLMs) are vulnerable to backdoor attacks
through data poisoning, yet the internal mechanisms governing these attacks
remain a black box. Previous research on interpretability for LLM safety tends
to focus on alignment, jailbreak, and hallucination, but overlooks backdoor
mechanisms, making it difficult to understand and fully eliminate the backdoor
threat. In this paper, aiming to bridge this gap, we explore the interpretable
mechanisms of LLM backdoors through Backdoor Attribution (BkdAttr), a
tripartite causal analysis framework. We first introduce the Backdoor Probe
that proves the existence of learnable backdoor features encoded within the
representations. Building on this insight, we further develop Backdoor
Attention Head Attribution (BAHA), efficiently pinpointing the specific
attention heads responsible for processing these features. Our primary
experiments reveals these heads are relatively sparse; ablating a minimal
\textbf{$\sim$ 3%} of total heads is sufficient to reduce the Attack Success
Rate (ASR) by \textbf{over 90%}. More importantly, we further employ these
findings to construct the Backdoor Vector derived from these attributed heads
as a master controller for the backdoor. Through only \textbf{1-point}
intervention on \textbf{single} representation, the vector can either boost ASR
up to \textbf{$\sim$ 100% ($\uparrow$)} on clean inputs, or completely
neutralize backdoor, suppressing ASR down to \textbf{$\sim$ 0% ($\downarrow$)}
on triggered inputs. In conclusion, our work pioneers the exploration of
mechanistic interpretability in LLM backdoors, demonstrating a powerful method
for backdoor control and revealing actionable insights for the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>后门归因：阐明和控制语言模型中的后门</div>
<div class="mono" style="margin-top:8px">微调的大型语言模型（LLMs）易受数据中毒的后门攻击，但这些攻击的内部机制仍然是一个黑箱。以往关于LLM安全性的可解释性研究往往集中在对齐、越狱和幻觉上，而忽视了后门机制，使得理解和完全消除后门威胁变得困难。本文旨在填补这一空白，通过后门归因（BkdAttr）这一三方因果分析框架，探索LLM后门的可解释机制。我们首先引入后门探测器，证明可学习的后门特征存在于表示中。基于这一见解，我们进一步开发了后门注意力头归因（BAHA），有效地定位负责处理这些特征的特定注意力头。我们的主要实验表明，这些头相对稀疏；去除约3%的总头数足以将攻击成功率（ASR）降低超过90%。更重要的是，我们进一步利用这些发现构建了基于这些归因头的后门向量，作为后门的主控器。通过对单一表示进行1点干预，该向量可以在干净输入上将ASR提升至约100%（↑），或在未触发输入上完全中和后门，将ASR压制至约0%（↓）。总之，我们的工作开创了LLM后门机制可解释性的探索，展示了一种强大的后门控制方法，并为社区提供了可操作的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the vulnerability of fine-tuned Large Language Models (LLMs) to backdoor attacks via data poisoning, highlighting a gap in existing interpretability studies that typically focus on alignment and hallucination while neglecting backdoor mechanisms. Previous methods have struggled to elucidate these mechanisms, making it challenging to mitigate backdoor threats effectively. The proposed approach, Backdoor Attribution (BkdAttr), introduces a tripartite causal analysis framework that includes the Backdoor Probe to identify learnable backdoor features and the Backdoor Attention Head Attribution (BAHA) to isolate specific attention heads responsible for these features. The methodology demonstrates that ablating approximately 3% of total heads can reduce the Attack Success Rate (ASR) by over 90%, and a single intervention using the Backdoor Vector can either enhance ASR to nearly 100% on clean inputs or neutralize the backdoor, reducing ASR to around 0% on triggered inputs. This work contributes to the understanding of mechanistic interpretability in LLM backdoors and provides a robust method for controlling backdoor threats.</div>
<div class="mono" style="margin-top:8px">本研究关注微调的大型语言模型（LLMs）在数据中毒下对后门攻击的脆弱性，强调了对这些攻击内部机制理解的不足，而以往的可解释性研究大多忽视了这一点。现有方法主要集中在对齐和幻觉等方面，未能提供对后门机制的深入见解，因此需要一种新方法。本文提出了后门归因（BkdAttr），一种三方因果分析框架，包括后门探测器以识别可学习的后门特征，以及后门注意力头归因（BAHA）以定位负责这些特征的特定注意力头。该方法表明，去除大约3%的注意力头可以使攻击成功率（ASR）降低超过90%，而构建的后门向量则允许对后门效果进行显著控制，实现ASR从近100%到0%的操控，仅需最小干预。该研究为LLM后门的机械可解释性做出了贡献，并提供了一种强有力的后门控制方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
