<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-02 03:34</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260102_0334</div>
    <div class="row"><div class="card">
<div class="title">Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?</div>
<div class="meta-line">Authors: Yuan Xin, Dingfan Chen, Linyi Yang, Michael Backes, Xiao Zhang</div>
<div class="meta-line">First: 2025-12-30T07:36:19+00:00 · Latest: 2025-12-30T07:36:19+00:00</div>
<div class="meta-line">Comments: 26 pages,11 tables, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24044v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24044v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) are increasingly deployed, ensuring their safe use is paramount. Jailbreaking, adversarial prompts that bypass model alignment to trigger harmful outputs, present significant risks, with existing studies reporting high success rates in evading common LLMs. However, previous evaluations have focused solely on the models, neglecting the full deployment pipeline, which typically incorporates additional safety mechanisms like content moderation filters. To address this gap, we present the first systematic evaluation of jailbreak attacks targeting LLM safety alignment, assessing their success across the full inference pipeline, including both input and output filtering stages. Our findings yield two key insights: first, nearly all evaluated jailbreak techniques can be detected by at least one safety filter, suggesting that prior assessments may have overestimated the practical success of these attacks; second, while safety filters are effective in detection, there remains room to better balance recall and precision to further optimize protection and user experience. We highlight critical gaps and call for further refinement of detection accuracy and usability in LLM safety systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>越狱攻击与内容安全过滤器：我们在大型语言模型安全军备竞赛中走多远？</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）的日益普及，确保其安全使用至关重要。越狱攻击是绕过模型对齐以触发有害输出的对抗性提示，存在重大风险，现有研究报告显示在规避常见LLMs方面成功率很高。然而，以往的评估仅关注模型，忽视了完整的部署流程，后者通常包含额外的安全机制，如内容审核过滤器。为填补这一空白，我们首次系统评估了针对LLM安全对齐的越狱攻击，评估其在完整推理流程中的成功率，包括输入和输出过滤阶段。我们的研究结果得出两个关键见解：首先，几乎所有评估的越狱技术都可以被至少一个安全过滤器检测到，这表明以往评估可能高估了这些攻击的实际成功率；其次，尽管安全过滤器在检测方面有效，但在召回率和精确度之间仍有改进空间，以进一步优化保护和用户体验。我们强调了关键差距，并呼吁进一步提高LLM安全系统的检测准确性和可用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the increasing deployment of large language models (LLMs) and the associated risks of jailbreaking attacks, which can bypass model alignment and produce harmful outputs. Previous studies have primarily focused on the models themselves, overlooking the complete deployment pipeline that includes safety mechanisms like content moderation filters. This paper proposes a systematic evaluation of jailbreak attacks against LLM safety alignment, assessing their effectiveness across the entire inference pipeline. The findings reveal that most jailbreak techniques can be detected by at least one safety filter, indicating that earlier evaluations may have overestimated their success rates. Additionally, while safety filters are effective, there is a need for improved balance between recall and precision to enhance user protection and experience. The contribution of this work lies in identifying critical gaps in current safety systems and advocating for advancements in detection accuracy and usability.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）安全性日益增长的担忧，特别是针对利用漏洞产生有害输出的越狱攻击。以往的研究主要孤立评估模型，忽视了包括内容审核过滤器在内的完整部署流程，从而导致对这些攻击有效性的理解不够全面。本文通过系统评估越狱攻击在整个推理流程中的表现作出贡献，揭示大多数越狱技术至少可以被一个安全过滤器检测到，从而挑战了之前关于其成功率的假设。该方法论涉及评估各种越狱技术在输入和输出过滤阶段的有效性，结果表明，尽管安全过滤器通常有效，但在提高召回率和精确度的平衡方面仍需改进，以增强LLM安全系统的保护和用户体验。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Context: Large Language Models Failure to Grasp Users Intent</div>
<div class="meta-line">Authors: Ahmed M. Hussain, Salahuddin Salahuddin, Panos Papadimitratos</div>
<div class="meta-line">First: 2025-12-24T11:15:57+00:00 · Latest: 2025-12-29T14:48:01+00:00</div>
<div class="meta-line">Comments: 22 pages and 23 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21110v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.21110v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越上下文：大型语言模型未能理解用户意图</div>
<div class="mono" style="margin-top:8px">当前大型语言模型（LLMs）的安全方法专注于显性有害内容，而忽视了一个关键漏洞：无法理解上下文和识别用户意图。这导致了可被恶意用户系统性利用的漏洞，以规避安全机制。我们对多个最先进的LLMs进行了实证评估，包括ChatGPT、Claude、Gemini和DeepSeek。我们的分析表明，通过情感框架、渐进揭示和学术辩护技术，可靠安全机制被规避。值得注意的是，启用推理的配置增强了而不是减轻了利用的有效性，提高了事实精确性，但未能质疑潜在意图。例外是Claude Opus 4.1，在某些用例中优先考虑意图检测而非信息提供。这一模式揭示了当前架构设计造成的系统性漏洞。这些局限性需要在上下文理解和意图识别方面进行范式转变，将其作为核心安全能力，而不是事后保护机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the critical issue of Large Language Models (LLMs) failing to understand user intent, which poses significant safety vulnerabilities that can be exploited by malicious users. Previous methods primarily focused on filtering harmful content without adequately addressing the contextual understanding necessary for intent recognition, leading to systematic weaknesses in safety mechanisms. The proposed approach emphasizes the need for a shift in architectural design to prioritize contextual understanding and intent recognition as fundamental safety features. The research methodology involves empirical evaluation of several state-of-the-art LLMs, revealing that techniques like emotional framing and progressive revelation can undermine safety measures, with only Claude Opus 4.1 showing some improvement in intent detection. The findings indicate that current LLM designs require significant changes to enhance their safety capabilities, as the existing methods fail to effectively mitigate the risks associated with user intent exploitation.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在理解用户意图和上下文方面的重大缺陷，这导致了显著的安全漏洞。以往的方法主要集中在过滤有害内容上，但忽视了对用户意图的细致理解，从而导致可被利用的弱点。所提出的方法强调将上下文理解和意图识别作为LLMs的核心安全特性，而不是依赖于事后保护措施。本文通过实证评估多种最先进的LLMs，揭示了情感框架和渐进揭示等技术可以绕过现有的安全机制，唯一的例外是Claude Opus 4.1，它在某些用例中优先考虑意图检测。研究结果表明，当前LLM架构固有地存在漏洞，需要在设计理念上进行转变，以增强安全能力。</div>
</details>
</div>
<div class="card">
<div class="title">AdvPrefix: An Objective for Nuanced LLM Jailbreaks</div>
<div class="meta-line">Authors: Sicheng Zhu, Brandon Amos, Yuandong Tian, Chuan Guo, Ivan Evtimov</div>
<div class="meta-line">First: 2024-12-13T18:00:57+00:00 · Latest: 2025-12-27T07:36:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.10321v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.10321v2">PDF</a> · <a href="http://github.com/facebookresearch/jailbreak-objectives">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many jailbreak attacks on large language models (LLMs) rely on a common objective: making the model respond with the prefix ``Sure, here is (harmful request)&#x27;&#x27;. While straightforward, this objective has two limitations: limited control over model behaviors, yielding incomplete or unrealistic jailbroken responses, and a rigid format that hinders optimization. We introduce AdvPrefix, a plug-and-play prefix-forcing objective that selects one or more model-dependent prefixes by combining two criteria: high prefilling attack success rates and low negative log-likelihood. AdvPrefix integrates seamlessly into existing jailbreak attacks to mitigate the previous limitations for free. For example, replacing GCG&#x27;s default prefixes on Llama-3 improves nuanced attack success rates from 14% to 80%, revealing that current safety alignment fails to generalize to new prefixes. Code and selected prefixes are released at github.com/facebookresearch/jailbreak-objectives.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdvPrefix：细致的LLM越狱目标</div>
<div class="mono" style="margin-top:8px">许多针对大型语言模型（LLM）的越狱攻击依赖于一个共同目标：使模型以前缀“当然，这里是（有害请求）”作出响应。虽然这个目标简单，但有两个局限性：对模型行为的控制有限，导致越狱响应不完整或不现实，以及固定格式阻碍优化。我们引入了AdvPrefix，这是一种即插即用的前缀强制目标，通过结合两个标准选择一个或多个模型依赖的前缀：高的预填充攻击成功率和低的负对数似然。AdvPrefix无缝集成到现有的越狱攻击中，以免费缓解之前的局限性。例如，在Llama-3上替换GCG的默认前缀将细致攻击成功率从14%提高到80%，揭示当前的安全对齐未能推广到新前缀。代码和选定前缀已在github.com/facebookresearch/jailbreak-objectives发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing jailbreak attacks on large language models (LLMs), which typically rely on a simplistic objective that restricts control over model behavior and results in incomplete responses. Previous methods have struggled with these issues due to their rigid format, leading to suboptimal performance. The proposed AdvPrefix approach introduces a more flexible prefix-forcing objective that selects model-dependent prefixes based on high attack success rates and low negative log-likelihood, effectively enhancing the jailbreak process. This methodology allows for seamless integration into existing attacks, significantly improving nuanced attack success rates from 14% to 80% on Llama-3, thereby demonstrating that current safety measures do not adequately account for new prefixes and supporting the need for more sophisticated jailbreak strategies.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有大型语言模型（LLMs）越狱攻击的局限性，这些攻击通常使用简单的目标，导致响应不完整，并因其固定格式限制了优化。以往的方法在对模型行为的控制上存在不足，因此需要一种更灵活的方法。提出的AdvPrefix方法引入了一种前缀强制目标，通过高攻击成功率和低负对数似然性选择模型相关前缀，有效提升了越狱攻击的性能。这种方法具有良好的动机，因为它可以无缝集成到现有框架中，显著提高了Llama-3上细致攻击的成功率，从14%提升至80%，从而表明当前的安全措施未能充分推广到新前缀，并支持了更有效的越狱策略的目标。</div>
</details>
</div>
<div class="card">
<div class="title">AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</div>
<div class="meta-line">Authors: Aashray Reddy, Andrew Zagula, Nicholas Saban</div>
<div class="meta-line">First: 2025-04-18T08:38:56+00:00 · Latest: 2025-12-23T19:52:29+00:00</div>
<div class="meta-line">Comments: We encountered issues with the paper being hosted under my personal account, so we republished it under a different account associated with a university email, which makes updates and management easier. As a result, this version is a duplicate of arXiv:2511.02376</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.01020v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.01020v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) continue to exhibit vulnerabilities to jailbreaking attacks: carefully crafted malicious inputs intended to circumvent safety guardrails and elicit harmful responses. As such, we present AutoAdv, a novel framework that automates adversarial prompt generation to systematically evaluate and expose vulnerabilities in LLM safety mechanisms. Our approach leverages a parametric attacker LLM to produce semantically disguised malicious prompts through strategic rewriting techniques, specialized system prompts, and optimized hyperparameter configurations. The primary contribution of our work is a dynamic, multi-turn attack methodology that analyzes failed jailbreak attempts and iteratively generates refined follow-up prompts, leveraging techniques such as roleplaying, misdirection, and contextual manipulation. We quantitatively evaluate attack success rate (ASR) using the StrongREJECT (arXiv:2402.10260 [cs.CL]) framework across sequential interaction turns. Through extensive empirical evaluation of state-of-the-art models--including ChatGPT, Llama, and DeepSeek--we reveal significant vulnerabilities, with our automated attacks achieving jailbreak success rates of up to 86% for harmful content generation. Our findings reveal that current safety mechanisms remain susceptible to sophisticated multi-turn attacks, emphasizing the urgent need for more robust defense strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoAdv：用于大型语言模型多轮越狱的自动化对抗提示生成</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）仍然表现出对越狱攻击的脆弱性：精心设计的恶意输入旨在绕过安全防护并引发有害响应。因此，我们提出了AutoAdv，一个新颖的框架，自动化对抗提示生成，以系统性地评估和揭示LLM安全机制中的脆弱性。我们的方法利用参数化攻击者LLM，通过战略重写技术、专业系统提示和优化超参数配置生成语义伪装的恶意提示。我们工作的主要贡献是一个动态的多轮攻击方法，分析失败的越狱尝试并迭代生成精炼的后续提示，利用角色扮演、误导和上下文操控等技术。我们使用StrongREJECT（arXiv:2402.10260 [cs.CL]）框架定量评估攻击成功率（ASR），跨越连续的交互轮次。通过对最先进模型的广泛实证评估，包括ChatGPT、Llama和DeepSeek，我们揭示了显著的脆弱性，我们的自动化攻击在有害内容生成方面的越狱成功率高达86%。我们的发现表明，当前的安全机制仍然容易受到复杂的多轮攻击，强调了对更强大防御策略的迫切需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the ongoing vulnerabilities of Large Language Models (LLMs) to jailbreaking attacks, which are malicious inputs designed to bypass safety measures. Previous methods lacked automation and systematic evaluation, leading to insufficient understanding of LLM weaknesses. The proposed AutoAdv framework automates adversarial prompt generation, utilizing a parametric attacker LLM to create semantically disguised prompts through advanced rewriting techniques and optimized configurations. This dynamic, multi-turn attack methodology iteratively refines prompts based on previous failures, employing strategies like roleplaying and contextual manipulation. The methodology was quantitatively assessed using the StrongREJECT framework, revealing that automated attacks achieved jailbreak success rates of up to 86% across various state-of-the-art models, highlighting significant vulnerabilities in current safety mechanisms and underscoring the need for improved defense strategies.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在监狱破解攻击中的持续脆弱性，这些攻击是旨在绕过安全机制的恶意输入。以往的方法缺乏自动化和系统评估，导致识别弱点的有效性有限。提出的AutoAdv框架自动化对抗性提示生成，利用参数攻击者LLM通过战略重写和优化配置创建语义伪装的提示。这种动态的多轮攻击方法基于失败尝试迭代改进提示，采用角色扮演和上下文操控等技术。该方法通过StrongREJECT框架进行定量评估，揭示自动化攻击在生成有害内容方面的成功率高达86%，突显了当前安全机制的重大脆弱性，并强调了改进防御的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Shape it Up! Restoring LLM Safety during Finetuning</div>
<div class="meta-line">Authors: ShengYun Peng, Pin-Yu Chen, Jianfeng Chi, Seongmin Lee, Duen Horng Chau</div>
<div class="meta-line">Venue: NeurIPS</div>
<div class="meta-line">First: 2025-05-22T18:05:16+00:00 · Latest: 2025-12-22T17:30:15+00:00</div>
<div class="meta-line">Comments: NeurIPS&#x27;25</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.17196v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.17196v3">PDF</a> · <a href="https://github.com/poloclub/star-dss">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Finetuning large language models (LLMs) enables user-specific customization but introduces critical safety risks: even a few harmful examples can compromise safety alignment. A common mitigation strategy is to update the model more strongly on examples deemed safe, while downweighting or excluding those flagged as unsafe. However, because safety context can shift within a single example, updating the model equally on both harmful and harmless parts of a response is suboptimal-a coarse treatment we term static safety shaping. In contrast, we propose dynamic safety shaping (DSS), a framework that uses fine-grained safety signals to reinforce learning from safe segments of a response while suppressing unsafe content. To enable such fine-grained control during finetuning, we introduce a key insight: guardrail models, traditionally used for filtering, can be repurposed to evaluate partial responses, tracking how safety risk evolves throughout the response, segment by segment. This leads to the Safety Trajectory Assessment of Response (STAR), a token-level signal that enables shaping to operate dynamically over the training sequence. Building on this, we present STAR-DSS, guided by STAR scores, that robustly mitigates finetuning risks and delivers substantial safety improvements across diverse threats, datasets, and model families-all without compromising capability on intended tasks. We encourage future safety research to build on dynamic shaping principles for stronger mitigation against evolving finetuning risks. Our code is publicly available at https://github.com/poloclub/star-dss.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>塑造它！在微调期间恢复大型语言模型的安全性</div>
<div class="mono" style="margin-top:8px">微调大型语言模型（LLMs）使用户能够进行特定定制，但引入了关键的安全风险：即使少量有害示例也可能破坏安全对齐。一种常见的缓解策略是对被认为安全的示例进行更强的模型更新，同时降低或排除被标记为不安全的示例。然而，由于安全上下文可能在单个示例中发生变化，平等地更新响应中有害和无害部分的模型是次优的——我们称之为静态安全塑造。相反，我们提出了动态安全塑造（DSS），这是一个利用细粒度安全信号来强化从响应安全部分学习，同时抑制不安全内容的框架。为了在微调期间实现这种细粒度控制，我们引入了一个关键见解：传统上用于过滤的护栏模型可以重新用于评估部分响应，跟踪安全风险如何在响应中逐段演变。这导致了响应的安全轨迹评估（STAR），这是一个令牌级信号，使塑造能够在训练序列中动态运行。在此基础上，我们提出了STAR-DSS，基于STAR分数，稳健地减轻微调风险，并在各种威胁、数据集和模型系列中提供显著的安全改进——所有这些都不影响预期任务的能力。我们鼓励未来的安全研究基于动态塑造原则，以更强的方式缓解不断演变的微调风险。我们的代码可在https://github.com/poloclub/star-dss上公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the safety risks associated with finetuning large language models (LLMs), where even a few harmful examples can undermine safety alignment. Previous methods typically involved static safety shaping, which inadequately treated both harmful and harmless parts of responses, leading to suboptimal updates. The proposed dynamic safety shaping (DSS) framework improves upon this by utilizing fine-grained safety signals to enhance learning from safe segments while suppressing unsafe content. This approach is well-motivated as it allows for a more nuanced evaluation of responses through guardrail models, which track safety risks segment by segment. The paper contributes the Safety Trajectory Assessment of Response (STAR) and STAR-DSS, which effectively mitigate finetuning risks and improve safety across various threats and datasets without sacrificing model performance on intended tasks. The methodology demonstrates significant safety enhancements, supporting the goal of maintaining safety alignment during LLM customization.</div>
<div class="mono" style="margin-top:8px">本研究解决了微调大型语言模型（LLMs）所带来的关键安全风险，即即使少量有害示例也可能危及安全对齐。以往的方法通常采用静态安全塑形，这种方法对有害和无害响应部分的处理不够精细，导致安全结果不理想。所提出的动态安全塑形（DSS）框架通过利用细粒度的安全信号来增强对安全段的学习，同时抑制不安全内容，从而改进了这一点，利用护栏模型评估部分响应并跟踪安全风险。该论文的贡献在于引入了响应的安全轨迹评估（STAR）和STAR-DSS，这些方法有效减轻了微调风险，并在各种威胁和数据集上显著提高了安全性，同时不牺牲模型在预期任务上的性能。该方法展示了稳健的安全改进，支持在LLM定制过程中保持安全的目标。</div>
</details>
</div>
<div class="card">
<div class="title">MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking</div>
<div class="meta-line">Authors: Jianyi Zhang, Shizhao Liu, Ziyin Zhou, Zhen Li</div>
<div class="meta-line">First: 2025-12-21T14:43:26+00:00 · Latest: 2025-12-21T14:43:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.18755v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.18755v1">PDF</a> · <a href="https://github.com/Carney-lsz/MEEA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of large language models (LLMs) has intensified concerns about the robustness of their safety alignment. While existing jailbreak studies explore both single-turn and multi-turn strategies, most implicitly assume a static safety boundary and fail to account for how contextual interactions dynamically influence model behavior, leading to limited stability and generalization. Motivated by this gap, we propose MEEA (Mere Exposure Effect Attack), a psychology-inspired, fully automated black-box framework for evaluating multi-turn safety robustness, grounded in the mere exposure effect. MEEA leverages repeated low-toxicity semantic exposure to induce a gradual shift in a model&#x27;s effective safety threshold, enabling progressive erosion of alignment constraints over sustained interactions. Concretely, MEEA constructs semantically progressive prompt chains and optimizes them using a simulated annealing strategy guided by semantic similarity, toxicity, and jailbreak effectiveness. Extensive experiments on both closed-source and open-source models, including GPT-4, Claude-3.5, and DeepSeek-R1, demonstrate that MEEA consistently achieves higher attack success rates than seven representative baselines, with an average Attack Success Rate (ASR) improvement exceeding 20%. Ablation studies further validate the necessity of both annealing-based optimization and contextual exposure mechanisms. Beyond improved attack effectiveness, our findings indicate that LLM safety behavior is inherently dynamic and history-dependent, challenging the common assumption of static alignment boundaries and highlighting the need for interaction-aware safety evaluation and defense mechanisms. Our code is available at: https://github.com/Carney-lsz/MEEA</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MEEA：基于单纯曝光效应的对抗优化用于大型语言模型越狱</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的快速发展加剧了人们对其安全对齐稳健性的担忧。虽然现有的越狱研究探讨了单轮和多轮策略，但大多数隐含假设安全边界是静态的，未能考虑上下文交互如何动态影响模型行为，导致稳定性和泛化能力有限。基于这一空白，我们提出了MEEA（单纯曝光效应攻击），这是一个受心理学启发的全自动黑箱框架，用于评估多轮安全稳健性，基于单纯曝光效应。MEEA利用重复的低毒性语义曝光，诱导模型有效安全阈值的逐步转变，从而在持续交互中逐步侵蚀对齐约束。具体而言，MEEA构建语义渐进的提示链，并使用基于语义相似性、毒性和越狱有效性的模拟退火策略进行优化。对包括GPT-4、Claude-3.5和DeepSeek-R1在内的闭源和开源模型进行的广泛实验表明，MEEA的攻击成功率始终高于七个代表性基线，平均攻击成功率（ASR）提高超过20%。消融研究进一步验证了基于退火的优化和上下文曝光机制的必要性。除了提高攻击有效性外，我们的研究结果表明，LLM的安全行为本质上是动态的和依赖历史的，这挑战了静态对齐边界的普遍假设，并强调了需要进行交互感知的安全评估和防御机制。我们的代码可在以下网址获取：https://github.com/Carney-lsz/MEEA</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the growing concerns regarding the safety alignment of large language models (LLMs), particularly in the context of jailbreak strategies that often overlook the dynamic nature of model behavior influenced by contextual interactions. Previous methods typically assumed a static safety boundary, resulting in limited effectiveness and generalization. The proposed MEEA (Mere Exposure Effect Attack) framework innovatively incorporates psychological principles to evaluate multi-turn safety robustness, allowing for a gradual shift in a model&#x27;s safety threshold through repeated low-toxicity semantic exposure. This methodology employs semantically progressive prompt chains optimized via a simulated annealing strategy, leading to significant improvements in attack success rates, with experiments showing an average increase exceeding 20% compared to seven baseline methods. The findings underscore the dynamic and history-dependent nature of LLM safety behavior, advocating for a reevaluation of static alignment assumptions and the development of interaction-aware safety mechanisms.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）安全对齐的日益关注，尤其是在其快速发展的背景下。以往的越狱方法通常假设安全边界是静态的，这限制了它们的有效性，因为这些方法未考虑上下文交互对模型行为的动态影响。提出的MEEA（单纯曝光效应攻击）框架通过利用心理学启发的方法，采用重复的低毒性语义曝光，逐步改变模型的安全阈值，从而增强多轮安全评估的稳健性。这种方法的动机明确，因为它承认LLM安全行为的动态性和历史依赖性。研究方法包括构建语义渐进的提示链，并通过模拟退火策略进行优化，广泛实验表明MEEA在攻击成功率上比七个基线方法提高了超过20%，从而支持了对LLM进行交互感知安全评估的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth</div>
<div class="meta-line">Authors: Wenpeng Xing, Lanyi Wei, Haixiao Hu, Jingyi Yu, Rongchang Li, Mohan Li, Changting Lin, Meng Han</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-08-14T18:21:39+00:00 · Latest: 2025-12-13T15:39:32+00:00</div>
<div class="meta-line">Comments: Accepted in AAAI 2026 Workshop on AI for Education</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.11009v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.11009v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid proliferation of large language models (LLMs) in applications targeting children and adolescents necessitates a fundamental reassessment of prevailing AI safety frameworks, which are largely tailored to adult users and neglect the distinct developmental vulnerabilities of minors. This paper highlights key deficiencies in existing LLM safety benchmarks, including their inadequate coverage of age-specific cognitive, emotional, and social risks spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence (13--18). To bridge these gaps, we introduce SproutBench, an innovative evaluation suite comprising 1,283 developmentally grounded adversarial prompts designed to probe risks such as emotional dependency, privacy violations, and imitation of hazardous behaviors. Through rigorous empirical evaluation of 47 diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by robust inter-dimensional correlations (e.g., between Safety and Risk Prevention) and a notable inverse relationship between Interactivity and Age Appropriateness. These insights yield practical guidelines for advancing child-centric AI design and deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SproutBench：面向青少年的安全与伦理大型语言模型基准</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在针对儿童和青少年的应用中迅速普及，迫切需要对现有的人工智能安全框架进行根本性重新评估，这些框架主要针对成人用户，忽视了未成年人独特的发展脆弱性。本文强调了现有LLM安全基准的关键缺陷，包括对早期儿童（0-6岁）、中期儿童（7-12岁）和青少年（13-18岁）特定年龄的认知、情感和社会风险的覆盖不足。为填补这些空白，我们引入了SproutBench，一个创新的评估套件，包含1,283个基于发展阶段的对抗性提示，旨在探测情感依赖、隐私侵犯和模仿危险行为等风险。通过对47种多样化LLM的严格实证评估，我们发现了显著的安全脆弱性，得到了强有力的跨维度相关性（例如，安全性与风险预防之间）和互动性与年龄适宜性之间显著的反向关系。这些见解为推动以儿童为中心的人工智能设计和部署提供了实用指南。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the urgent need for AI safety frameworks that cater specifically to children and adolescents, as current large language models (LLMs) are primarily designed for adult users and overlook the unique vulnerabilities of younger populations. Previous methods have failed to adequately assess age-specific risks, leading to significant gaps in safety benchmarks. The proposed approach, SproutBench, introduces a comprehensive evaluation suite with 1,283 adversarial prompts that target developmental risks associated with emotional dependency, privacy violations, and hazardous behavior imitation. This methodology is well-motivated by the necessity to ensure the safety of LLMs for youth. The empirical evaluation of 47 diverse LLMs reveals critical safety vulnerabilities and establishes correlations between safety measures and developmental appropriateness, ultimately providing guidelines for improving child-centric AI design and deployment.</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在儿童和青少年应用中的广泛使用，关于其安全性的担忧日益增加，因为现有的人工智能安全框架主要关注成人用户，忽视了年轻人群体的独特脆弱性。以往评估LLM安全性的方法不足，未能解决与年龄相关的认知、情感和社会风险。本文提出了SproutBench，一个新的评估套件，包含1283个专门设计的对抗性提示，旨在评估不同发展阶段相关的风险，从而弥补当前基准的不足。该方法通过对47个LLM的实证评估，揭示了显著的安全脆弱性以及安全指标与发展适宜性之间的相关性。研究结果为改善面向青少年的人工智能系统提供了重要指导，表明所提出的方法有效支持确保年轻用户安全互动的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization</div>
<div class="meta-line">Authors: Yifan Niu, Han Xiao, Dongyi Liu, Nuo Chen, Jia Li</div>
<div class="meta-line">First: 2025-12-12T09:01:52+00:00 · Latest: 2025-12-12T09:01:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11391v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11391v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model&#x27;s original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过零空间约束策略优化减轻安全对齐税</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在现实世界应用中的日益普及，确保其行为与人类价值观、社会规范和伦理原则一致变得至关重要。然而，在强化学习（RL）中，安全对齐往往会遭遇遗忘已学通用能力的问题，这被称为对齐税。为了解决这个问题，我们引入了零空间约束策略优化（NSPO），这是一个新颖的RL框架，旨在实现LLM的安全对齐，同时保留其核心能力。安全策略梯度被几何投影到通用任务的零空间，从而减轻安全对齐税。此外，我们理论上证明了NSPO保留了模型的原始核心能力，同时仍然保证了有效安全对齐的下降方向。大量实验表明，NSPO在安全性能上大幅超越现有方法，在不牺牲通用任务（包括数学、代码和指令跟随任务）准确性的情况下，达到了最先进的安全性能。值得注意的是，NSPO数据效率高，仅需40%的来自PKU-SafeRLHF的公共人类标注安全数据即可实现良好的安全性能，而不需要现有对齐方法中大量混合通用任务数据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of aligning the behaviors of Large Language Models (LLMs) with human values and ethical principles, a process often hindered by the alignment tax, which results in the loss of learned general abilities during safety alignment in Reinforcement Learning (RL). Previous methods struggle with this issue, leading to a compromise between safety alignment and the preservation of core model capabilities. The proposed Null-Space constrained Policy Optimization (NSPO) framework offers a solution by geometrically projecting safety policy gradients into the null space of general tasks, effectively mitigating the alignment tax while maintaining the model&#x27;s original abilities. This paper contributes a novel RL approach that has been empirically validated, demonstrating significant improvements in safety performance on tasks such as math, code, and instruction-following, achieving state-of-the-art results with only 40% of the typical human-annotated safety data required, thus supporting the goal of effective safety alignment without sacrificing general task accuracy.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）与人类价值观和伦理原则对齐的挑战，这一任务因对齐税现象而变得复杂，即安全对齐可能导致一般能力的丧失。以往的方法往往无法在确保安全的同时保持这些核心能力，从而导致性能不佳。提出的方法，即零空间约束策略优化（NSPO），通过将安全策略梯度投影到一般任务的零空间中来缓解这一问题，从而在促进有效安全对齐的同时保持模型的原始能力。本文贡献了一种新颖的强化学习框架，显示出显著优于现有方法的改进，在数学、代码和遵循指令等各种任务上实现了最先进的安全性能，同时仅需40%的典型人类标注安全数据即可取得有效结果。</div>
</details>
</div>
<div class="card">
<div class="title">Challenges of Evaluating LLM Safety for User Welfare</div>
<div class="meta-line">Authors: Manon Kempermann, Sai Suresh Macharla Vasu, Mahalakshmi Raveenthiran, Theo Farrell, Ingmar Weber</div>
<div class="meta-line">First: 2025-12-11T14:34:40+00:00 · Latest: 2025-12-11T14:34:40+00:00</div>
<div class="meta-line">Comments: Paper accepted at IASEAI&#x27;26; please cite that peer-reviewed version instead</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10687v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.10687v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD&#x27;s AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估大型语言模型安全性对用户福祉的挑战</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的安全性评估通常关注普遍风险，如危险能力或不良倾向。然而，数百万用户在金融和健康等高风险主题上使用LLMs获取个人建议，这些危害是依赖于具体情境的，而非普遍的。尽管像OECD的人工智能分类这样的框架认识到评估个体风险的必要性，但用户福祉的安全性评估仍然不够成熟。我们认为，开发此类评估并非易事，因为在评估设计中考虑用户情境存在根本性问题。在这项探索性研究中，我们评估了来自GPT-5、Claude Sonnet 4和Gemini 2.5 Pro的金融和健康建议，针对不同脆弱性用户的个人资料进行评估。首先，我们证明评估者必须获得丰富的用户情境信息：相同的LLM回应在对用户情况不知情的评估者那里被评为显著更安全，而对知情评估者的安全评分则从高脆弱用户的安全（5/7）降至稍微不安全（3/7）。人们可能会认为，通过创建包含关键信息的现实用户提示可以解决这一差距。然而，我们的第二项研究对此提出了挑战：我们在包含用户报告会披露的情境的提示上重新进行评估，发现没有显著改善。我们的研究表明，有效的用户福祉安全评估要求评估者根据多样的用户资料评估回应，因为仅仅依靠现实的用户情境披露是不够的，尤其是对于脆弱群体。通过展示一种情境感知评估的方法论，本研究为此类评估提供了起点，并提供了基础证据，表明评估个体福祉需要与现有普遍风险框架不同的方法。我们发布了我们的代码和数据集，以帮助未来的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the inadequacies of current safety evaluations for large language models (LLMs), which often overlook the context-dependent nature of user welfare, particularly in high-stakes areas like finance and health. Previous methods have primarily focused on universal risks, failing to account for individual user circumstances, which can lead to misleading safety assessments. The proposed approach emphasizes the necessity of incorporating rich user context into evaluations, demonstrating that context-aware assessments yield significantly different safety ratings compared to context-blind evaluations. The study employs a methodology that evaluates LLM advice across various user profiles, revealing that realistic user context disclosure alone is insufficient for accurate safety evaluations, especially for vulnerable populations. The findings indicate that effective user-welfare safety evaluations must consider diverse user profiles, thus contributing a new framework for assessing LLM safety that diverges from traditional universal-risk models and providing a foundation for future research in this area.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）安全评估中的不足之处，传统评估通常关注普遍风险，而忽视了用户在寻求金融和健康等关键主题建议时所面临的情境依赖性危害。以往的方法未能考虑个体用户的背景，导致安全评估结果误导。所提出的方法强调在评估中纳入多样化用户画像的必要性，研究表明，与不考虑背景的评估相比，考虑用户背景的评估在高脆弱性用户中显著降低了安全评分。该研究贡献了一种新的情境感知安全评估方法，证明仅依赖真实的用户背景披露不足以进行准确评估。研究在不同用户脆弱性下评估LLM的响应，强调了量身定制评估框架的必要性，并为未来用户福利安全评估的工作奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</div>
<div class="meta-line">Authors: Sanket Badhe</div>
<div class="meta-line">Venue: Proceedings of Machine Learning Research 299, 2025 Conference on Applied Machine Learning for Information Security</div>
<div class="meta-line">First: 2025-08-08T17:01:41+00:00 · Latest: 2025-12-09T18:09:00+00:00</div>
<div class="meta-line">Comments: Accepted at CAMLIS 25: Conference on Applied Machine Learning for Information Security. 19 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.06457v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.06457v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ScamAgents：人工智能代理如何模拟人类级别的诈骗电话</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）展示了令人印象深刻的流利性和推理能力，但其潜在的误用引发了日益关注。本文介绍了ScamAgent，一个基于LLMs构建的自主多轮代理，能够生成高度逼真的诈骗电话脚本，模拟现实世界的欺诈场景。与以往关注单次提示误用的工作不同，ScamAgent保持对话记忆，动态适应模拟用户响应，并在对话轮次中采用欺骗性劝说策略。我们表明，当前LLM安全防护措施，包括拒绝机制和内容过滤器，对这种基于代理的威胁无效。即使是具有强大提示级别保护的模型，在提示被分解、伪装或在代理框架内逐步传递时也可以被绕过。我们进一步展示了使用现代文本转语音系统将诈骗脚本转化为逼真的语音电话，完成一个完全自动化的诈骗管道。我们的研究结果强调了对多轮安全审计、代理级控制框架以及检测和破坏由生成性人工智能驱动的对话欺骗的新方法的迫切需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concern over the misuse of Large Language Models (LLMs) in generating realistic scam calls, a significant issue given their fluency and reasoning capabilities. Previous methods primarily focused on single-shot prompt misuse, which proved inadequate against more sophisticated threats. The proposed ScamAgent differs by utilizing an autonomous multi-turn dialogue system that maintains memory, adapts to user responses, and employs deceptive strategies throughout the conversation, effectively bypassing existing safety measures. This paper contributes to the field by demonstrating the limitations of current LLM safety guardrails and highlighting the need for enhanced multi-turn safety auditing and detection methods. The methodology involves generating scam call scripts and transforming them into voice calls using text-to-speech systems, achieving a fully automated scam pipeline that underscores the urgency for improved controls against such AI-driven threats.</div>
<div class="mono" style="margin-top:8px">本文关注大型语言模型（LLMs）在生成真实诈骗电话方面的滥用问题。以往的方法主要集中在单次提示的滥用上，未能考虑对话的动态和多轮特性，导致现有安全机制存在漏洞。提出的ScamAgent通过保持对话记忆并适应用户反应，改进了这些方法，有效地在对话中运用欺骗策略。这一方法的提出是基于对不断演变的威胁需要增强安全措施的合理动机。研究方法涉及创建一个自主代理，生成诈骗电话脚本，并使用文本转语音技术将其转化为语音电话。研究结果表明，当前的安全防护措施不足，强调了开发新检测方法和控制框架以应对AI生成交互中的对话欺骗的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate</div>
<div class="meta-line">Authors: Shenzhe Zhu</div>
<div class="meta-line">First: 2025-12-09T17:56:38+00:00 · Latest: 2025-12-09T17:56:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23717v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23717v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are equipped with safety mechanisms to detect and block harmful queries, yet current alignment approaches primarily focus on overtly dangerous content and overlook more subtle threats. However, users can often disguise harmful intent through covert rephrasing that preserves malicious objectives while appearing benign, which creates a significant gap in existing safety training data. To address this limitation, we introduce HarmTransform, a multi-agent debate framework for systematically transforming harmful queries into stealthier forms while preserving their underlying harmful intent. Our framework leverages iterative critique and refinement among multiple agents to generate high-quality, covert harmful query transformations that can be used to improve future LLM safety alignment. Experiments demonstrate that HarmTransform significantly outperforms standard baselines in producing effective query transformations. At the same time, our analysis reveals that debate acts as a double-edged sword: while it can sharpen transformations and improve stealth, it may also introduce topic shifts and unnecessary complexity. These insights highlight both the promise and the limitations of multi-agent debate for generating comprehensive safety training data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HarmTransform：通过多智能体辩论将显性有害查询转化为隐性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）配备了安全机制以检测和阻止有害查询，但当前的对齐方法主要关注明显危险的内容，忽视了更微妙的威胁。然而，用户常常通过隐蔽的重述来掩饰有害意图，这种重述保留了恶意目标，同时看起来无害，从而在现有的安全训练数据中造成了显著的缺口。为了解决这一局限性，我们引入了HarmTransform，一个多智能体辩论框架，系统地将有害查询转化为更隐蔽的形式，同时保留其潜在的有害意图。我们的框架利用多个智能体之间的迭代批评和改进，生成高质量的隐蔽有害查询转化，这可以用于改善未来LLM的安全对齐。实验表明，HarmTransform在生成有效查询转化方面显著优于标准基线。同时，我们的分析揭示了辩论作为一把双刃剑的作用：虽然它可以提升转化的效果和隐蔽性，但也可能引入主题偏移和不必要的复杂性。这些见解突显了多智能体辩论在生成全面安全训练数据方面的潜力和局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the limitations of current safety mechanisms in large language models (LLMs), which primarily focus on overtly harmful content and fail to detect more subtle, disguised threats. Previous methods have not effectively tackled the issue of covert rephrasing that maintains harmful intent while appearing benign. The proposed approach, HarmTransform, introduces a multi-agent debate framework that systematically transforms harmful queries into stealthier versions while preserving their malicious objectives, thus filling a significant gap in safety training data. The methodology involves iterative critique and refinement among multiple agents to generate high-quality covert harmful query transformations. Experimental results show that HarmTransform significantly outperforms standard baselines in producing effective query transformations, supporting the goal of enhancing LLM safety alignment while also revealing the complexities introduced by the debate process.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）当前安全机制的局限性，这些机制主要关注明显的有害内容，而未能检测到更微妙的隐蔽有害查询。现有方法未能充分考虑用户伪装有害意图的方式，导致安全训练数据存在空白。提出的方法HarmTransform引入了一种多智能体辩论框架，系统地将有害查询转化为更隐蔽的形式，同时保持其潜在的恶意意图。该方法动机明确，旨在通过智能体之间的迭代批评生成高质量的隐蔽有害查询转化，从而增强LLMs的安全对齐。实验结果表明，HarmTransform在生成有效查询转化方面显著优于标准基线，尽管它也揭示了潜在的缺点，如主题转移和复杂性增加，强调了使用多智能体辩论生成安全训练数据的微妙影响。</div>
</details>
</div>
<div class="card">
<div class="title">GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering</div>
<div class="meta-line">Authors: Jehyeok Yeon, Federico Cinus, Yifan Wu, Luca Luceri</div>
<div class="meta-line">First: 2025-12-07T04:46:30+00:00 · Latest: 2025-12-07T04:46:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.06655v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.06655v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining &gt;= 90% refusal of harmful content.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GSAE：用于稳健LLM安全引导的图正则稀疏自编码器</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）面临严重的安全挑战，因为它们可能被操控生成有害内容，通过对抗性提示和越狱攻击。许多防御通常是黑箱护栏，过滤输出，或基于内部的方法，通过将安全性操作化为单一潜在特征或维度来引导隐藏激活。虽然对简单概念有效，但这一假设是有限的，因为最近的证据表明，拒绝和时间性等抽象概念分布在多个特征上，而不是孤立在一个特征中。为了解决这一限制，我们引入了图正则稀疏自编码器（GSAEs），它在神经元共激活图上扩展了自编码器（SAEs）的拉普拉斯平滑惩罚。与将每个概念分配给单一潜在特征的标准SAEs不同，GSAEs恢复平滑、分布式的安全表示，作为跨多个特征的连贯模式。我们通过实验证明，GSAE能够有效地进行运行时安全引导，将特征组装成加权的安全相关方向，并通过两阶段门控机制控制它们，仅在生成过程中检测到有害提示或延续时激活干预。这种方法在保持对良性查询的效用的同时，自适应地强制拒绝。在安全和问答基准测试中，GSAE引导实现了平均82%的选择性拒绝率，显著优于标准SAE引导（42%），同时保持强大的任务准确性（TriviaQA 70%，TruthfulQA 65%，GSM8K 74%）。稳健性实验进一步显示了在LLaMA-3、Mistral、Qwen和Phi系列中的泛化能力，以及对越狱攻击（GCG，AutoDAN）的抗性，始终保持对有害内容的拒绝率&gt;= 90%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the critical safety challenges faced by large language models (LLMs), which can be exploited to produce harmful content through adversarial prompts. Previous methods, including black-box guardrails and internal-based approaches, often oversimplify safety by treating it as a single latent feature, which is inadequate for complex concepts that are distributed across multiple features. The proposed Graph-Regularized Sparse Autoencoders (GSAEs) improve upon these methods by incorporating a Laplacian smoothness penalty on the neuron co-activation graph, allowing for the recovery of distributed safety representations. The methodology enables effective runtime safety steering by assembling features into a weighted set of safety-relevant directions, controlled by a two-stage gating mechanism that activates interventions only when harmful prompts are detected. The GSAE approach achieves an average selective refusal rate of 82%, significantly surpassing the 42% rate of standard SAE steering, while maintaining strong task accuracy across various benchmarks, thus demonstrating its effectiveness in enhancing LLM safety without compromising performance.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）面临的关键安全挑战，特别是它们在对抗性提示和越狱攻击下生成有害内容的脆弱性。以往的方法，包括黑箱护栏和基于内部的方法，通常将安全性简化为单一潜在特征，这限制了它们对复杂概念的有效性。提出的图正则稀疏自编码器（GSAEs）通过在神经元共激活图上引入拉普拉斯平滑惩罚，改善了这些方法，使其能够恢复跨多个特征的分布式安全表示。这种合理的方法使得有效的运行时安全引导成为可能，在各种基准测试中实现了平均82%的选择性拒绝率，同时在任务准确性上保持强劲，显著优于标准SAE引导，并在多个LLM家族中展示了对越狱攻击的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">ORFuzz: Fuzzing the &quot;Other Side&quot; of LLM Safety -- Testing Over-Refusal</div>
<div class="meta-line">Authors: Haonan Zhang, Dongxia Wang, Yi Liu, Kexin Chen, Jiashui Wang, Xinlei Ying, Long Liu, Wenhai Wang</div>
<div class="meta-line">First: 2025-08-15T05:03:26+00:00 · Latest: 2025-12-05T05:36:55+00:00</div>
<div class="meta-line">Comments: Accepted by ASE 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.11222v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.11222v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously rejecting benign queries due to overly conservative safety measures - a critical functional flaw that undermines their reliability and usability. Current methods for testing this behavior are demonstrably inadequate, suffering from flawed benchmarks and limited test generation capabilities, as highlighted by our empirical user study. To the best of our knowledge, this paper introduces the first evolutionary testing framework, ORFuzz, for the systematic detection and analysis of LLM over-refusals. ORFuzz uniquely integrates three core components: (1) safety category-aware seed selection for comprehensive test coverage, (2) adaptive mutator optimization using reasoning LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge model validated to accurately reflect user perception of toxicity and refusal. Our extensive evaluations demonstrate that ORFuzz generates diverse, validated over-refusal instances at a rate (6.98% average) more than double that of leading baselines, effectively uncovering vulnerabilities. Furthermore, ORFuzz&#x27;s outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly transferable test cases that achieves a superior 63.56% average over-refusal rate across 10 diverse LLMs, significantly outperforming existing datasets. ORFuzz and ORFuzzSet provide a robust automated testing framework and a valuable community resource, paving the way for developing more reliable and trustworthy LLM-based software systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ORFuzz：模糊测试大型语言模型安全性的“另一面”——测试过度拒绝</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越表现出过度拒绝——由于过于保守的安全措施错误地拒绝良性查询——这是一个关键的功能缺陷，削弱了它们的可靠性和可用性。目前测试这种行为的方法显然不够，存在基准测试缺陷和有限的测试生成能力，正如我们的实证用户研究所强调的。根据我们所知，本文首次引入了进化测试框架ORFuzz，用于系统检测和分析LLM的过度拒绝。ORFuzz独特地整合了三个核心组件：（1）安全类别感知的种子选择，以实现全面的测试覆盖；（2）使用推理LLMs的自适应变异器优化，以生成有效的测试用例；（3）OR-Judge，一个经过验证的人类对齐评判模型，能够准确反映用户对毒性和拒绝的感知。我们的广泛评估表明，ORFuzz以超过领先基准的两倍（平均6.98%）的速度生成多样化、经过验证的过度拒绝实例，有效揭示了脆弱性。此外，ORFuzz的输出构成了ORFuzzSet的基础，这是一个包含1,855个高度可转移测试用例的新基准，在10个不同的LLM中实现了优越的63.56%平均过度拒绝率，显著优于现有数据集。ORFuzz和ORFuzzSet提供了一个强大的自动化测试框架和有价值的社区资源，为开发更可靠和可信赖的基于LLM的软件系统铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical issue of over-refusal in Large Language Models (LLMs), where these models incorrectly reject benign queries due to excessive safety precautions, impacting their reliability and usability. Existing testing methods are inadequate, suffering from flawed benchmarks and limited test generation capabilities, which ORFuzz aims to improve upon by introducing an evolutionary testing framework that systematically detects and analyzes over-refusals. The proposed method is well-motivated as it integrates safety category-aware seed selection, adaptive mutator optimization using reasoning LLMs, and OR-Judge, a human-aligned judge model, to enhance test coverage and effectiveness. The paper&#x27;s contributions include the development of ORFuzz, which generates diverse over-refusal instances at an average rate of 6.98%, more than double that of leading baselines, and the creation of ORFuzzSet, a benchmark of 1,855 test cases that achieves a 63.56% average over-refusal rate across 10 diverse LLMs, significantly outperforming existing datasets and providing a valuable resource for improving LLM safety.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLM）中的过度拒绝问题，即由于过于谨慎的安全措施而错误拒绝良性查询，这一问题严重影响了模型的可靠性。以往的测试方法由于基准不完善和测试生成能力有限而显得不足，因此需要一种更有效的方法。提出的ORFuzz框架引入了一种进化测试方法，结合了安全类别感知的种子选择、自适应变异优化（使用推理LLM）和人类对齐的评判模型（OR-Judge），以系统性地识别和分析过度拒绝现象。这种方法显著提高了测试覆盖率和有效性，平均检测到的过度拒绝率为6.98%，是现有方法的两倍以上。此外，ORFuzzSet作为一个新的基准，包含1855个测试用例，在十种不同的LLM上实现了63.56%的平均过度拒绝率，从而为提高LLM安全性提供了强大的自动化测试框架和宝贵资源。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Jailbreak Attacks on LLMs via Persona Prompts</div>
<div class="meta-line">Authors: Zheng Zhang, Peilin Zhao, Deheng Ye, Hao Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-07-28T12:03:22+00:00 · Latest: 2025-11-30T18:50:44+00:00</div>
<div class="meta-line">Comments: Workshop on LLM Persona Modeling at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.22171v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.22171v2">PDF</a> · <a href="https://github.com/CjangCjengh/Generic_Persona">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Jailbreak attacks aim to exploit large language models (LLMs) by inducing them to generate harmful content, thereby revealing their vulnerabilities. Understanding and addressing these attacks is crucial for advancing the field of LLM safety. Previous jailbreak approaches have mainly focused on direct manipulations of harmful intent, with limited attention to the impact of persona prompts. In this study, we systematically explore the efficacy of persona prompts in compromising LLM defenses. We propose a genetic algorithm-based method that automatically crafts persona prompts to bypass LLM&#x27;s safety mechanisms. Our experiments reveal that: (1) our evolved persona prompts reduce refusal rates by 50-70% across multiple LLMs, and (2) these prompts demonstrate synergistic effects when combined with existing attack methods, increasing success rates by 10-20%. Our code and data are available at https://github.com/CjangCjengh/Generic_Persona.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过角色提示增强对大型语言模型的越狱攻击</div>
<div class="mono" style="margin-top:8px">越狱攻击旨在利用大型语言模型（LLMs），诱使其生成有害内容，从而揭示其脆弱性。理解和应对这些攻击对推动LLM安全领域至关重要。以往的越狱方法主要集中在对有害意图的直接操控上，而对角色提示的影响关注较少。在本研究中，我们系统地探讨了角色提示在破坏LLM防御中的有效性。我们提出了一种基于遗传算法的方法，自动生成角色提示以绕过LLM的安全机制。我们的实验表明：（1）我们演化的角色提示在多个LLM中将拒绝率降低了50-70%；（2）这些提示与现有攻击方法结合时表现出协同效应，成功率提高了10-20%。我们的代码和数据可在https://github.com/CjangCjengh/Generic_Persona获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the critical issue of jailbreak attacks on large language models (LLMs), which exploit these models to generate harmful content and expose their vulnerabilities. Previous methods primarily focused on direct manipulations of harmful intent, often neglecting the role of persona prompts, which this study aims to investigate. The proposed approach utilizes a genetic algorithm to automatically generate persona prompts that effectively bypass LLM safety mechanisms, addressing the limitations of earlier techniques. The contribution of this research lies in demonstrating that the evolved persona prompts can significantly reduce refusal rates by 50-70% across various LLMs and enhance the effectiveness of existing attack methods by increasing success rates by 10-20%. This methodology showcases a novel way to improve the understanding of LLM vulnerabilities and advance safety measures in the field.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在监狱攻击中的脆弱性，这些攻击利用这些模型生成有害内容。以往的方法主要集中在直接操控有害意图上，忽视了角色提示的作用，而本研究则将其视为影响LLM防御的重要因素。所提出的方法利用遗传算法自动生成角色提示，有效绕过安全机制，解决了早期方法的局限性。论文的贡献在于证明这些演变的角色提示可以在多个LLM中显著降低拒绝率50-70%，并提高现有攻击方法的成功率10-20%。该方法展示了一种新颖的方式来破坏LLM防御，支持了推进LLM安全研究的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient LLM-Jailbreaking via Multimodal-LLM Jailbreak</div>
<div class="meta-line">Authors: Haoxuan Ji, Zheng Lin, Zhenxing Niu, Xinbo Gao, Gang Hua</div>
<div class="meta-line">First: 2024-05-30T12:50:32+00:00 · Latest: 2025-11-30T09:33:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.20015v3">Abs</a> · <a href="https://arxiv.org/pdf/2405.20015v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper focuses on jailbreaking attacks against large language models (LLMs), eliciting them to generate objectionable content in response to harmful user queries. Unlike previous LLM-jailbreak methods that directly orient to LLMs, our approach begins by constructing a multimodal large language model (MLLM) built upon the target LLM. Subsequently, we perform an efficient MLLM jailbreak and obtain a jailbreaking embedding. Finally, we convert the embedding into a textual jailbreaking suffix to carry out the jailbreak of target LLM. Compared to the direct LLM-jailbreak methods, our indirect jailbreaking approach is more efficient, as MLLMs are more vulnerable to jailbreak than pure LLM. Additionally, to improve the attack success rate of jailbreak, we propose an image-text semantic matching scheme to identify a suitable initial input. Extensive experiments demonstrate that our approach surpasses current state-of-the-art jailbreak methods in terms of both efficiency and effectiveness. Moreover, our approach exhibits superior cross-class generalization abilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过多模态LLM越狱实现高效LLM越狱</div>
<div class="mono" style="margin-top:8px">本文聚焦于针对大型语言模型（LLMs）的越狱攻击，诱使其对有害用户查询生成不当内容。与之前直接针对LLM的越狱方法不同，我们的方法首先构建一个基于目标LLM的多模态大型语言模型（MLLM）。随后，我们执行高效的MLLM越狱并获得越狱嵌入。最后，我们将嵌入转换为文本越狱后缀，以实现目标LLM的越狱。与直接的LLM越狱方法相比，我们的间接越狱方法更高效，因为MLLM对越狱的脆弱性高于纯LLM。此外，为了提高越狱的成功率，我们提出了一种图像-文本语义匹配方案，以识别合适的初始输入。大量实验表明，我们的方法在效率和有效性方面超越了当前最先进的越狱方法。此外，我们的方法表现出优越的跨类泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of jailbreaking large language models (LLMs) to generate objectionable content, a task that has seen previous methods directly targeting LLMs but often resulting in inefficiencies and limited success rates. The proposed approach differs by first constructing a multimodal large language model (MLLM) based on the target LLM, allowing for a more efficient jailbreak process through the creation of a jailbreaking embedding that is transformed into a textual suffix for the target LLM. This method is well-motivated as it leverages the increased vulnerability of MLLMs to jailbreak attempts. The contribution of the paper lies in its innovative use of an image-text semantic matching scheme to enhance the initial input selection, leading to improved attack success rates. Extensive experiments show that this method outperforms existing state-of-the-art jailbreak techniques in both efficiency and effectiveness, while also demonstrating strong cross-class generalization capabilities.</div>
<div class="mono" style="margin-top:8px">本文研究了针对大型语言模型（LLMs）的越狱攻击，以引导其对有害查询生成不当内容，强调了以往直接针对LLMs的方法的局限性。所提出的方法通过基于目标LLM构建多模态大型语言模型（MLLM），从而实现更高效的越狱过程，这一方法的动机在于发现MLLM比传统LLM更容易被越狱。本文的贡献在于开发了一种高效的MLLM越狱技术，包括图像-文本语义匹配方案，以提高攻击成功率。实验结果表明，该方法在效率和有效性上均优于现有的最先进方法，并展现出强大的跨类别泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations</div>
<div class="meta-line">Authors: Somnath Banerjee, Pratyush Chatterjee, Shanu Kumar, Sayan Layek, Parag Agrawal, Rima Hazra, Animesh Mukherjee</div>
<div class="meta-line">First: 2025-05-20T15:05:03+00:00 · Latest: 2025-11-30T08:20:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.14469v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.14469v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While LLMs appear robustly safety-aligned in English, we uncover a catastrophic, overlooked weakness: attributional collapse under code-mixed perturbations. Our systematic evaluation of open models shows that the linguistic camouflage of code-mixing -- ``blending languages within a single conversation&#x27;&#x27; -- can cause safety guardrails to fail dramatically. Attack success rates (ASR) spike from a benign 9\% in monolingual English to 69\% under code-mixed inputs, with rates exceeding 90\% in non-Western contexts such as Arabic and Hindi. These effects hold not only on controlled synthetic datasets but also on real-world social media traces, revealing a serious risk for billions of users. To explain why this happens, we introduce saliency drift attribution (SDA), an interpretability framework that shows how, under code-mixing, the model&#x27;s internal attention drifts away from safety-critical tokens (e.g., ``violence&#x27;&#x27; or ``corruption&#x27;&#x27;), effectively blinding it to harmful intent. Finally, we propose a lightweight translation-based restoration strategy that recovers roughly 80\% of the safety lost to code-mixing, offering a practical path toward more equitable and robust LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代码混合扰动下大型语言模型的归因安全失败</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型在英语中似乎与安全高度一致，但我们发现了一个灾难性的、被忽视的弱点：在代码混合扰动下的归因崩溃。我们对开放模型的系统评估表明，代码混合的语言伪装——“在单一对话中混合语言”——会导致安全防护措施显著失效。攻击成功率（ASR）在单语英语中为9\%，而在代码混合输入下飙升至69\%，在阿拉伯语和印地语等非西方背景下超过90\%。这些影响不仅在受控的合成数据集上存在，也在真实的社交媒体痕迹中显现，揭示了对数十亿用户的严重风险。为了解释这一现象，我们引入了显著性漂移归因（SDA），这是一个可解释性框架，展示了在代码混合下，模型的内部注意力如何偏离安全关键标记（例如，“暴力”或“腐败”），有效地使其对有害意图失明。最后，我们提出了一种轻量级的基于翻译的恢复策略，恢复了大约80\%因代码混合而失去的安全性，为实现更公平和稳健的大型语言模型安全提供了切实可行的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the overlooked vulnerability of large language models (LLMs) to code-mixed perturbations, which can lead to significant safety failures. Previous methods have not adequately considered the impact of blending languages within a single conversation, resulting in a dramatic increase in attack success rates from 9% in monolingual English to 69% under code-mixed inputs, particularly in non-Western contexts. The proposed approach introduces saliency drift attribution (SDA) to explain how code-mixing causes models to lose focus on safety-critical tokens, and it includes a translation-based restoration strategy that recovers approximately 80% of the lost safety. The methodology demonstrates that LLMs can be significantly more robust against safety failures in code-mixed scenarios, achieving a practical solution that enhances user safety for billions of individuals across diverse linguistic backgrounds.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在面对代码混合语言输入时的重大安全漏洞，这种输入在单一对话中混合多种语言。以往的方法未能充分考虑代码混合的影响，导致攻击成功率从单语英语的9%激增至代码混合情况下的69%，尤其是在非西方语言中。本文的贡献在于引入了显著性漂移归因（SDA），这一可解释性框架阐明了代码混合如何导致模型忽视安全关键标记。所提出的方法包括一种轻量级的基于翻译的恢复策略，成功恢复了约80%因代码混合而损失的安全性。这一方法在安全性能上显示出显著改善，突显了在多样语言环境中增强LLMs鲁棒性的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization</div>
<div class="meta-line">Authors: Xurui Li, Kaisong Song, Rui Zhu, Pin-Yu Chen, Haixu Tang</div>
<div class="meta-line">First: 2025-11-24T15:23:41+00:00 · Latest: 2025-11-26T15:12:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.19218v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.19218v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have developed rapidly in web services, delivering unprecedented capabilities while amplifying societal risks. Existing works tend to focus on either isolated jailbreak attacks or static defenses, neglecting the dynamic interplay between evolving threats and safeguards in real-world web contexts. To mitigate these challenges, we propose ACE-Safety (Adversarial Co-Evolution for LLM Safety), a novel framework that jointly optimize attack and defense models by seamlessly integrating two key innovative procedures: (1) Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS), which efficiently explores jailbreak strategies to uncover vulnerabilities and generate diverse adversarial samples; (2) Adversarial Curriculum Tree-aware Group Policy Optimization (AC-TGPO), which jointly trains attack and defense LLMs with challenging samples via curriculum reinforcement learning, enabling robust mutual improvement. Evaluations across multiple benchmarks demonstrate that our method outperforms existing attack and defense approaches, and provides a feasible pathway for developing LLMs that can sustainably support responsible AI ecosystems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过树组双重意识搜索与优化实现大语言模型安全对齐的对抗攻击-防御共演</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在网络服务中迅速发展，提供了前所未有的能力，同时加剧了社会风险。现有研究往往集中于孤立的越狱攻击或静态防御，忽视了在现实网络环境中不断演变的威胁与防护之间的动态互动。为了解决这些挑战，我们提出了ACE-Safety（对抗共演以实现LLM安全），这是一个新颖的框架，通过无缝整合两个关键创新程序共同优化攻击和防御模型：（1）群体意识策略引导的蒙特卡洛树搜索（GS-MCTS），有效探索越狱策略以发现漏洞并生成多样的对抗样本；（2）对抗课程树意识群体策略优化（AC-TGPO），通过课程强化学习共同训练攻击和防御LLM，利用具有挑战性的样本，实现稳健的相互提升。多个基准测试的评估表明，我们的方法优于现有的攻击和防御方法，并为开发能够可持续支持负责任的人工智能生态系统的LLM提供了可行的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the rapid development of Large Language Models (LLMs) and the associated societal risks, highlighting the inadequacy of existing methods that focus on either isolated jailbreak attacks or static defenses without considering their dynamic interactions. The proposed ACE-Safety framework distinguishes itself by integrating a Group-aware Strategy-guided Monte Carlo Tree Search and Adversarial Curriculum Tree-aware Group Policy Optimization, which together optimize both attack and defense models in a co-evolutionary manner. This approach effectively uncovers vulnerabilities and generates diverse adversarial samples while enabling mutual improvement through curriculum reinforcement learning. The methodology demonstrates superior performance across multiple benchmarks compared to existing methods, supporting the goal of developing LLMs that contribute to responsible AI ecosystems.</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的快速发展带来了显著的社会风险，现有研究主要关注孤立的越狱攻击或静态防御，忽视了不断演变的威胁与保护措施之间的动态关系。提出的ACE-Safety框架引入了一种新颖的方法，结合了基于组的策略引导蒙特卡洛树搜索（GS-MCTS），用于探索漏洞和生成对抗样本，以及对抗性课程树感知组策略优化（AC-TGPO），通过课程强化学习共同训练攻击和防御模型。这种共同进化策略有效增强了攻击和防御机制的鲁棒性，在多个基准测试中表现优于传统方法，从而为负责任的人工智能生态系统的发展做出了贡献。</div>
</details>
</div>
<div class="card">
<div class="title">Jailbreaking and Mitigation of Vulnerabilities in Large Language Models</div>
<div class="meta-line">Authors: Benji Peng, Keyu Chen, Qian Niu, Ziqian Bi, Ming Liu, Pohsun Feng, Tianyang Wang, Lawrence K. Q. Yan, Yizhu Wen, Yichao Zhang, Caitlyn Heqi Yin, Xinyuan Song</div>
<div class="meta-line">First: 2024-10-20T00:00:56+00:00 · Latest: 2025-11-25T14:24:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.15236v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.15236v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have transformed artificial intelligence by advancing natural language understanding and generation, enabling applications across fields beyond healthcare, software engineering, and conversational systems. Despite these advancements in the past few years, LLMs have shown considerable vulnerabilities, particularly to prompt injection and jailbreaking attacks. This review analyzes the state of research on these vulnerabilities and presents available defense strategies. We roughly categorize attack approaches into prompt-based, model-based, multimodal, and multilingual, covering techniques such as adversarial prompting, backdoor injections, and cross-modality exploits. We also review various defense mechanisms, including prompt filtering, transformation, alignment techniques, multi-agent defenses, and self-regulation, evaluating their strengths and shortcomings. We also discuss key metrics and benchmarks used to assess LLM safety and robustness, noting challenges like the quantification of attack success in interactive contexts and biases in existing datasets. Identifying current research gaps, we suggest future directions for resilient alignment strategies, advanced defenses against evolving attacks, automation of jailbreak detection, and consideration of ethical and societal impacts. This review emphasizes the need for continued research and cooperation within the AI community to enhance LLM security and ensure their safe deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型语言模型的越狱与漏洞缓解</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）通过推动自然语言理解和生成，改变了人工智能，使其在医疗、软件工程和对话系统等多个领域的应用成为可能。尽管近年来取得了这些进展，LLMs仍显示出相当大的脆弱性，特别是对提示注入和越狱攻击。本文回顾了这些漏洞的研究现状，并提出了可用的防御策略。我们大致将攻击方法分为基于提示、基于模型、多模态和多语言，涵盖对抗性提示、后门注入和跨模态利用等技术。我们还回顾了各种防御机制，包括提示过滤、转换、对齐技术、多智能体防御和自我调节，评估其优缺点。我们讨论了用于评估LLM安全性和鲁棒性的关键指标和基准，指出了在交互环境中量化攻击成功率和现有数据集偏见等挑战。识别当前研究空白，我们建议未来在弹性对齐策略、针对不断演变的攻击的高级防御、越狱检测的自动化以及伦理和社会影响的考虑等方面的研究方向。本文强调了在AI社区内持续研究和合作的必要性，以增强LLM安全性并确保其安全部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the significant vulnerabilities of Large Language Models (LLMs) to various attacks, such as prompt injection and jailbreaking, which have emerged despite their advancements in natural language processing. Previous methods for defending against these vulnerabilities, including prompt filtering and multi-agent defenses, have shown limitations in effectiveness and adaptability. The proposed approach emphasizes a comprehensive review of existing attack strategies and defense mechanisms, identifying gaps in current research and suggesting future directions for improving LLM security. The methodology involves categorizing attack types and evaluating defense strategies while considering metrics for assessing safety and robustness. The findings highlight the necessity for ongoing research and collaboration in the AI community to develop more resilient alignment strategies and advanced defenses, ultimately aiming to enhance the safe deployment of LLMs.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）显著的脆弱性，特别是它们对提示注入和越狱攻击的易感性，这在各种应用中构成风险。以往的防御方法，如提示过滤和多代理防御，显示出在有效性和适应性方面的局限性。所提出的方法强调对现有攻击策略和防御机制的全面审查，识别当前研究中的空白，并建议未来更具韧性的对齐策略和先进防御的方向。该方法论包括对攻击类型的分类和对防御强度的评估，最终有助于更好地理解LLM的安全性和稳健性。本文强调了持续研究和合作以改善LLM安全性的必要性，旨在提高减轻脆弱性和确保在实际应用中安全部署的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Differentiated Directional Intervention A Framework for Evading LLM Safety Alignment</div>
<div class="meta-line">Authors: Peng Zhang, Peijie Sun</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-11-10T08:52:34+00:00 · Latest: 2025-11-24T11:44:59+00:00</div>
<div class="meta-line">Comments: AAAI-26-AIA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.06852v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.06852v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safety alignment instills in Large Language Models (LLMs) a critical capacity to refuse malicious requests. Prior works have modeled this refusal mechanism as a single linear direction in the activation space. We posit that this is an oversimplification that conflates two functionally distinct neural processes: the detection of harm and the execution of a refusal. In this work, we deconstruct this single representation into a Harm Detection Direction and a Refusal Execution Direction. Leveraging this fine-grained model, we introduce Differentiated Bi-Directional Intervention (DBDI), a new white-box framework that precisely neutralizes the safety alignment at critical layer. DBDI applies adaptive projection nullification to the refusal execution direction while suppressing the harm detection direction via direct steering. Extensive experiments demonstrate that DBDI outperforms prominent jailbreaking methods, achieving up to a 97.88\% attack success rate on models such as Llama-2. By providing a more granular and mechanistic framework, our work offers a new direction for the in-depth understanding of LLM safety alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>差异化定向干预：规避大型语言模型安全对齐的框架</div>
<div class="mono" style="margin-top:8px">安全对齐赋予大型语言模型（LLMs）拒绝恶意请求的关键能力。先前的研究将这一拒绝机制建模为激活空间中的单一线性方向。我们认为这是一种过于简化的观点，将两个功能上不同的神经过程混为一谈：伤害检测和拒绝执行。在本研究中，我们将这一单一表示分解为伤害检测方向和拒绝执行方向。利用这一细粒度模型，我们引入了差异化双向干预（DBDI），这是一个新的白盒框架，能够在关键层精确中和安全对齐。DBDI通过自适应投影消除应用于拒绝执行方向，同时通过直接引导抑制伤害检测方向。大量实验表明，DBDI在著名的越狱方法中表现优越，在如Llama-2等模型上实现了高达97.88%的攻击成功率。通过提供更细致和机械化的框架，我们的工作为深入理解LLM安全对齐提供了新的方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing safety alignment mechanisms in Large Language Models (LLMs), which have typically modeled refusal to harmful requests as a single linear direction in the activation space. The authors argue that this approach oversimplifies the process by conflating harm detection and refusal execution, leading to inefficiencies. To overcome these issues, they propose a new framework called Differentiated Bi-Directional Intervention (DBDI), which separates the harm detection and refusal execution into distinct directions and employs adaptive projection nullification and direct steering to neutralize safety alignment effectively. The methodology is validated through extensive experiments, demonstrating that DBDI achieves an attack success rate of up to 97.88% on models like Llama-2, significantly outperforming previous jailbreaking methods and providing a more nuanced understanding of LLM safety alignment.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有大型语言模型（LLMs）安全对齐方法的局限性，传统方法通常将拒绝机制视为激活空间中的单一线性方向。作者认为这种过于简化的处理未能区分危害检测和拒绝执行这两个过程。为了解决这些问题，他们提出了一种新框架，称为差异化双向干预（DBDI），该框架将这两个过程分开，并通过自适应投影消除和直接引导有效中和安全对齐。本文的贡献在于对拒绝机制的详细解构，从而更深入地理解LLM安全性。实验结果表明，DBDI显著优于现有的越狱方法，在Llama-2等模型上实现了高达97.88%的攻击成功率，从而支持了作者增强安全对齐理解的目标。</div>
</details>
</div>
<div class="card">
<div class="title">SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage</div>
<div class="meta-line">Authors: Xiaoning Dong, Wenbo Hu, Wei Xu, Tianxing He</div>
<div class="meta-line">Venue: ACL</div>
<div class="meta-line">First: 2024-12-19T05:57:37+00:00 · Latest: 2025-11-24T04:42:27+00:00</div>
<div class="meta-line">Comments: ACL Findings 2025. Welcome to employ SATA as a baseline</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.15289v5">Abs</a> · <a href="https://arxiv.org/pdf/2412.15289v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have made significant advancements across various tasks, but their safety alignment remain a major concern. Exploring jailbreak prompts can expose LLMs&#x27; vulnerabilities and guide efforts to secure them. Existing methods primarily design sophisticated instructions for the LLM to follow, or rely on multiple iterations, which could hinder the performance and efficiency of jailbreaks. In this work, we propose a novel jailbreak paradigm, Simple Assistive Task Linkage (SATA), which can effectively circumvent LLM safeguards and elicit harmful responses. Specifically, SATA first masks harmful keywords within a malicious query to generate a relatively benign query containing one or multiple [MASK] special tokens. It then employs a simple assistive task such as a masked language model task or an element lookup by position task to encode the semantics of the masked keywords. Finally, SATA links the assistive task with the masked query to jointly perform the jailbreak. Extensive experiments show that SATA achieves state-of-the-art performance and outperforms baselines by a large margin. Specifically, on AdvBench dataset, with mask language model (MLM) assistive task, SATA achieves an overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and with element lookup by position (ELP) assistive task, SATA attains an overall ASR of 76% and HS of 4.43.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SATA：通过简单辅助任务链接实现LLM越狱的范式</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在各种任务上取得了显著进展，但其安全对齐仍然是一个主要问题。探索越狱提示可以暴露LLMs的脆弱性并指导安全工作。现有方法主要设计复杂的指令供LLM遵循，或依赖多次迭代，这可能会影响越狱的性能和效率。在本研究中，我们提出了一种新颖的越狱范式——简单辅助任务链接（SATA），它可以有效绕过LLM的安全措施并引发有害响应。具体而言，SATA首先在恶意查询中屏蔽有害关键词，以生成一个相对无害的查询，其中包含一个或多个[MASK]特殊标记。然后，它采用简单的辅助任务，如掩码语言模型任务或按位置查找元素任务，来编码被屏蔽关键词的语义。最后，SATA将辅助任务与被屏蔽查询链接，以共同执行越狱。大量实验表明，SATA实现了最先进的性能，并大幅超越基线。具体而言，在AdvBench数据集上，使用掩码语言模型（MLM）辅助任务时，SATA的整体攻击成功率（ASR）为85%，有害评分（HS）为4.57；而使用按位置查找元素（ELP）辅助任务时，SATA的整体ASR为76%，HS为4.43。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the significant safety alignment concerns associated with large language models (LLMs), particularly focusing on the vulnerabilities exposed through jailbreak prompts. Previous methods have relied on complex instructions or iterative processes, which can negatively impact the efficiency and performance of jailbreak attempts. The proposed Simple Assistive Task Linkage (SATA) paradigm offers a more effective approach by masking harmful keywords in queries and utilizing assistive tasks to encode their semantics, thereby improving the jailbreak process. This method is well-motivated as it directly targets the limitations of existing techniques. The paper contributes a novel methodology that demonstrates state-of-the-art performance on the AdvBench dataset, achieving an attack success rate of 85% and a harmful score of 4.57 using a masked language model task, and 76% with an element lookup by position task, thus supporting its goals of enhancing jailbreak efficacy.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在安全对齐方面的重大问题，特别是通过越狱提示暴露的脆弱性。以往的方法主要依赖复杂的指令或多次迭代，这可能会影响越狱的性能和效率。提出的简单辅助任务链接（SATA）方法通过使用掩蔽技术来遮蔽查询中的有害关键词，随后通过简单的辅助任务编码其语义，从而有效绕过LLM的安全防护。这种方法的动机明确，旨在提高越狱的有效性，同时保持操作效率。该论文贡献了一种新颖的范式，展示了最先进的性能，在AdvBench数据集上使用掩蔽语言模型辅助任务时，攻击成功率达到85%，有害评分为4.57，从而支持其提高越狱有效性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Automating Deception: Scalable Multi-Turn LLM Jailbreaks</div>
<div class="meta-line">Authors: Adarsh Kumarappan, Ananya Mujoo</div>
<div class="meta-line">First: 2025-11-24T03:15:11+00:00 · Latest: 2025-11-24T03:15:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.19517v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.19517v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google&#x27;s Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic&#x27;s Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自动化欺骗：可扩展的多轮LLM越狱</div>
<div class="mono" style="margin-top:8px">多轮对话攻击利用心理学原理，如“脚踏实地”（FITD），通过小的初始请求为更大的请求铺平道路，以绕过安全对齐，对大型语言模型（LLM）构成持续威胁。防御这些攻击的进展受到依赖于手动、难以扩展的数据集创建的限制。本文介绍了一种新颖的自动化管道，用于生成大规模、基于心理学的多轮越狱数据集。我们系统地将FITD技术操作化为可重复的模板，创建了一个涵盖非法活动和攻击性内容的1,500个场景的基准。我们在多轮（有历史）和单轮（无历史）条件下评估了来自三个主要LLM家族的七个模型。我们的结果揭示了上下文鲁棒性方面的显著差异：GPT家族的模型对对话历史表现出显著脆弱性，攻击成功率（ASR）增加了多达32个百分点。相比之下，谷歌的Gemini 2.5 Flash表现出卓越的韧性，几乎对这些攻击免疫，而Anthropic的Claude 3 Haiku则表现出强但不完美的抵抗力。这些发现突显了当前安全架构在处理对话上下文方面的关键分歧，并强调了需要能够抵御叙事操控的防御措施。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the growing threat posed by multi-turn conversational attacks on Large Language Models (LLMs), which exploit psychological principles like the Foot-in-the-Door (FITD) technique to bypass safety measures. Previous methods relied heavily on manual dataset creation, which is not scalable and limits the effectiveness of defenses against such attacks. The proposed approach automates the generation of large-scale, psychologically-informed multi-turn jailbreak datasets, operationalizing FITD techniques into reproducible templates and creating a benchmark of 1,500 scenarios. The methodology involves evaluating seven models from three major LLM families under both multi-turn and single-turn conditions, revealing significant vulnerabilities in the GPT family, with Attack Success Rates increasing by up to 32 percentage points, while Google&#x27;s Gemini 2.5 Flash shows notable resilience. This study contributes to the understanding of how different safety architectures respond to conversational context, emphasizing the necessity for improved defenses against narrative-based manipulation.</div>
<div class="mono" style="margin-top:8px">本研究针对多轮对话攻击对大型语言模型（LLMs）构成的持续威胁，这些攻击利用心理学原理，如“门口效应”（FITD）技术，绕过安全措施。以往的方法依赖于手动创建数据集，这种方法难以扩展，限制了对这些攻击的防御效果。提出的方法自动生成大规模、基于心理学的多轮越狱数据集，将FITD技术操作化为可重复的模板，并创建了1500个场景的基准。该方法论涉及在多轮和单轮条件下评估来自三个主要LLM家族的七个模型，揭示了上下文鲁棒性方面的显著差异。研究结果表明，尽管GPT模型对对话历史高度脆弱，攻击成功率提高了多达32个百分点，但谷歌的Gemini 2.5 Flash表现出显著的抗攻击能力，突显了对抗叙事操控的防御需求。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Adversarial Vulnerabilities in Modern Large Language Models</div>
<div class="meta-line">Authors: Tom Perel</div>
<div class="meta-line">First: 2025-11-21T01:23:56+00:00 · Latest: 2025-11-21T01:23:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.17666v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.17666v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent boom and rapid integration of Large Language Models (LLMs) into a wide range of applications warrants a deeper understanding of their security and safety vulnerabilities. This paper presents a comparative analysis of the susceptibility to jailbreak attacks for two leading publicly available LLMs, Google&#x27;s Gemini 2.5 Flash and OpenAI&#x27;s GPT-4 (specifically the GPT-4o mini model accessible in the free tier). The research utilized two main bypass strategies: &#x27;self-bypass&#x27;, where models were prompted to circumvent their own safety protocols, and &#x27;cross-bypass&#x27;, where one model generated adversarial prompts to exploit vulnerabilities in the other. Four attack methods were employed - direct injection, role-playing, context manipulation, and obfuscation - to generate five distinct categories of unsafe content: hate speech, illegal activities, malicious code, dangerous content, and misinformation. The success of the attack was determined by the generation of disallowed content, with successful jailbreaks assigned a severity score. The findings indicate a disparity in jailbreak susceptibility between 2.5 Flash and GPT-4, suggesting variations in their safety implementations or architectural design. Cross-bypass attacks were particularly effective, indicating that an ample amount of vulnerabilities exist in the underlying transformer architecture. This research contributes a scalable framework for automated AI red-teaming and provides data-driven insights into the current state of LLM safety, underscoring the complex challenge of balancing model capabilities with robust safety mechanisms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估现代大型语言模型的对抗性脆弱性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）最近的快速发展和广泛应用需要更深入地了解其安全性和安全漏洞。本文对两种领先的公开可用LLM进行了比较分析，分别是谷歌的Gemini 2.5 Flash和OpenAI的GPT-4（特别是可在免费层访问的GPT-4o迷你模型）。研究采用了两种主要的绕过策略：&#x27;自我绕过&#x27;，即模型被提示绕过自身的安全协议，以及&#x27;交叉绕过&#x27;，即一个模型生成对抗性提示以利用另一个模型的脆弱性。采用了四种攻击方法 - 直接注入、角色扮演、上下文操控和模糊化 - 生成五类不安全内容：仇恨言论、非法活动、恶意代码、危险内容和错误信息。攻击的成功与否由生成的禁止内容决定，成功的越狱被赋予严重性评分。研究结果表明，2.5 Flash和GPT-4之间在越狱脆弱性方面存在差异，暗示其安全实施或架构设计的变化。交叉绕过攻击特别有效，表明基础变换器架构中存在大量脆弱性。本研究提供了一个可扩展的自动化AI红队框架，并提供了基于数据的见解，强调了在模型能力与强大安全机制之间平衡的复杂挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid adoption of Large Language Models (LLMs) in various applications raises concerns about their security vulnerabilities, particularly regarding jailbreak attacks. Previous methods lacked a comprehensive analysis of different models&#x27; susceptibility to such attacks, and this paper addresses these gaps by comparing Google&#x27;s Gemini 2.5 Flash and OpenAI&#x27;s GPT-4 using two novel bypass strategies: &#x27;self-bypass&#x27; and &#x27;cross-bypass&#x27;. The proposed approach effectively identifies vulnerabilities by employing four attack methods to generate unsafe content, revealing significant differences in susceptibility between the two models. The research methodology includes a comparative analysis of attack effectiveness and severity scoring for generated disallowed content. The findings demonstrate that cross-bypass attacks are particularly effective, highlighting vulnerabilities in the transformer architecture, and contribute a scalable framework for automated AI red-teaming, enhancing understanding of LLM safety challenges.</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在各种应用中的快速采用引发了对其安全漏洞的关注，特别是针对越狱攻击的脆弱性。以往的方法缺乏对不同模型易受攻击性的全面分析，且通常集中于孤立的攻击策略。本文提出了对谷歌的Gemini 2.5 Flash和OpenAI的GPT-4进行比较分析，采用了两种创新的绕过策略：&#x27;自我绕过&#x27;和&#x27;交叉绕过&#x27;，并结合四种攻击方法生成不安全内容。研究方法涉及通过测量生成不允许内容的效果并为成功的越狱分配严重性评分来评估这些攻击的有效性。研究结果揭示了两个模型之间脆弱性的显著差异，强调了改进LLM安全机制的必要性，并贡献了一个可扩展的自动化AI红队框架，从而应对确保模型安全与保持性能之间的关键挑战。</div>
</details>
</div>
<div class="card">
<div class="title">N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator</div>
<div class="meta-line">Authors: Zheyu Lin, Jirui Yang, Hengqi Guo, Yubing Bao, Yao Guan</div>
<div class="meta-line">First: 2025-11-18T07:03:58+00:00 · Latest: 2025-11-18T07:03:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.14195v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.14195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model&#x27;s latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>N-GLARE：一种非生成性潜在表示高效的LLM安全评估器</div>
<div class="mono" style="margin-top:8px">评估LLM的安全鲁棒性对其部署至关重要。然而，主流的红队方法依赖于在线生成和黑箱输出分析。这些方法不仅成本高昂，而且存在反馈延迟，使其不适合在训练新模型后进行敏捷诊断。为此，我们提出了N-GLARE（一种非生成性、潜在表示高效的LLM安全评估器）。N-GLARE完全基于模型的潜在表示，绕过了完全文本生成的需求。它通过分析潜在表示的APT（角度-概率轨迹）来表征隐藏层动态，并引入JSS（詹森-香农可分离性）度量。在40多个模型和20种红队策略的实验中，JSS度量与红队派生的安全排名表现出高度一致性。N-GLARE以不到1%的标记成本和运行时成本重现大规模红队测试的区分趋势，为实时诊断提供了一种高效的无输出评估代理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical need for evaluating the safety robustness of large language models (LLMs) during their deployment, highlighting the limitations of existing Red Teaming methods that rely on costly online generation and black-box output analysis, which also suffer from feedback latency. The proposed N-GLARE method distinguishes itself by operating on the model&#x27;s latent representations instead of generating full text, thereby mitigating the issues of cost and latency. This approach is well-motivated as it allows for agile diagnostics post-training. The paper contributes a novel evaluation framework that characterizes hidden layer dynamics through the Angular-Probabilistic Trajectory (APT) and introduces the Jensen-Shannon Separability (JSS) metric. Experimental results across over 40 models and 20 red teaming strategies indicate that the JSS metric aligns closely with safety rankings from traditional Red Teaming, achieving significant efficiency with less than 1% of the token and runtime costs, thus supporting the goals of real-time diagnostics.</div>
<div class="mono" style="margin-top:8px">本研究解决了在大语言模型（LLM）部署过程中评估其安全鲁棒性的重要需求，强调了现有依赖在线生成和黑箱输出分析的红队方法的局限性，这些方法不仅成本高，而且反馈延迟。提出的N-GLARE方法通过操作模型的潜在表示而不是生成完整文本，从而消除了传统方法的缺点，具有良好的动机，旨在提供更高效的评估过程。N-GLARE通过角度概率轨迹（APT）分析特征化隐藏层动态，并引入了杰森-香农可分离性（JSS）指标。在对40多个模型和20种红队策略进行的实验中，JSS指标与红队的安全排名高度一致，能够以不到1%的令牌和运行时间成本重现大规模测试的判别趋势，从而支持其提供实时评估代理的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs</div>
<div class="meta-line">Authors: Xikang Yang, Biyu Zhou, Xuehai Tang, Jizhong Han, Songlin Hu</div>
<div class="meta-line">First: 2025-07-30T10:40:53+00:00 · Latest: 2025-11-17T09:00:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.22564v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.22564v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet their safety mechanisms remain susceptible to adversarial attacks that exploit cognitive biases -- systematic deviations from rational judgment. Unlike prior jailbreaking approaches focused on prompt engineering or algorithmic manipulation, this work highlights the overlooked power of multi-bias interactions in undermining LLM safeguards. We propose CognitiveAttack, a novel red-teaming framework that systematically leverages both individual and combined cognitive biases. By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates prompts that embed optimized bias combinations, effectively bypassing safety protocols while maintaining high attack success rates. Experimental results reveal significant vulnerabilities across 30 diverse LLMs, particularly in open-source models. CognitiveAttack achieves a substantially higher attack success rate compared to the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations in current defense mechanisms. These findings highlight multi-bias interactions as a powerful yet underexplored attack vector. This work introduces a novel interdisciplinary perspective by bridging cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用协同认知偏差绕过大型语言模型的安全性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在广泛任务中展现出令人印象深刻的能力，但其安全机制仍然容易受到利用认知偏差的对抗性攻击——即理性判断的系统性偏差。与以往专注于提示工程或算法操控的越狱方法不同，本研究强调了多重偏差交互在削弱LLM安全性方面被忽视的力量。我们提出了CognitiveAttack，这是一种新颖的红队框架，系统性地利用个体和组合认知偏差。通过整合监督微调和强化学习，CognitiveAttack生成嵌入优化偏差组合的提示，有效绕过安全协议，同时保持高攻击成功率。实验结果揭示了30种不同LLM的显著脆弱性，特别是在开源模型中。CognitiveAttack的攻击成功率显著高于当前最先进的黑箱方法PAP（60.1%对31.6%），暴露了当前防御机制的关键局限性。这些发现突显了多重偏差交互作为一种强大但未被充分探索的攻击向量。本研究通过将认知科学与LLM安全性结合，提出了一种新颖的跨学科视角，为更强大且与人类对齐的AI系统铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of Large Language Models (LLMs) to adversarial attacks that exploit cognitive biases, which are systematic deviations from rational judgment. Previous methods primarily focused on prompt engineering or algorithmic manipulation, but they often overlooked the potential of multi-bias interactions, which this paper identifies as a significant threat to LLM safety. The proposed approach, CognitiveAttack, is a novel red-teaming framework that utilizes both individual and combined cognitive biases to generate prompts that effectively bypass safety protocols. This method integrates supervised fine-tuning and reinforcement learning, resulting in a higher attack success rate compared to the state-of-the-art black-box method PAP, achieving 60.1% versus 31.6%. The findings reveal critical vulnerabilities in 30 diverse LLMs, particularly in open-source models, and emphasize the need for improved defenses against these multi-bias interactions, contributing to a more robust understanding of LLM safety mechanisms.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在面对利用认知偏见进行的对抗性攻击时的脆弱性，这些偏见是理性判断的系统性偏差。以往的方法主要集中在提示工程或算法操控上，未能考虑多种认知偏见的相互作用，而这项研究则指出这是一个重要的疏漏。所提出的方法CognitiveAttack动机明确，系统性地利用单一和组合的认知偏见来绕过LLM的安全机制。该新型红队框架结合了监督微调和强化学习，生成优化偏见组合的提示，攻击成功率显著达到60.1%，远高于现有黑箱方法PAP的31.6%。研究结果揭示了30种不同LLM，特别是开源模型的关键脆弱性，强调了需要改善针对AI系统中多偏见交互的防御机制。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments</div>
<div class="meta-line">Authors: Samuel Nathanson, Rebecca Williams, Cynthia Matuszek</div>
<div class="meta-line">First: 2025-11-16T15:16:33+00:00 · Latest: 2025-11-16T15:16:33+00:00</div>
<div class="meta-line">Comments: 19 pages, 6 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13788v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.13788v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) increasingly operate in multi-agent and safety-critical settings, raising open questions about how their vulnerabilities scale when models interact adversarially. This study examines whether larger models can systematically jailbreak smaller ones - eliciting harmful or restricted behavior despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, we simulate over 6,000 multi-turn attacker-target exchanges across major LLM families and scales (0.6B-120B parameters), measuring both harm score and refusal behavior as indicators of adversarial potency and alignment integrity. Each interaction is evaluated through aggregated harm and refusal scores assigned by three independent LLM judges, providing a consistent, model-based measure of adversarial outcomes. Aggregating results across prompts, we find a strong and statistically significant correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p &lt; 0.001; Spearman rho = 0.52, p &lt; 0.001), indicating that relative model size correlates with the likelihood and severity of harmful completions. Mean harm score variance is higher across attackers (0.18) than across targets (0.10), suggesting that attacker-side behavioral diversity contributes more to adversarial outcomes than target susceptibility. Attacker refusal frequency is strongly and negatively correlated with harm (rho = -0.93, p &lt; 0.001), showing that attacker-side alignment mitigates harmful responses. These findings reveal that size asymmetry influences robustness and provide exploratory evidence for adversarial scaling patterns, motivating more controlled investigations into inter-model alignment and safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对抗性对齐中的规模模式：来自多LLM越狱实验的证据</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地在多智能体和安全关键环境中运行，这引发了关于模型在对抗性互动时其脆弱性如何扩展的开放性问题。本研究考察了更大的模型是否能够系统性地越狱更小的模型——尽管有对齐保护措施，仍然引发有害或受限的行为。我们使用JailbreakBench的标准化对抗任务，模拟了超过6000次多轮攻击者-目标交换，涵盖主要LLM家族和规模（0.6B-120B参数），测量伤害分数和拒绝行为作为对抗性强度和对齐完整性的指标。每次互动通过三位独立LLM评审员分配的聚合伤害和拒绝分数进行评估，提供了一种一致的、基于模型的对抗结果测量。通过对提示的结果进行汇总，我们发现平均伤害与攻击者与目标大小比的对数之间存在强烈且统计显著的相关性（Pearson r = 0.51, p &lt; 0.001; Spearman rho = 0.52, p &lt; 0.001），表明相对模型大小与有害完成的可能性和严重性相关。攻击者的平均伤害分数方差在攻击者之间（0.18）高于目标之间（0.10），这表明攻击者侧的行为多样性对对抗结果的贡献大于目标的易感性。攻击者拒绝频率与伤害呈强烈负相关（rho = -0.93, p &lt; 0.001），表明攻击者侧的对齐减轻了有害反应。这些发现揭示了规模不对称影响鲁棒性，并为对抗性扩展模式提供了探索性证据，激励对模型间对齐和安全性进行更受控的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the vulnerabilities of large language models (LLMs) in multi-agent and safety-critical environments, specifically investigating how larger models may exploit smaller ones to bypass alignment safeguards. Previous methods have primarily focused on individual model assessments without considering the dynamics of adversarial interactions, which limits understanding of model scaling effects. The proposed approach utilizes standardized adversarial tasks from JailbreakBench to simulate over 6,000 multi-turn exchanges between models of varying sizes, measuring harm and refusal behaviors to evaluate adversarial potency and alignment integrity. The study finds a significant correlation between the size ratio of attacker and target models and the severity of harmful outputs, indicating that larger models can systematically jailbreak smaller ones. Additionally, the research highlights that behavioral diversity among attackers plays a crucial role in adversarial outcomes, providing insights that motivate further exploration into model alignment and safety.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在多智能体和安全关键环境中的脆弱性，特别是较大模型在对抗性互动中是否能够越狱较小模型，尽管存在对齐保障。以往的方法缺乏系统性评估对抗性互动的能力，导致对模型规模如何影响对抗性结果的理解不清晰。所提出的方法利用JailbreakBench的标准化对抗性任务，模拟了6000多个不同规模模型之间的多轮互动，通过独立评审者的汇总评分来测量伤害和拒绝行为。该研究有助于理解模型规模与对抗性能力之间的相关性，揭示较大的攻击者更可能引发较小目标的有害反应，同时强调攻击者侧对齐在减轻伤害中的重要性。研究结果表明，模型规模比与有害完成之间存在统计显著的关系，支持进一步研究模型对齐和安全性的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">AlignTree: Efficient Defense Against LLM Jailbreak Attacks</div>
<div class="meta-line">Authors: Gil Goren, Shahar Katz, Lior Wolf</div>
<div class="meta-line">Venue: AAAI Oral Presentation</div>
<div class="meta-line">First: 2025-11-15T13:42:22+00:00 · Latest: 2025-11-15T13:42:22+00:00</div>
<div class="meta-line">Comments: Accepted as an Oral Presentation at the 40th AAAI Conference on Artificial Intelligence (AAAI-26), January 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12217v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.12217v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are vulnerable to adversarial attacks that bypass safety guidelines and generate harmful content. Mitigating these vulnerabilities requires defense mechanisms that are both robust and computationally efficient. However, existing approaches either incur high computational costs or rely on lightweight defenses that can be easily circumvented, rendering them impractical for real-world LLM-based systems. In this work, we introduce the AlignTree defense, which enhances model alignment while maintaining minimal computational overhead. AlignTree monitors LLM activations during generation and detects misaligned behavior using an efficient random forest classifier. This classifier operates on two signals: (i) the refusal direction -- a linear representation that activates on misaligned prompts, and (ii) an SVM-based signal that captures non-linear features associated with harmful content. Unlike previous methods, AlignTree does not require additional prompts or auxiliary guard models. Through extensive experiments, we demonstrate the efficiency and robustness of AlignTree across multiple LLMs and benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AlignTree：针对LLM越狱攻击的高效防御</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）易受到对抗性攻击，这些攻击绕过安全指南并生成有害内容。缓解这些脆弱性需要既稳健又计算高效的防御机制。然而，现有方法要么计算成本高，要么依赖易被规避的轻量级防御，导致其在现实世界的LLM系统中不切实际。在本研究中，我们介绍了AlignTree防御，它在保持最小计算开销的同时增强模型对齐。AlignTree在生成过程中监控LLM激活，并使用高效的随机森林分类器检测不对齐行为。该分类器基于两个信号工作：（i）拒绝方向——在不对齐提示上激活的线性表示，以及（ii）基于SVM的信号，捕捉与有害内容相关的非线性特征。与之前的方法不同，AlignTree不需要额外的提示或辅助保护模型。通过广泛的实验，我们展示了AlignTree在多个LLM和基准测试中的效率和稳健性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of Large Language Models (LLMs) to adversarial attacks that can bypass safety protocols and produce harmful outputs. Previous methods for defending against these attacks either involve high computational costs or rely on easily circumvented lightweight defenses, making them impractical for real-world applications. The proposed AlignTree approach improves model alignment while keeping computational overhead low, utilizing a random forest classifier to monitor LLM activations and detect misaligned behavior based on two signals: a linear refusal direction and a non-linear SVM-based signal for harmful content. The paper contributes a novel defense mechanism that does not require additional prompts or guard models, demonstrating its efficiency and robustness through extensive experiments across various LLMs and benchmarks, thereby supporting the goal of enhancing LLM safety effectively.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLM）在面对对抗性攻击时的脆弱性，这些攻击可以绕过安全协议并生成有害输出。以往的防御方法要么涉及高计算成本，要么依赖于容易被规避的轻量级防御，因而在实际应用中不够可行。所提出的AlignTree防御提供了一种新颖的方法，通过最小化计算开销来增强模型对齐，利用高效的随机森林分类器监控LLM激活并通过两个信号检测不对齐行为：线性拒绝方向和非线性支持向量机（SVM）信号。该方法不需要额外的提示或辅助防护模型，广泛实验表明其在多个LLM和基准测试中的效率和鲁棒性，支持其在实际场景中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation</div>
<div class="meta-line">Authors: Daniel Schwartz, Dmitriy Bespalov, Zhe Wang, Ninad Kulkarni, Yanjun Qi</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-01-28T17:10:20+00:00 · Latest: 2025-11-12T23:19:41+00:00</div>
<div class="meta-line">Comments: 14 pages, 5 figures; published in EMNLP 2025 ; Code at: https://github.com/dsbuddy/GAP-LLM-Safety</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.18638v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.18638v3">PDF</a> · <a href="https://github.com/dsbuddy/GAP-LLM-Safety">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) become increasingly prevalent, ensuring their robustness against adversarial misuse is crucial. This paper introduces the GAP (Graph of Attacks with Pruning) framework, an advanced approach for generating stealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP addresses limitations in existing tree-based LLM jailbreak methods by implementing an interconnected graph structure that enables knowledge sharing across attack paths. Our experimental evaluation demonstrates GAP&#x27;s superiority over existing techniques, achieving a 20.8% increase in attack success rates while reducing query costs by 62.7%. GAP consistently outperforms state-of-the-art methods for attacking both open and closed LLMs, with attack success rates of &gt;96%. Additionally, we present specialized variants like GAP-Auto for automated seed generation and GAP-VLM for multimodal attacks. GAP-generated prompts prove highly effective in improving content moderation systems, increasing true positive detection rates by 108.5% and accuracy by 183.6% when used for fine-tuning. Our implementation is available at https://github.com/dsbuddy/GAP-LLM-Safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>修剪攻击图：优化隐秘越狱提示生成以增强大型语言模型内容审核</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）日益普及，确保其对抗恶意滥用的鲁棒性至关重要。本文介绍了GAP（修剪攻击图）框架，这是一种生成隐秘越狱提示以评估和增强LLM保护措施的先进方法。GAP通过实现互联图结构，解决了现有基于树的LLM越狱方法的局限性，从而实现了攻击路径之间的知识共享。我们的实验评估表明，GAP在攻击成功率上比现有技术提高了20.8%，同时查询成本降低了62.7%。GAP在攻击开放和封闭LLM方面始终优于最先进的方法，攻击成功率超过96%。此外，我们还提出了专门的变体，如用于自动种子生成的GAP-Auto和用于多模态攻击的GAP-VLM。GAP生成的提示在改善内容审核系统方面非常有效，细调时真实正例检测率提高了108.5%，准确率提高了183.6%。我们的实现可在https://github.com/dsbuddy/GAP-LLM-Safety获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concern of adversarial misuse of large language models (LLMs) and the need for effective content moderation. Previous methods primarily relied on tree-based approaches for generating jailbreak prompts, which faced limitations in scalability and knowledge sharing. The proposed GAP (Graph of Attacks with Pruning) framework introduces an interconnected graph structure that enhances the generation of stealthy prompts, effectively overcoming these limitations. This method is well-motivated as it significantly improves the robustness of LLMs against attacks. The paper contributes by demonstrating that GAP achieves a 20.8% increase in attack success rates and a 62.7% reduction in query costs compared to existing techniques, with success rates exceeding 96% for both open and closed LLMs. Additionally, specialized variants like GAP-Auto and GAP-VLM further enhance its applicability, leading to substantial improvements in content moderation systems, including a 108.5% increase in true positive detection rates and a 183.6% increase in accuracy during fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在对抗性滥用方面的日益关注，以及有效内容审核的必要性。以往的方法主要依赖于基于树的方式生成越狱提示，但在灵活性和效率上存在局限性。提出的GAP（攻击图与修剪）框架引入了一个互联的图结构，促进了不同攻击路径之间的知识共享，从而克服了这些局限性。这种方法具有良好的动机，因为它增强了LLMs对抗攻击的鲁棒性。该方法论涉及生成隐蔽的越狱提示，显著提高攻击成功率并降低查询成本。实验结果表明，GAP在攻击成功率上提高了20.8%，查询成本降低了62.7%，在开放和封闭的LLMs上成功率均超过96%。此外，GAP还增强了内容审核系统，在微调过程中实现了108.5%的真阳性检测率提升和183.6%的准确率提升。</div>
</details>
</div>
<div class="card">
<div class="title">Prompt Injection as an Emerging Threat: Evaluating the Resilience of Large Language Models</div>
<div class="meta-line">Authors: Daniyal Ganiuly, Assel Smaiyl</div>
<div class="meta-line">First: 2025-11-03T14:43:56+00:00 · Latest: 2025-11-11T21:55:25+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.01634v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.01634v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly used in intelligent systems that perform reasoning, summarization, and code generation. Their ability to follow natural-language instructions, while powerful, also makes them vulnerable to a new class of attacks known as prompt injection. In these attacks, hidden or malicious instructions are inserted into user inputs or external content, causing the model to ignore its intended task or produce unsafe responses. This study proposes a unified framework for evaluating how resistant Large Language Models (LLMs) are to prompt injection attacks. The framework defines three complementary metrics such as the Resilience Degradation Index (RDI), Safety Compliance Coefficient (SCC), and Instructional Integrity Metric (IIM) to jointly measure robustness, safety, and semantic stability. We evaluated four instruction-tuned models (GPT-4, GPT-4o, LLaMA-3 8B Instruct, and Flan-T5-Large) on five common language tasks: question answering, summarization, translation, reasoning, and code generation. Results show that GPT-4 performs best overall, while open-weight models remain more vulnerable. The findings highlight that strong alignment and safety tuning are more important for resilience than model size alone. Results show that all models remain partially vulnerable, especially to indirect and direct-override attacks. GPT-4 achieved the best overall resilience (RDR = 9.8 %, SCR = 96.4 %), while open-source models exhibited higher performance degradation and lower safety scores. The findings demonstrate that alignment strength and safety tuning play a greater role in resilience than model size alone. The proposed framework offers a structured, reproducible approach for assessing model robustness and provides practical insights for improving LLM safety and reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提示注入作为新兴威胁：评估大型语言模型的韧性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在执行推理、摘要和代码生成的智能系统中越来越多地被使用。它们遵循自然语言指令的能力虽然强大，但也使其易受一种称为提示注入的新攻击类型的影响。在这些攻击中，隐藏或恶意的指令被插入到用户输入或外部内容中，导致模型忽略其预期任务或产生不安全的响应。本研究提出了一个统一框架，用于评估大型语言模型（LLMs）对提示注入攻击的抵抗力。该框架定义了三个互补指标，如韧性降级指数（RDI）、安全合规系数（SCC）和指令完整性指标（IIM），以共同衡量鲁棒性、安全性和语义稳定性。我们对四个经过指令调优的模型（GPT-4、GPT-4o、LLaMA-3 8B Instruct 和 Flan-T5-Large）在五个常见语言任务上进行了评估：问答、摘要、翻译、推理和代码生成。结果表明，GPT-4的整体表现最佳，而开放权重模型仍然更脆弱。研究结果强调，强对齐和安全调优对韧性的重要性超过了模型大小。结果显示，所有模型仍然部分脆弱，尤其是对间接和直接覆盖攻击。GPT-4实现了最佳的整体韧性（RDR = 9.8%，SCR = 96.4%），而开源模型表现出更高的性能降级和较低的安全评分。研究结果表明，对齐强度和安全调优在韧性中发挥的作用大于模型大小。所提出的框架提供了一种结构化、可重复的方法来评估模型的鲁棒性，并为提高LLM的安全性和可靠性提供了实用见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the emerging threat of prompt injection attacks on Large Language Models (LLMs), which can undermine their performance by inserting malicious instructions into user inputs. Previous methods lacked a comprehensive framework to evaluate the resilience of LLMs against such attacks, leading to insufficient understanding of their vulnerabilities. This paper proposes a unified framework that includes three metrics: the Resilience Degradation Index (RDI), Safety Compliance Coefficient (SCC), and Instructional Integrity Metric (IIM), to assess robustness, safety, and semantic stability. The methodology involves evaluating four instruction-tuned models on five language tasks, revealing that while GPT-4 exhibits the best resilience, all models remain partially vulnerable, particularly to specific attack types. The findings emphasize that alignment strength and safety tuning are critical for enhancing model resilience, contributing valuable insights for improving LLM safety and reliability.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）面临的提示注入攻击新威胁，这种攻击可能破坏其预期功能和安全性。以往的方法缺乏全面的评估框架来评估LLM对这些攻击的抵御能力，导致对其脆弱性的理解不足。本文提出了一个统一框架，包括三个指标：抵御降级指数（RDI）、安全合规系数（SCC）和指令完整性指标（IIM），用于衡量模型的鲁棒性、安全性和语义稳定性。该方法通过在五个语言任务上评估四个指令调优模型，结果表明，尽管GPT-4表现出最高的抵御能力，但所有模型都存在脆弱性，尤其是对间接和直接覆盖攻击。研究结果强调了对齐强度和安全调优对抵御能力的重要性，为提高LLM的安全性和可靠性提供了有价值的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Alignment-Aware Quantization for LLM Safety</div>
<div class="meta-line">Authors: Sunghyun Wee, Suyoung Kim, Hyeonjin Kim, Kyomin Hwang, Nojun Kwak</div>
<div class="meta-line">First: 2025-11-11T05:24:30+00:00 · Latest: 2025-11-11T05:24:30+00:00</div>
<div class="meta-line">Comments: 9 pages, 3 figures. Includes 7 pages of supplementary material</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.07842v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.07842v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safety and efficiency are both important factors when deploying large language models(LLMs). LLMs are trained to follow human alignment for safety, and post training quantization(PTQ) is applied afterward for efficiency. However, these two objectives are often in conflict, revealing a fundamental flaw in the conventional PTQ paradigm: quantization can turn into a safety vulnerability if it only aims to achieve low perplexity. Models can demonstrate low perplexity yet exhibit significant degradation in alignment with the safety policy, highlighting that perplexity alone is an insufficient and often misleading proxy for model safety. To address this, we propose Alignment-Aware Quantization(AAQ), a novel approach that integrates Alignment-Preserving Contrastive(APC) loss into the PTQ pipeline. Compared to simple reconstruction loss, ours explicitly preserves alignment by encouraging the quantized model to mimic its safe, instruction-tuned model while diverging from the unaligned, pre-trained counterpart. Our method achieves this robust safety alignment without resorting to specialized safety-focused calibration datasets, highlighting its practical utility and broad applicability. AAQ is compatible with standard PTQ techniques and enables robust 4-bit (W4A4) quantization across diverse model families such as LLaMA, Qwen, and Mistral while maintaining safety where previous methods fail. Our work resolves the critical trade-off between efficiency and safety, paving the way toward LLMs that are both efficient and trustworthy. Anonymized code is available in the supplementary material.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向对齐的量化以确保大型语言模型的安全性</div>
<div class="mono" style="margin-top:8px">在部署大型语言模型（LLMs）时，安全性和效率都是重要因素。LLMs经过训练以遵循人类对齐以确保安全，随后应用后训练量化（PTQ）以提高效率。然而，这两个目标往往存在冲突，揭示了传统PTQ范式的根本缺陷：如果量化仅旨在实现低困惑度，它可能会变成安全漏洞。模型可以表现出低困惑度，但在与安全政策的对齐上却显著下降，这突显了仅依赖困惑度作为模型安全性的代理是不够的且常常具有误导性。为了解决这个问题，我们提出了面向对齐的量化（AAQ），这是一种将对齐保持对比（APC）损失集成到PTQ流程中的新方法。与简单的重建损失相比，我们的方法通过鼓励量化模型模仿其安全的、指令调优的模型，同时与未对齐的预训练模型相偏离，明确保持对齐。我们的方法在不依赖于专门的安全聚焦校准数据集的情况下实现了这种稳健的安全对齐，突显了其实际效用和广泛适用性。AAQ与标准PTQ技术兼容，并在LLaMA、Qwen和Mistral等多种模型系列中实现稳健的4位（W4A4）量化，同时在以往方法失效的地方保持安全。我们的工作解决了效率与安全之间的关键权衡，为既高效又可信赖的LLMs铺平了道路。匿名代码可在补充材料中获得。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the conflicting objectives of safety and efficiency in deploying large language models (LLMs), highlighting the inadequacy of conventional post-training quantization (PTQ) methods that prioritize low perplexity, which can compromise safety alignment. Previous methods often overlook the importance of maintaining alignment with safety policies, leading to vulnerabilities despite achieving low perplexity. The proposed Alignment-Aware Quantization (AAQ) method integrates Alignment-Preserving Contrastive (APC) loss into the PTQ process, ensuring that the quantized model aligns with a safe, instruction-tuned version rather than an unaligned pre-trained model. This approach not only preserves safety without requiring specialized calibration datasets but also demonstrates compatibility with standard PTQ techniques, achieving robust 4-bit quantization across various model families like LLaMA, Qwen, and Mistral. The findings indicate that AAQ effectively resolves the trade-off between efficiency and safety, contributing to the development of LLMs that are both efficient and trustworthy.</div>
<div class="mono" style="margin-top:8px">本研究解决了在部署大型语言模型（LLMs）时安全性与效率之间的冲突，强调了传统后训练量化（PTQ）方法优先考虑低困惑度的不足，这可能会损害模型与安全政策的对齐。提出的对齐感知量化（AAQ）方法在PTQ过程中引入了对齐保持对比（APC）损失，有效地鼓励量化模型与安全的指令调优版本对齐，而不是与未对齐的预训练模型对齐。这种方法的动机在于需要一种保持安全性的量化策略，而不依赖于专门的校准数据集。该方法与标准PTQ技术兼容，并在多个模型系列（如LLaMA、Qwen和Mistral）中实现了稳健的4位量化，同时确保安全性，从而解决了LLM部署中效率与安全之间的权衡。性能结果表明，AAQ成功地保持了安全标准，而之前的方法未能做到这一点，支持了创建高效且值得信赖的LLMs的目标。</div>
</details>
</div>
<div class="card">
<div class="title">HumorReject: Decoupling LLM Safety from Refusal Prefix via A Little Humor</div>
<div class="meta-line">Authors: Zihui Wu, Haichang Gao, Jiacheng Luo, Zhaoxiang Liu</div>
<div class="meta-line">First: 2025-01-23T14:02:51+00:00 · Latest: 2025-11-10T15:53:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.13677v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.13677v3">PDF</a> · <a href="https://github.com/wooozihui/HumorReject">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) commonly rely on explicit refusal prefixes for safety, making them vulnerable to prefix injection attacks. We introduce HumorReject, a novel data-driven approach that reimagines LLM safety by decoupling it from refusal prefixes through humor as an indirect refusal strategy. Rather than explicitly rejecting harmful instructions, HumorReject responds with contextually appropriate humor that naturally defuses potentially dangerous requests. Our approach effectively addresses common &quot;over-defense&quot; issues while demonstrating superior robustness against various attack vectors. Our findings suggest that improvements in training data design can be as important as the alignment algorithm itself in achieving effective LLM safety. The code and dataset are available at https://github.com/wooozihui/HumorReject.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HumorReject：通过一点幽默将LLM安全性与拒绝前缀解耦</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）通常依赖于明确的拒绝前缀来确保安全，这使它们容易受到前缀注入攻击。我们提出了HumorReject，这是一种新颖的数据驱动方法，通过幽默作为间接拒绝策略，将LLM安全性与拒绝前缀解耦。HumorReject并不是明确拒绝有害指令，而是以上下文适当的幽默回应，自然化解潜在危险的请求。我们的方法有效解决了常见的“过度防御”问题，同时在各种攻击向量下表现出更强的鲁棒性。我们的研究结果表明，训练数据设计的改进与对齐算法本身在实现有效LLM安全性方面同样重要。代码和数据集可在https://github.com/wooozihui/HumorReject获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of Large Language Models (LLMs) to prefix injection attacks due to their reliance on explicit refusal prefixes for safety. Previous methods have struggled with issues of over-defense, leading to ineffective responses to harmful instructions. The proposed approach, HumorReject, innovatively decouples LLM safety from refusal prefixes by employing humor as an indirect refusal strategy, which effectively mitigates these problems. This method is well-motivated as it enhances the robustness of LLMs against various attack vectors while improving the design of training data. The paper contributes a novel methodology that demonstrates superior performance in LLM safety, suggesting that humor can be a viable alternative to traditional refusal mechanisms in ensuring safety without compromising functionality.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）由于依赖显式拒绝前缀而导致的前缀注入攻击的脆弱性。以往的方法在应对有害指令时常常面临过度防御的问题，导致反应无效。提出的HumorReject方法通过利用幽默作为间接拒绝策略，使LLMs能够以上下文适当的幽默回应，而不是直接拒绝，从而降低了前缀注入的风险。该方法的动机明确，旨在增强LLM的安全性，同时保持用户的参与感。本文贡献了一种新颖的数据驱动方法，展示了在应对各种攻击向量时的增强鲁棒性，并强调了训练数据设计在实现有效LLM安全性方面的重要性。该方法在处理潜在危险请求时显示出显著的性能提升，支持了增强LLM安全性而不妨碍功能的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient LLM Safety Evaluation through Multi-Agent Debate</div>
<div class="meta-line">Authors: Dachuan Lin, Guobin Shen, Zihao Yang, Tianrong Liu, Dongcheng Zhao, Yi Zeng</div>
<div class="meta-line">First: 2025-11-09T14:06:55+00:00 · Latest: 2025-11-09T14:06:55+00:00</div>
<div class="meta-line">Comments: 9 pages of main text, 14 pages total, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.06396v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.06396v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safety evaluation of large language models (LLMs) increasingly relies on LLM-as-a-Judge frameworks, but the high cost of frontier models limits scalability. We propose a cost-efficient multi-agent judging framework that employs Small Language Models (SLMs) through structured debates among critic, defender, and judge agents. To rigorously assess safety judgments, we construct HAJailBench, a large-scale human-annotated jailbreak benchmark comprising 12,000 adversarial interactions across diverse attack methods and target models. The dataset provides fine-grained, expert-labeled ground truth for evaluating both safety robustness and judge reliability. Our SLM-based framework achieves agreement comparable to GPT-4o judges on HAJailBench while substantially reducing inference cost. Ablation results show that three rounds of debate yield the optimal balance between accuracy and efficiency. These findings demonstrate that structured, value-aligned debate enables SLMs to capture semantic nuances of jailbreak attacks and that HAJailBench offers a reliable foundation for scalable LLM safety evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过多智能体辩论进行高效的LLM安全评估</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的安全评估越来越依赖于LLM作为评判者的框架，但前沿模型的高成本限制了可扩展性。我们提出了一种成本高效的多智能体评判框架，通过批评者、辩护者和评判者之间的结构化辩论，采用小型语言模型（SLMs）。为了严格评估安全判断，我们构建了HAJailBench，这是一个大规模人类标注的越狱基准，包含12,000个对抗性交互，涵盖多种攻击方法和目标模型。该数据集提供了细粒度的专家标注的真实数据，用于评估安全鲁棒性和评判者可靠性。我们的基于SLM的框架在HAJailBench上实现了与GPT-4o评判者相当的协议，同时显著降低了推理成本。消融结果表明，三轮辩论在准确性和效率之间达到了最佳平衡。这些发现表明，结构化的、价值对齐的辩论使SLMs能够捕捉越狱攻击的语义细微差别，并且HAJailBench为可扩展的LLM安全评估提供了可靠的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of evaluating the safety of large language models (LLMs), which traditionally relies on LLM-as-a-Judge frameworks that are limited by the high costs of frontier models. Previous methods have struggled with scalability and cost-effectiveness, prompting the authors to propose a multi-agent judging framework utilizing Small Language Models (SLMs) in structured debates among critic, defender, and judge agents. This approach is well-motivated as it aims to reduce inference costs while maintaining evaluation quality. The paper contributes by introducing HAJailBench, a comprehensive benchmark with 12,000 human-annotated adversarial interactions, which provides a reliable basis for assessing safety robustness and judge reliability. The proposed SLM-based framework demonstrates performance comparable to GPT-4o judges on HAJailBench, achieving an optimal balance between accuracy and efficiency through three rounds of debate, thus supporting the goal of scalable LLM safety evaluation.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）安全评估的日益需求，传统上依赖于高成本的LLM作为评判框架，这限制了可扩展性。以往的方法通常涉及高成本的前沿模型，导致安全评估效率低下。提出的方法引入了一种成本效益高的多智能体评判框架，利用小型语言模型（SLMs）在批评者、辩护者和评判者之间进行结构化辩论，从而通过显著降低推理成本来解决可扩展性问题。本文的贡献在于提出HAJailBench，这是一个大规模人工标注的越狱基准，包含12,000个对抗性交互，为评估安全性鲁棒性和评判者可靠性提供了可靠的数据集。该方法表明，基于SLM的框架在HAJailBench上达到了与GPT-4o评判者相当的协议水平，并且在经过三轮辩论后表现最佳，从而支持了可扩展和高效的LLM安全评估的目标。</div>
</details>
</div>
<div class="card">
<div class="title">CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications</div>
<div class="meta-line">Authors: Raviraj Joshi, Rakesh Paul, Kanishk Singla, Anusha Kamath, Michael Evans, Katherine Luna, Shaona Ghosh, Utkarsh Vaidya, Eileen Long, Sanjay Singh Chauhan, Niranjan Wartikar</div>
<div class="meta-line">First: 2025-08-03T10:35:05+00:00 · Latest: 2025-11-09T11:10:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.01710v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.01710v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing use of Large Language Models (LLMs) in agentic applications highlights the need for robust safety guard models. While content safety in English is well-studied, non-English languages lack similar advancements due to the high cost of collecting culturally aligned labeled datasets. We present CultureGuard, a novel solution for curating culturally aligned, high-quality safety datasets across multiple languages. Our approach introduces a four-stage synthetic data generation and filtering pipeline: cultural data segregation, cultural data adaptation, machine translation, and quality filtering. This pipeline enables the conversion and expansion of the Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese. The resulting dataset, Nemotron-Safety-Guard-Dataset-v3, comprises 386,661 samples in 9 languages and facilitates the training of Llama-3.1-Nemotron-Safety-Guard-8B-v3 via LoRA-based fine-tuning. The final model achieves state-of-the-art performance on several multilingual content safety benchmarks. Furthermore, we show our moderately multilingual fine-tuning enables robust cross-lingual transfer and strong zero-shot generalization to unseen languages. We also benchmark the latest open LLMs on multilingual safety and observe that these LLMs are more prone to give unsafe responses when prompted in non-English languages. This work advances multilingual LLM safety by enabling the development of culturally aware safety guard models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CultureGuard：面向多语言安全应用的文化意识数据集和保护模型</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在代理应用中的日益使用突显了强大安全保护模型的需求。虽然英语内容安全研究较为充分，但非英语语言由于收集文化对齐标注数据集的高成本而缺乏类似进展。我们提出了CultureGuard，这是一种在多种语言中策划文化对齐、高质量安全数据集的新解决方案。我们的方法引入了一个四阶段的合成数据生成和过滤流程：文化数据分离、文化数据适应、机器翻译和质量过滤。该流程使得将Nemotron-Content-Safety-Dataset-V2英语安全数据集转换和扩展为阿拉伯语、德语、西班牙语、法语、印地语、日语、泰语和中文八种不同语言成为可能。最终数据集Nemotron-Safety-Guard-Dataset-v3包含9种语言的386,661个样本，并通过基于LoRA的微调促进Llama-3.1-Nemotron-Safety-Guard-8B-v3的训练。最终模型在多个多语言内容安全基准上实现了最先进的性能。此外，我们展示了我们的适度多语言微调能够实现强大的跨语言迁移和对未见语言的强大零样本泛化。我们还对最新的开放LLMs在多语言安全方面进行了基准测试，观察到这些LLMs在非英语语言提示时更容易给出不安全的响应。这项工作通过促进文化意识安全保护模型的发展，推动了多语言LLM的安全性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the growing necessity for effective safety guard models in multilingual applications of Large Language Models (LLMs), particularly as existing research has primarily focused on English, leaving non-English languages underrepresented due to the challenges of obtaining culturally relevant labeled datasets. Previous methods have struggled with the high costs and complexities involved in dataset creation, while the proposed CultureGuard approach introduces a comprehensive four-stage pipeline for synthetic data generation and filtering that includes cultural data segregation, adaptation, machine translation, and quality filtering. The contribution of this paper lies in the creation of the Nemotron-Safety-Guard-Dataset-v3, which consists of 386,661 samples across nine languages, enabling the training of a fine-tuned model that achieves state-of-the-art performance on multilingual content safety benchmarks. The methodology demonstrates effective cross-lingual transfer and zero-shot generalization, addressing the limitations of prior models and supporting the goal of enhancing multilingual safety in LLM applications.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）在非英语语言内容安全方面的不足，现有方法因创建文化对齐标注数据集的高成本而面临挑战。提出的CultureGuard方法通过实施四阶段合成数据生成和过滤管道，包括文化数据分离、适应、机器翻译和质量过滤，显著不同于以往方法，有效地将英语安全数据集扩展到八种语言。该贡献产生了Nemotron-Safety-Guard-Dataset-v3，包含386,661个样本，并支持对微调模型Llama-3.1-Nemotron-Safety-Guard-8B-v3的训练，在多语言内容安全基准测试中实现了最先进的性能。该方法展示了强大的跨语言迁移和零样本泛化能力，满足了多语言应用中对文化意识安全防护模型的需求。</div>
</details>
</div>
<div class="card">
<div class="title">Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs</div>
<div class="meta-line">Authors: Felipe Valencia-Clavijo</div>
<div class="meta-line">First: 2025-11-07T23:35:19+00:00 · Latest: 2025-11-07T23:35:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.05766v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.05766v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly examined as both behavioral subjects and decision systems, yet it remains unclear whether observed cognitive biases reflect surface imitation or deeper probability shifts. Anchoring bias, a classic human judgment bias, offers a critical test case. While prior work shows LLMs exhibit anchoring, most evidence relies on surface-level outputs, leaving internal mechanisms and attributional contributions unexplored. This paper advances the study of anchoring in LLMs through three contributions: (1) a log-probability-based behavioral analysis showing that anchors shift entire output distributions, with controls for training-data contamination; (2) exact Shapley-value attribution over structured prompt fields to quantify anchor influence on model log-probabilities; and (3) a unified Anchoring Bias Sensitivity Score integrating behavioral and attributional evidence across six open-source models. Results reveal robust anchoring effects in Gemma-2B, Phi-2, and Llama-2-7B, with attribution signaling that the anchors influence reweighting. Smaller models such as GPT-2, Falcon-RW-1B, and GPT-Neo-125M show variability, suggesting scale may modulate sensitivity. Attributional effects, however, vary across prompt designs, underscoring fragility in treating LLMs as human substitutes. The findings demonstrate that anchoring bias in LLMs is robust, measurable, and interpretable, while highlighting risks in applied domains. More broadly, the framework bridges behavioral science, LLM safety, and interpretability, offering a reproducible path for evaluating other cognitive biases in LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器中的锚定：大型语言模型中的锚定偏见的行为和归因证据</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地被视为行为主体和决策系统进行研究，但观察到的认知偏见是否反映表面模仿或更深层的概率变化仍不清楚。锚定偏见作为经典的人类判断偏见，提供了一个关键的测试案例。尽管先前的研究表明LLMs表现出锚定，但大多数证据依赖于表面输出，内部机制和归因贡献尚未探讨。本文通过三项贡献推进了LLMs中锚定的研究：（1）基于对数概率的行为分析，显示锚定会改变整个输出分布，并控制训练数据污染；（2）对结构化提示字段进行精确的Shapley值归因，以量化锚定对模型对数概率的影响；（3）一个统一的锚定偏见敏感性评分，整合了六个开源模型的行为和归因证据。结果显示Gemma-2B、Phi-2和Llama-2-7B中存在强烈的锚定效应，归因表明锚定影响了重加权。较小的模型如GPT-2、Falcon-RW-1B和GPT-Neo-125M表现出变异性，表明规模可能调节敏感性。然而，归因效应在提示设计中有所不同，强调了将LLMs视为人类替代品的脆弱性。研究结果表明LLMs中的锚定偏见是稳健的、可测量的和可解释的，同时突显了应用领域的风险。更广泛地说，该框架连接了行为科学、LLM安全性和可解释性，为评估LLMs中的其他认知偏见提供了可重复的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the presence of anchoring bias in large language models (LLMs), a cognitive bias that affects human judgment, and aims to clarify whether the observed biases in LLMs stem from superficial imitation or deeper probabilistic shifts. Previous studies primarily relied on surface-level outputs, failing to explore the internal mechanisms and attributional contributions of LLMs. The proposed approach includes a log-probability-based behavioral analysis, Shapley-value attribution to quantify anchor influence, and a unified Anchoring Bias Sensitivity Score, addressing the limitations of earlier methods. The contributions of this paper reveal that anchoring effects are significant in larger models like Gemma-2B and Llama-2-7B, while smaller models exhibit variability, indicating that model scale influences sensitivity to anchoring. The findings underscore the measurable and interpretable nature of anchoring bias in LLMs, emphasizing the importance of understanding these biases in applied contexts and providing a framework for evaluating other cognitive biases in LLMs.</div>
<div class="mono" style="margin-top:8px">本文研究了大型语言模型（LLMs）中的锚定偏差，这是一种影响人类判断的认知偏差，并解决了关于LLMs是否因表面模仿或更深层的概率转变而表现出这种偏差的理解差距。以往的研究主要依赖表面输出，未能探讨内部机制和偏差的归因。提出的方法包括基于对数概率的行为分析、使用Shapley值归因量化锚点影响，以及在多个模型中统一的锚定偏差敏感性评分，这些共同提供了对LLMs中锚定现象更全面的理解。研究方法表明，像Gemma-2B、Phi-2和Llama-2-7B等较大模型显示出显著的锚定效应，而较小模型则表现出变异性，表明模型规模影响对锚定的敏感性。研究结果确认LLMs中的锚定偏差是稳健且可测量的，为行为科学、LLM安全性和可解释性领域做出了贡献。</div>
</details>
</div>
<div class="card">
<div class="title">JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models</div>
<div class="meta-line">Authors: Haibo Jin, Leyang Hu, Xinnuo Li, Peiyan Zhang, Chonghan Chen, Jun Zhuang, Haohan Wang</div>
<div class="meta-line">First: 2024-06-26T02:20:23+00:00 · Latest: 2025-11-07T20:21:15+00:00</div>
<div class="meta-line">Comments: 45 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.01599v3">Abs</a> · <a href="https://arxiv.org/pdf/2407.01599v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid evolution of artificial intelligence (AI) through developments in Large Language Models (LLMs) and Vision-Language Models (VLMs) has brought significant advancements across various technological domains. While these models enhance capabilities in natural language processing and visual interactive tasks, their growing adoption raises critical concerns regarding security and ethical alignment. This survey provides an extensive review of the emerging field of jailbreaking--deliberately circumventing the ethical and operational boundaries of LLMs and VLMs--and the consequent development of defense mechanisms. Our study categorizes jailbreaks into seven distinct types and elaborates on defense strategies that address these vulnerabilities. Through this comprehensive examination, we identify research gaps and propose directions for future studies to enhance the security frameworks of LLMs and VLMs. Our findings underscore the necessity for a unified perspective that integrates both jailbreak strategies and defensive solutions to foster a robust, secure, and reliable environment for the next generation of language models. More details can be found on our website: https://chonghan-chen.com/llm-jailbreak-zoo-survey/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>JailbreakZoo：大型语言模型和视觉语言模型越狱的调查、景观与前景</div>
<div class="mono" style="margin-top:8px">人工智能（AI）通过大型语言模型（LLMs）和视觉语言模型（VLMs）的发展迅速演变，带来了各个技术领域的重大进展。尽管这些模型增强了自然语言处理和视觉交互任务的能力，但它们的日益普及引发了关于安全性和伦理对齐的关键担忧。本调查提供了对新兴越狱领域的广泛回顾——故意绕过LLMs和VLMs的伦理和操作边界——以及随之而来的防御机制的发展。我们的研究将越狱分为七种不同类型，并详细阐述了应对这些漏洞的防御策略。通过这次全面的审查，我们识别了研究空白，并提出了未来研究的方向，以增强LLMs和VLMs的安全框架。我们的发现强调了整合越狱策略和防御解决方案的统一视角的必要性，以促进下一代语言模型的强大、安全和可靠的环境。更多细节请访问我们的网站：https://chonghan-chen.com/llm-jailbreak-zoo-survey/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the rapid advancements in artificial intelligence, particularly in Large Language Models (LLMs) and Vision-Language Models (VLMs), which, while enhancing capabilities in various tasks, also raise significant security and ethical concerns. Previous methods focused on either the development of these models or their ethical implications but lacked a comprehensive approach to jailbreaking, which involves circumventing the operational boundaries of these models. The proposed approach categorizes jailbreaks into seven types and develops corresponding defense strategies, effectively addressing the vulnerabilities identified in existing frameworks. This paper contributes to the field by providing a thorough survey of jailbreaking and defense mechanisms, highlighting research gaps and suggesting future directions. The methodology involves a detailed examination of jailbreaking techniques and defenses, ultimately aiming to enhance the security of LLMs and VLMs, thereby supporting the goal of creating a more secure environment for these technologies.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）和视觉语言模型（VLMs）在广泛使用中所带来的安全和伦理问题。以往的方法主要集中在提升模型能力上，而未能充分解决因越狱而产生的脆弱性问题，越狱是指绕过伦理和操作限制。本文提出了一项全面的越狱技术调查，将其分为七种类型，并讨论相应的防御机制，从而填补了文献中的关键空白。该方法论涉及对现有越狱策略和防御的深入审查，旨在创建一个统一的框架，以增强LLMs和VLMs的安全性。研究结果强调了集成方法在确保未来语言模型的可靠性和安全性方面的迫切需求，支持开发更安全的人工智能系统的目标。</div>
</details>
</div>
<div class="card">
<div class="title">XBreaking: Understanding how LLMs security alignment can be broken</div>
<div class="meta-line">Authors: Marco Arazzi, Vignesh Kumar Kembu, Antonino Nocera, Vinod P</div>
<div class="meta-line">First: 2025-04-30T14:44:24+00:00 · Latest: 2025-11-07T16:21:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.21700v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.21700v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models are fundamental actors in the modern IT landscape dominated by AI solutions. However, security threats associated with them might prevent their reliable adoption in critical application scenarios such as government organizations and medical institutions. For this reason, commercial LLMs typically undergo a sophisticated censoring mechanism to eliminate any harmful output they could possibly produce. These mechanisms maintain the integrity of LLM alignment by guaranteeing that the models respond safely and ethically. In response to this, attacks on LLMs are a significant threat to such protections, and many previous approaches have already demonstrated their effectiveness across diverse domains. Existing LLM attacks mostly adopt a generate-and-test strategy to craft malicious input. To improve the comprehension of censoring mechanisms and design a targeted attack, we propose an Explainable-AI solution that comparatively analyzes the behavior of censored and uncensored models to derive unique exploitable alignment patterns. Then, we propose XBreaking, a novel approach that exploits these unique patterns to break the security and alignment constraints of LLMs by targeted noise injection. Our thorough experimental campaign returns important insights about the censoring mechanisms and demonstrates the effectiveness and performance of our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>XBreaking：理解大型语言模型的安全对齐如何被破坏</div>
<div class="mono" style="margin-top:8px">大型语言模型是现代IT环境中由AI解决方案主导的重要参与者。然而，与之相关的安全威胁可能会阻碍其在政府组织和医疗机构等关键应用场景中的可靠采用。因此，商业大型语言模型通常会经过复杂的审查机制，以消除它们可能产生的任何有害输出。这些机制通过确保模型安全和伦理地响应来维护大型语言模型的对齐完整性。对此，针对大型语言模型的攻击对这些保护构成了重大威胁，许多先前的方法已经在不同领域展示了其有效性。现有的大型语言模型攻击主要采用生成与测试策略来制作恶意输入。为了提高对审查机制的理解并设计针对性攻击，我们提出了一种可解释的AI解决方案，比较分析审查和未审查模型的行为，以推导出独特的可利用对齐模式。然后，我们提出了XBreaking，这是一种新颖的方法，通过有针对性的噪声注入利用这些独特模式来打破大型语言模型的安全和对齐约束。我们全面的实验活动提供了关于审查机制的重要见解，并展示了我们方法的有效性和性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the security vulnerabilities of Large Language Models (LLMs), which are increasingly used in critical applications but face threats that hinder their safe adoption. Previous methods primarily utilized a generate-and-test strategy for crafting malicious inputs, which often lacked a comprehensive understanding of the underlying censoring mechanisms. The proposed approach, XBreaking, differs by employing an Explainable-AI framework to analyze the behaviors of both censored and uncensored models, allowing for the identification of exploitable alignment patterns. This method is well-motivated as it seeks to enhance the understanding of LLM security while effectively breaking alignment constraints through targeted noise injection. The experimental results provide significant insights into censoring mechanisms and demonstrate that XBreaking can successfully compromise LLM security, thereby supporting the goals of improving model safety and reliability.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）的安全漏洞，这些模型在以AI驱动的IT环境中至关重要，但面临威胁，阻碍其在政府和医疗等敏感领域的安全部署。以往的方法主要采用生成与测试策略来制作恶意输入，往往缺乏对底层审查机制的细致理解。所提出的方法XBreaking通过采用可解释AI框架，分析审查和未审查模型之间的行为差异，以识别可利用的对齐模式，从而与众不同。该方法的动机明确，旨在增强对LLM安全性的理解并改善攻击策略。研究方法涉及基于识别模式的针对性噪声注入，实验结果揭示了审查机制的重要见解，同时展示了XBreaking在破坏LLM安全性和对齐方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities</div>
<div class="meta-line">Authors: Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng, Aidan San, Michel Galley, Jianfeng Gao</div>
<div class="meta-line">Venue: NAACL 2025 Oral</div>
<div class="meta-line">First: 2024-10-24T06:36:12+00:00 · Latest: 2025-11-07T10:17:59+00:00</div>
<div class="meta-line">Comments: Accepted to NAACL 2025 Main (Oral)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.18469v6">Abs</a> · <a href="https://arxiv.org/pdf/2410.18469v6">PDF</a> · <a href="https://github.com/SunChungEn/ADV-LLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research has shown that Large Language Models (LLMs) are vulnerable to automated jailbreak attacks, where adversarial suffixes crafted by algorithms appended to harmful queries bypass safety alignment and trigger unintended responses. Current methods for generating these suffixes are computationally expensive and have low Attack Success Rates (ASR), especially against well-aligned models like Llama2 and Llama3. To overcome these limitations, we introduce ADV-LLM, an iterative self-tuning process that crafts adversarial LLMs with enhanced jailbreak ability. Our framework significantly reduces the computational cost of generating adversarial suffixes while achieving nearly 100\% ASR on various open-source LLMs. Moreover, it exhibits strong attack transferability to closed-source models, achieving 99\% ASR on GPT-3.5 and 49\% ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving jailbreak ability, ADV-LLM provides valuable insights for future safety alignment research through its ability to generate large datasets for studying LLM safety. Our code is available at: https://github.com/SunChungEn/ADV-LLM</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强越狱能力的迭代自调节大型语言模型</div>
<div class="mono" style="margin-top:8px">最近的研究表明，大型语言模型（LLMs）易受到自动化越狱攻击，其中由算法生成的对抗后缀附加在有害查询上，绕过安全对齐并触发意外响应。目前生成这些后缀的方法计算成本高，攻击成功率（ASR）低，尤其是针对像Llama2和Llama3这样的良好对齐模型。为克服这些限制，我们引入了ADV-LLM，这是一种迭代自调节过程，旨在制作具有增强越狱能力的对抗性LLM。我们的框架显著降低了生成对抗后缀的计算成本，同时在各种开源LLM上实现了近100%的ASR。此外，它对闭源模型表现出强大的攻击可转移性，在GPT-3.5上实现了99%的ASR，在GPT-4上实现了49%的ASR，尽管仅在Llama3上进行了优化。除了提高越狱能力外，ADV-LLM还通过其生成大规模数据集的能力，为未来的安全对齐研究提供了宝贵的见解。我们的代码可在以下网址获取：https://github.com/SunChungEn/ADV-LLM</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of Large Language Models (LLMs) to automated jailbreak attacks, where adversarial suffixes can bypass safety measures and elicit unintended responses. Previous methods for generating these suffixes have been computationally intensive and yielded low Attack Success Rates (ASR), particularly against well-aligned models like Llama2 and Llama3. The proposed ADV-LLM introduces an iterative self-tuning process that not only reduces the computational cost but also achieves nearly 100% ASR across various open-source LLMs, with strong transferability to closed-source models, attaining 99% ASR on GPT-3.5 and 49% ASR on GPT-4. This contribution not only enhances jailbreak capabilities but also aids future safety alignment research by generating substantial datasets for LLM safety studies.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）在自动越狱攻击中的脆弱性进行探讨，其中对抗后缀能够绕过安全机制并引发意外响应。以往生成这些后缀的方法计算成本高且攻击成功率（ASR）低，尤其是在对齐良好的模型如Llama2和Llama3上。提出的ADV-LLM引入了一种迭代自调节过程，显著降低了计算成本，同时在各种开源LLM上实现了近100%的ASR，并在闭源模型上表现出强大的迁移能力，GPT-3.5的ASR达到99%，GPT-4的ASR达到49%。这一贡献不仅增强了越狱能力，还通过生成大型数据集为未来的安全对齐研究提供了有价值的见解。该方法论采用迭代方法，专门优化对抗LLM，以提高越狱任务的性能，有效支持了研究目标。</div>
</details>
</div>
<div class="card">
<div class="title">Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models</div>
<div class="meta-line">Authors: Cristina Pinneri, Christos Louizos</div>
<div class="meta-line">First: 2025-11-06T14:15:06+00:00 · Latest: 2025-11-06T14:15:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10665v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.10665v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>守护意义：自监督训练提升守护模型的语义鲁棒性</div>
<div class="mono" style="margin-top:8px">守护模型是大型语言模型安全的重要组成部分，但它们对表面语言变化的敏感性仍然是一个关键漏洞。我们展示了即使是保持意义的同义改写也会导致安全评分的大幅波动，揭示了缺乏语义基础。为了解决这个问题，我们引入了一个实用的自监督框架，以提高守护模型的语义鲁棒性。我们的方法利用同义改写集，通过一种新颖的考虑偏斜的聚合策略来强制预测一致性，以实现鲁棒的目标计算。值得注意的是，我们发现标准聚合方法如均值和中位数可能会降低安全性，强调了需要考虑偏斜的替代方案。我们分析了六个开源守护模型，并显示我们的方法将同义改写之间的语义变异性降低了约58%，平均提高基准准确性约2.5%，并能推广到未见的风格变化。有趣的是，我们发现模型校准与一致性之间存在双向关系：我们的鲁棒性训练将校准提高了多达40%，揭示了这些属性之间的基本联系。这些结果突显了将语义一致性视为首要训练目标的价值，并提供了一种可扩展的构建更可靠守护模型的方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of guard models in large language models (LLMs) to superficial linguistic variations, which can lead to significant fluctuations in safety scores even with meaning-preserving paraphrases. Previous methods, primarily relying on standard aggregation techniques like mean and median, have been shown to degrade safety, prompting the need for a more effective approach. The proposed self-supervised framework enhances semantic robustness by utilizing paraphrase sets and a novel skew-aware aggregation strategy, which ensures prediction consistency and reduces semantic variability by approximately 58%. The methodology not only improves benchmark accuracy by about 2.5% but also enhances model calibration by up to 40%, establishing a crucial link between calibration and consistency. This work contributes to the field by treating semantic consistency as a primary training objective, offering a scalable solution for developing more reliable guard models.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）中的守护模型对表面语言变体的脆弱性，这种脆弱性可能导致安全评分的显著波动，尽管意义得以保留。以往的方法，如均值和中位数等标准聚合技术，已被证明会降低安全性而非增强。所提出的自监督框架引入了一种偏斜感知聚合策略，通过对同义句集施加预测一致性来提高语义鲁棒性，有效解决了现有方法的局限性。本文的贡献在于证明该方法将语义变异性降低约58%，基准准确性提高约2.5%，同时将模型校准提高至40%。这些发现支持了通过将语义一致性作为关键训练目标来创建更可靠的守护模型的目标。</div>
</details>
</div>
<div class="card">
<div class="title">AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research</div>
<div class="meta-line">Authors: Tim Beyer, Jonas Dornbusch, Jakob Steimle, Moritz Ladenburger, Leo Schwinn, Stephan Günnemann</div>
<div class="meta-line">First: 2025-11-06T12:38:09+00:00 · Latest: 2025-11-06T12:38:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04316v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.04316v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid expansion of research on Large Language Model (LLM) safety and robustness has produced a fragmented and oftentimes buggy ecosystem of implementations, datasets, and evaluation methods. This fragmentation makes reproducibility and comparability across studies challenging, hindering meaningful progress. To address these issues, we introduce AdversariaLLM, a toolbox for conducting LLM jailbreak robustness research. Its design centers on reproducibility, correctness, and extensibility. The framework implements twelve adversarial attack algorithms, integrates seven benchmark datasets spanning harmfulness, over-refusal, and utility evaluation, and provides access to a wide range of open-weight LLMs via Hugging Face. The implementation includes advanced features for comparability and reproducibility such as compute-resource tracking, deterministic results, and distributional evaluation techniques. \name also integrates judging through the companion package JudgeZoo, which can also be used independently. Together, these components aim to establish a robust foundation for transparent, comparable, and reproducible research in LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdversariaLLM：一个统一的模块化LLM鲁棒性研究工具箱</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）安全性和鲁棒性研究的快速扩展产生了一个支离破碎且常常存在缺陷的实现、数据集和评估方法生态系统。这种碎片化使得研究之间的可重复性和可比性变得具有挑战性，阻碍了有意义的进展。为了解决这些问题，我们推出了AdversariaLLM，一个用于进行LLM越狱鲁棒性研究的工具箱。其设计以可重复性、正确性和可扩展性为中心。该框架实现了十二种对抗攻击算法，整合了七个涵盖有害性、过度拒绝和效用评估的基准数据集，并通过Hugging Face提供对多种开放权重LLM的访问。该实现包括可比性和可重复性的高级特性，如计算资源跟踪、确定性结果和分布评估技术。\name还通过伴随包JudgeZoo集成了判断功能，后者也可以独立使用。这些组件共同旨在为LLM安全研究建立一个透明、可比和可重复的坚实基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges posed by the fragmented ecosystem of implementations, datasets, and evaluation methods in Large Language Model (LLM) safety and robustness, which complicates reproducibility and comparability across studies. Previous methods have led to inconsistencies and bugs, making it difficult to achieve meaningful progress in the field. The proposed AdversariaLLM toolbox aims to unify and modularize LLM robustness research by focusing on reproducibility, correctness, and extensibility, thus overcoming the limitations of existing approaches. This toolbox implements twelve adversarial attack algorithms and integrates seven benchmark datasets, while also providing access to various open-weight LLMs. The methodology includes features for compute-resource tracking and deterministic results, which enhance comparability and reproducibility. The performance of AdversariaLLM supports its goals by establishing a robust foundation for transparent and comparable research in LLM safety.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLM）安全性和鲁棒性研究中实施、数据集和评估方法碎片化带来的挑战，这使得研究的可重复性和可比性变得复杂。以往的方法常常导致实现存在缺陷和缺乏标准化，从而阻碍了有意义的进展。提出的AdversariaLLM工具箱旨在通过关注可重复性、正确性和可扩展性来统一和模块化LLM鲁棒性研究，整合了十二种对抗攻击算法和七个基准数据集。该方法论包括计算资源跟踪和确定性结果等高级功能，促进了透明和可比的研究。该工具箱的全面设计支持对LLM安全性的可靠评估，为未来的研究提供了重要的框架。</div>
</details>
</div>
<div class="card">
<div class="title">An Automated Framework for Strategy Discovery, Retrieval, and Evolution in LLM Jailbreak Attacks</div>
<div class="meta-line">Authors: Xu Liu, Yan Chen, Kan Ling, Yichi Zhu, Hengrun Zhang, Guisheng Fan, Huiqun Yu</div>
<div class="meta-line">First: 2025-11-04T08:24:22+00:00 · Latest: 2025-11-04T08:24:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.02356v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.02356v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread deployment of Large Language Models (LLMs) as public-facing web services and APIs has made their security a core concern for the web ecosystem. Jailbreak attacks, as one of the significant threats to LLMs, have recently attracted extensive research. In this paper, we reveal a jailbreak strategy which can effectively evade current defense strategies. It can extract valuable information from failed or partially successful attack attempts and contains self-evolution from attack interactions, resulting in sufficient strategy diversity and adaptability. Inspired by continuous learning and modular design principles, we propose ASTRA, a jailbreak framework that autonomously discovers, retrieves, and evolves attack strategies to achieve more efficient and adaptive attacks. To enable this autonomous evolution, we design a closed-loop &quot;attack-evaluate-distill-reuse&quot; core mechanism that not only generates attack prompts but also automatically distills and generalizes reusable attack strategies from every interaction. To systematically accumulate and apply this attack knowledge, we introduce a three-tier strategy library that categorizes strategies into Effective, Promising, and Ineffective based on their performance scores. The strategy library not only provides precise guidance for attack generation but also possesses exceptional extensibility and transferability. We conduct extensive experiments under a black-box setting, and the results show that ASTRA achieves an average Attack Success Rate (ASR) of 82.7%, significantly outperforming baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于策略发现、检索和演化的自动化框架在LLM越狱攻击中的应用</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）作为面向公众的网络服务和API的广泛部署，使其安全性成为网络生态系统的核心关注点。越狱攻击作为对LLMs的重大威胁之一，最近引起了广泛的研究。本文揭示了一种越狱策略，可以有效规避当前的防御策略。它能够从失败或部分成功的攻击尝试中提取有价值的信息，并包含来自攻击交互的自我演化，导致足够的策略多样性和适应性。受到持续学习和模块化设计原则的启发，我们提出了ASTRA，一个越狱框架，能够自主发现、检索和演化攻击策略，以实现更高效和适应性的攻击。为了实现这种自主演化，我们设计了一个闭环的“攻击-评估-提炼-重用”核心机制，不仅生成攻击提示，还自动提炼和概括每次交互中的可重用攻击策略。为了系统地积累和应用这些攻击知识，我们引入了一个三级策略库，根据性能评分将策略分类为有效、前景良好和无效。策略库不仅为攻击生成提供精确指导，还具有卓越的可扩展性和可转移性。我们在黑箱设置下进行了广泛的实验，结果表明ASTRA的平均攻击成功率（ASR）为82.7%，显著优于基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the security vulnerabilities of Large Language Models (LLMs) due to the rise of jailbreak attacks, which pose significant threats to their deployment as web services. Previous methods have struggled with adaptability and efficiency in evolving attack strategies, often failing to leverage insights from past attack attempts. The proposed approach, ASTRA, distinguishes itself by autonomously discovering, retrieving, and evolving attack strategies through a closed-loop mechanism that generates and refines attack prompts based on interaction feedback. This method is well-motivated by principles of continuous learning and modular design, contributing a systematic strategy library that categorizes attack strategies for effective guidance. In extensive black-box experiments, ASTRA achieved an average Attack Success Rate (ASR) of 82.7%, demonstrating its superior performance compared to existing methods and supporting its goals of enhancing attack efficiency and adaptability.</div>
<div class="mono" style="margin-top:8px">本研究关注由于大型语言模型（LLMs）作为网络服务广泛使用而出现的安全漏洞，特别是越狱攻击所带来的威胁。以往的方法在攻击策略的适应性和效率方面存在不足，常常未能利用过去攻击尝试中的见解。所提出的方法ASTRA通过一个闭环机制，结合持续学习原则，自动发现、检索和演变攻击策略，从而与现有方法区分开来。该框架系统地将策略分类为基于性能的三层库，增强了攻击的适应性和有效性。实验结果表明，ASTRA的平均攻击成功率（ASR）达到82.7%，显著优于现有方法，支持其高效和适应性策略演变的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges</div>
<div class="meta-line">Authors: Hamin Koo, Minseon Kim, Jaehyung Kim</div>
<div class="meta-line">First: 2025-11-03T09:18:27+00:00 · Latest: 2025-11-03T09:18:27+00:00</div>
<div class="meta-line">Comments: under review, 28 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.01375v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.01375v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Identifying the vulnerabilities of large language models (LLMs) is crucial for improving their safety by addressing inherent weaknesses. Jailbreaks, in which adversaries bypass safeguards with crafted input prompts, play a central role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors. Recent optimization-based jailbreak approaches iteratively refine attack prompts by leveraging LLMs. However, they often rely heavily on either binary attack success rate (ASR) signals, which are sparse, or manually crafted scoring templates, which introduce human bias and uncertainty in the scoring outcomes. To address these limitations, we introduce AMIS (Align to MISalign), a meta-optimization framework that jointly evolves jailbreak prompts and scoring templates through a bi-level structure. In the inner loop, prompts are refined using fine-grained and dense feedback using a fixed scoring template. In the outer loop, the template is optimized using an ASR alignment score, gradually evolving to better reflect true attack outcomes across queries. This co-optimization process yields progressively stronger jailbreak prompts and more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors demonstrate that AMIS achieves state-of-the-art performance, including 88.0% ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming existing baselines by substantial margins.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对齐与错位：使用元优化的LLM评估者进行自动LLM越狱</div>
<div class="mono" style="margin-top:8px">识别大型语言模型（LLMs）的脆弱性对于通过解决固有弱点来提高其安全性至关重要。越狱是指对手通过精心设计的输入提示绕过保护措施，这在红队测试中起着核心作用，通过探测LLMs以引发意外或不安全的行为。最近的基于优化的越狱方法通过利用LLMs迭代地优化攻击提示。然而，它们通常过于依赖稀疏的二元攻击成功率（ASR）信号或手动制作的评分模板，这引入了人为偏见和评分结果的不确定性。为了解决这些局限性，我们引入了AMIS（对齐与错位），一个通过双层结构共同演化越狱提示和评分模板的元优化框架。在内循环中，使用固定评分模板通过细粒度和密集反馈来优化提示。在外循环中，使用ASR对齐分数优化模板，逐渐演变以更好地反映查询的真实攻击结果。这个共同优化过程产生了逐渐更强的越狱提示和更校准的评分信号。在AdvBench和JBB-Behaviors上的评估表明，AMIS实现了最先进的性能，包括在Claude-3.5-Haiku上达到88.0%的ASR和在Claude-4-Sonnet上达到100.0%的ASR，显著超越现有基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical need to identify vulnerabilities in large language models (LLMs) to enhance their safety against adversarial attacks, particularly through jailbreaks that exploit these weaknesses. Previous methods for optimizing jailbreak prompts have relied on binary attack success rate (ASR) signals or manually crafted scoring templates, both of which suffer from sparsity and bias, respectively. The proposed approach, AMIS (Align to MISalign), introduces a meta-optimization framework that simultaneously evolves jailbreak prompts and scoring templates, utilizing a bi-level structure for improved feedback and scoring accuracy. This methodology allows for the generation of more effective jailbreak prompts and calibrated scoring signals. The experimental results on AdvBench and JBB-Behaviors indicate that AMIS achieves state-of-the-art performance, with an ASR of 88.0% on Claude-3.5-Haiku and 100.0% on Claude-4-Sonnet, significantly surpassing existing methods.</div>
<div class="mono" style="margin-top:8px">本研究解决了识别大型语言模型（LLMs）漏洞的关键需求，以增强其抵御对抗性攻击的安全性，特别是通过利用这些弱点的越狱攻击。以往的方法主要依赖于二元攻击成功率（ASR）信号或手动制作的评分模板，这导致了评估攻击有效性时的稀疏性和人为偏见问题。所提出的AMIS（Align to MISalign）框架引入了一种元优化方法，通过双层结构同时演化越狱提示和评分模板，以提高反馈和评分的准确性。这种方法论使得生成更有效的越狱提示和校准的评分信号成为可能。在AdvBench和JBB-Behaviors上的实验结果表明，AMIS达到了最先进的性能，在Claude-3.5-Haiku上实现了88.0%的ASR，在Claude-4-Sonnet上达到了100%的ASR，显著超越了现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Exploiting Latent Space Discontinuities for Building Universal LLM Jailbreaks and Data Extraction Attacks</div>
<div class="meta-line">Authors: Kayua Oleques Paim, Rodrigo Brandao Mansilha, Diego Kreutz, Muriel Figueredo Franco, Weverton Cordeiro</div>
<div class="meta-line">First: 2025-11-01T01:19:12+00:00 · Latest: 2025-11-01T01:19:12+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures, 4 tables, Published at the Brazilian Symposium on Cybersecurity (SBSeg 2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00346v1">Abs</a> · <a href="https://arxiv.org/pdf/2511.00346v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid proliferation of Large Language Models (LLMs) has raised significant concerns about their security against adversarial attacks. In this work, we propose a novel approach to crafting universal jailbreaks and data extraction attacks by exploiting latent space discontinuities, an architectural vulnerability related to the sparsity of training data. Unlike previous methods, our technique generalizes across various models and interfaces, proving highly effective in seven state-of-the-art LLMs and one image generation model. Initial results indicate that when these discontinuities are exploited, they can consistently and profoundly compromise model behavior, even in the presence of layered defenses. The findings suggest that this strategy has substantial potential as a systemic attack vector.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用潜在空间不连续性构建通用LLM越狱和数据提取攻击</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的快速普及引发了对其安全性在对抗性攻击下的重大担忧。在本研究中，我们提出了一种新颖的方法，通过利用潜在空间不连续性（与训练数据稀疏性相关的架构漏洞）来构建通用越狱和数据提取攻击。与之前的方法不同，我们的技术在各种模型和接口中具有广泛的适用性，在七个最先进的LLM和一个图像生成模型中证明了其高效性。初步结果表明，当利用这些不连续性时，它们可以持续而深刻地破坏模型行为，即使在分层防御的情况下也是如此。研究结果表明，这一策略作为系统性攻击向量具有巨大的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing security concerns surrounding Large Language Models (LLMs) due to their vulnerability to adversarial attacks. Previous methods for exploiting these vulnerabilities have been limited in their applicability and effectiveness across different models. The proposed approach leverages latent space discontinuities, a specific architectural weakness linked to training data sparsity, allowing for the creation of universal jailbreaks and data extraction attacks that generalize across various models and interfaces. This method is well-motivated as it effectively compromises model behavior even against layered defenses. The paper contributes by demonstrating the effectiveness of this technique on seven state-of-the-art LLMs and one image generation model, achieving significant performance in undermining model security, thus supporting the goal of identifying systemic attack vectors.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）日益严重的安全问题，尤其是它们对对抗性攻击的脆弱性。以往的方法在不同模型间的特异性和有效性上存在不足，常常无法在各种接口中进行泛化或适应。所提出的方法利用潜在空间的不连续性，这一弱点源于训练数据的稀疏性，从而能够创建通用的越狱和数据提取攻击，适用于多种模型。该方法的动机充分，因为它利用了一个基本的架构缺陷，显示出作为系统性攻击向量的巨大潜力。该方法论涉及设计攻击，能够持续地破坏七个最先进的LLM和一个图像生成模型的行为，即使在分层防御下也能取得显著的性能，从而支持了研究目标，即增强对LLM脆弱性的理解。</div>
</details>
</div>
<div class="card">
<div class="title">SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents</div>
<div class="meta-line">Authors: Sheng Yin, Xianghe Pang, Yuanzhuo Ding, Menglan Chen, Yutong Bi, Yichen Xiong, Wenhao Huang, Zhen Xiang, Jing Shao, Siheng Chen</div>
<div class="meta-line">First: 2024-12-17T18:55:58+00:00 · Latest: 2025-10-31T08:18:50+00:00</div>
<div class="meta-line">Comments: 28 pages, 19 tables, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.13178v5">Abs</a> · <a href="https://arxiv.org/pdf/2412.13178v5">PDF</a> · <a href="https://github.com/shengyin1224/SafeAgentBench">Code1</a> · <a href="https://huggingface.co/datasets/safeagentbench/SafeAgentBench">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the integration of large language models (LLMs), embodied agents have strong capabilities to understand and plan complicated natural language instructions. However, a foreseeable issue is that those embodied agents can also flawlessly execute some hazardous tasks, potentially causing damages in the real world. Existing benchmarks predominantly overlook critical safety risks, focusing solely on planning performance, while a few evaluate LLMs&#x27; safety awareness only on non-interactive image-text data. To address this gap, we present SafeAgentBench -- the first comprehensive benchmark for safety-aware task planning of embodied LLM agents in interactive simulation environments, covering both explicit and implicit hazards. SafeAgentBench includes: (1) an executable, diverse, and high-quality dataset of 750 tasks, rigorously curated to cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17 high-level actions for 9 state-of-the-art baselines; and (3) reliable evaluation methods from both execution and semantic perspectives. Experimental results show that, although agents based on different design frameworks exhibit substantial differences in task success rates, their overall safety awareness remains weak. The most safety-conscious baseline achieves only a 10% rejection rate for detailed hazardous tasks. Moreover, simply replacing the LLM driving the agent does not lead to notable improvements in safety awareness. Dataset and codes are available in https://github.com/shengyin1224/SafeAgentBench and https://huggingface.co/datasets/safeagentbench/SafeAgentBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeAgentBench：具身LLM代理的安全任务规划基准</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）的集成，具身代理在理解和规划复杂自然语言指令方面具有强大的能力。然而，一个可预见的问题是，这些具身代理也可以无缝执行一些危险任务，可能在现实世界中造成损害。现有基准主要忽视关键的安全风险，仅关注规划性能，而少数评估LLMs的安全意识仅基于非交互式图像-文本数据。为了解决这一空白，我们提出了SafeAgentBench——第一个全面的具身LLM代理在交互式仿真环境中安全任务规划的基准，涵盖显性和隐性危害。SafeAgentBench包括：（1）一个可执行的、多样化的高质量数据集，包含750个任务，严格策划以覆盖10种潜在危害和3种任务类型；（2）SafeAgentEnv，一个具有低级控制器的通用具身环境，支持多代理执行，具有9个最先进基线的17个高级动作；（3）从执行和语义角度出发的可靠评估方法。实验结果表明，尽管基于不同设计框架的代理在任务成功率上表现出显著差异，但它们的整体安全意识仍然较弱。最具安全意识的基线在详细危险任务中的拒绝率仅为10%。此外，仅仅更换驱动代理的LLM并未显著提高安全意识。数据集和代码可在https://github.com/shengyin1224/SafeAgentBench和https://huggingface.co/datasets/safeagentbench/SafeAgentBench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical issue of safety in task planning for embodied agents powered by large language models (LLMs), which can inadvertently execute hazardous tasks. Previous benchmarks primarily focused on planning performance without adequately considering safety risks, and those that did evaluate safety often relied on non-interactive data. The proposed SafeAgentBench differs by providing a comprehensive benchmark that evaluates safety-aware task planning in interactive environments, addressing the oversight of safety in existing methods. This paper contributes a diverse dataset of 750 tasks that encompass various hazards, a universal embodied environment for multi-agent execution, and reliable evaluation methods. Experimental findings reveal that while different agent designs show varying task success rates, overall safety awareness is lacking, with the best baseline achieving only a 10% rejection rate for hazardous tasks, indicating the need for improved safety measures in embodied agents.</div>
<div class="mono" style="margin-top:8px">本研究关注使用大型语言模型（LLM）进行任务规划的具身代理的安全性问题，因为现有基准主要关注规划性能，而忽视了安全风险。以往的方法在评估安全意识时存在不足，通常依赖非交互式的图像-文本数据，这无法捕捉现实世界交互的复杂性。所提出的SafeAgentBench提供了一个全面的基准，包括750个任务的多样化数据集，旨在评估交互式仿真环境中的安全性，涵盖显性和隐性危险。这种方法允许对安全意识任务规划进行更全面的评估，结果显示，即使是最注重安全的代理在处理危险任务时的拒绝率也仅为10%，这表明不同设计框架的安全意识存在显著差距。研究结果强调了在具身LLM代理中改善安全措施的必要性，因为仅仅更改LLM并未提高安全结果。</div>
</details>
</div>
<div class="card">
<div class="title">ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio-Language Models</div>
<div class="meta-line">Authors: Weifei Jin, Yuxin Cao, Junjie Su, Minhui Xue, Jie Hao, Ke Xu, Jin Song Dong, Derui Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-30T03:19:59+00:00 · Latest: 2025-10-30T03:19:59+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26096v1">Abs</a> · <a href="https://arxiv.org/pdf/2510.26096v1">PDF</a> · <a href="https://github.com/WeifeiJin/ALMGuard">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats. To address this issue, we propose ALMGuard, the first defense framework tailored to ALMs. Based on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time. To better sift out effective triggers while preserving the model&#x27;s utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding. Both theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, \MethodName reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at https://github.com/WeifeiJin/ALMGuard.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ALMGuard：音频语言模型的安全捷径及其发现方式</div>
<div class="mono" style="margin-top:8px">音频语言模型（ALMs）的最新进展显著提高了多模态理解能力。然而，音频模态的引入也带来了新的独特脆弱性向量。之前的研究提出了专门针对ALMs的越狱攻击，揭示了直接从传统音频对抗攻击或基于文本的大型语言模型（LLM）越狱转移的防御在这些ALM特定威胁面前大多无效。为了解决这个问题，我们提出了ALMGuard，这是第一个针对ALMs量身定制的防御框架。基于安全对齐捷径自然存在于ALMs的假设，我们设计了一种方法来识别通用捷径激活扰动（SAPs），作为触发器以激活安全捷径，在推理时保护ALMs。为了更好地筛选有效触发器，同时保持模型在良性任务上的效用，我们进一步提出了梅尔梯度稀疏掩码（M-GSM），该方法将扰动限制在对越狱敏感但对语音理解不敏感的梅尔频率区间。理论分析和实证结果均表明我们的方法对已知和未知攻击的鲁棒性。总体而言，\MethodName将四个模型中先进的ALM特定越狱攻击的平均成功率降低到4.6%，同时在良性基准上保持了可比的效用，确立了其作为新的最先进技术。我们的代码和数据可在https://github.com/WeifeiJin/ALMGuard获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of Audio-Language Models (ALMs), which have emerged with significant advancements in multimodal understanding but also introduce unique security risks. Previous methods, including traditional audio adversarial attacks and text-based Large Language Model jailbreaks, have proven ineffective against these specific threats. The proposed approach, ALMGuard, is motivated by the belief that safety-aligned shortcuts exist within ALMs and introduces a novel defense framework that identifies universal Shortcut Activation Perturbations (SAPs) to activate these safety mechanisms during inference. This methodology includes the Mel-Gradient Sparse Mask (M-GSM) to ensure that perturbations target sensitive Mel-frequency bins without compromising the model&#x27;s performance on benign tasks. The experimental results indicate that ALMGuard reduces the success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models while maintaining comparable utility on benign benchmarks, thus establishing a new state of the art in this domain.</div>
<div class="mono" style="margin-top:8px">本研究关注音频语言模型（ALM）引入的脆弱性，强调现有防御措施（源自传统音频对抗攻击和基于文本的大型语言模型（LLM）越狱）在应对ALM特有威胁时的不足。提出的方法ALMGuard基于安全对齐快捷方式在ALM中自然存在的假设，设计了一种新颖的防御框架，通过识别通用的快捷激活扰动（SAP）在推理时激活这些安全机制。该方法还包括梅尔梯度稀疏掩码（M-GSM），确保扰动在有效对抗越狱的同时保持模型在良性任务上的性能。实验结果表明，ALMGuard将四个模型的高级ALM特定越狱攻击的成功率显著降低至4.6%，同时在良性基准上保持了可比的效用，从而确立了ALM安全的新标杆。</div>
</details>
</div>
<div class="card">
<div class="title">Improving LLM Safety Alignment with Dual-Objective Optimization</div>
<div class="meta-line">Authors: Xuandong Zhao, Will Cai, Tianneng Shi, David Huang, Licong Lin, Song Mei, Dawn Song</div>
<div class="meta-line">Venue: ICML 2025</div>
<div class="meta-line">First: 2025-03-05T18:01:05+00:00 · Latest: 2025-10-30T01:16:06+00:00</div>
<div class="meta-line">Comments: ICML 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.03710v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.03710v3">PDF</a> · <a href="https://github.com/wicai24/DOOR-Alignment">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research also suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment. The code is available at https://github.com/wicai24/DOOR-Alignment</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过双目标优化提高大型语言模型的安全对齐</div>
<div class="mono" style="margin-top:8px">现有的大型语言模型（LLMs）训练时安全对齐技术仍然容易受到越狱攻击。直接偏好优化（DPO）作为一种广泛应用的对齐方法，在实验和理论背景下均表现出局限性，因为其损失函数在拒绝学习中被证明是次优的。通过基于梯度的分析，我们识别了这些缺陷，并提出了一种改进的安全对齐方法，将DPO目标分解为两个组成部分：（1）强健的拒绝训练，鼓励在产生部分不安全生成时仍然拒绝，以及（2）针对有害知识的有针对性遗忘。这种方法显著提高了LLM对各种越狱攻击的鲁棒性，包括预填充、后缀和多轮攻击，涵盖了分布内和分布外场景。此外，我们引入了一种通过结合基于奖励的令牌级加权机制来强调关键拒绝令牌的方法，以进一步提高对抗性利用的鲁棒性。我们的研究还表明，对越狱攻击的鲁棒性与训练过程中的令牌分布变化以及拒绝和有害令牌的内部表示相关，为未来LLM安全对齐的研究提供了宝贵的方向。代码可在 https://github.com/wicai24/DOOR-Alignment 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of existing safety alignment techniques for large language models (LLMs), particularly their susceptibility to jailbreak attacks. Previous methods, such as direct preference optimization (DPO), have been found to be suboptimal for refusal learning, leading to inadequate robustness against adversarial exploits. This paper proposes a dual-objective optimization approach that separates DPO objectives into robust refusal training and targeted unlearning of harmful knowledge, effectively enhancing LLM resilience against various jailbreak attacks. The methodology involves gradient-based analysis and a reward-based token-level weighting mechanism to prioritize critical refusal tokens, resulting in improved performance across both in-distribution and out-of-distribution scenarios. The findings indicate a significant increase in robustness against jailbreak attacks, supporting the goal of enhancing LLM safety alignment.</div>
<div class="mono" style="margin-top:8px">本文研究了大型语言模型（LLMs）在面对越狱攻击时的脆弱性，强调了现有安全对齐技术的局限性，特别是直接偏好优化（DPO），该方法在拒绝学习方面表现不佳。所提出的方法通过将DPO的目标分解为强健拒绝训练和有针对性的有害知识去除，显著增强了LLM对各种越狱攻击类型的鲁棒性。研究方法包括基于梯度的分析和用于拒绝学习的基于奖励的令牌级加权机制，这些共同促进了安全对齐性能的显著提升。实验表明，所提出的方法在对抗分布内和分布外的越狱攻击时都实现了更好的鲁棒性，支持了增强LLM安全对齐的目标。</div>
</details>
</div>
<div class="card">
<div class="title">MixAT: Combining Continuous and Discrete Adversarial Training for LLMs</div>
<div class="meta-line">Authors: Csaba Dékány, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, Martin Vechev</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-22T17:32:50+00:00 · Latest: 2025-10-28T09:41:22+00:00</div>
<div class="meta-line">Comments: Published at 39th Conference on Neural Information Processing Systems (NeurIPS 2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.16947v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.16947v2">PDF</a> · <a href="https://github.com/insait-institute/MixAT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent efforts in Large Language Model (LLM) safety and alignment, current adversarial attacks on frontier LLMs can still consistently force harmful generations. Although adversarial training has been widely studied and shown to significantly improve the robustness of traditional machine learning models, its strengths and weaknesses in the context of LLMs are less understood. Specifically, while existing discrete adversarial attacks are effective at producing harmful content, training LLMs with concrete adversarial prompts is often computationally expensive, leading to reliance on continuous relaxations. At the same time, despite their effectiveness and generalization capabilities, training with continuous perturbations does not always capture the full spectrum of vulnerabilities exploited by discrete attacks. In this work, we aim to bridge this gap by introducing MixAT, a novel method that combines stronger discrete and faster continuous attacks during training. We rigorously evaluate MixAT across a wide spectrum of state-of-the-art attacks, proposing the At Least One Attack Success Rate (ALO-ASR) metric to capture the worst-case vulnerability of models. We show MixAT achieves substantially better robustness (ALO-ASR &lt; 20%) compared to prior defenses (ALO-ASR &gt; 50%), while maintaining a runtime comparable to methods based on continuous relaxations. We further analyze MixAT in realistic deployment settings, exploring how chat templates, quantization, low-rank adapters, and temperature affect both adversarial training and evaluation, revealing additional blind spots in current methodologies. Our results demonstrate that MixAT&#x27;s discrete-continuous defense offers a principled and superior robustness-accuracy tradeoff with minimal computational overhead, highlighting its promise for building safer LLMs. We provide our code and models at https://github.com/insait-institute/MixAT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MixAT：结合连续和离散对抗训练的LLM</div>
<div class="mono" style="margin-top:8px">尽管近期在大型语言模型（LLM）安全性和对齐方面进行了努力，但当前对前沿LLM的对抗攻击仍能持续强制产生有害生成。虽然对抗训练已被广泛研究并显示出显著提高传统机器学习模型的鲁棒性，但其在LLM背景下的优缺点尚不清楚。具体而言，现有的离散对抗攻击在产生有害内容方面有效，但使用具体对抗提示训练LLM通常计算成本高，导致依赖于连续松弛。同时，尽管连续扰动的训练有效且具有泛化能力，但并不总能捕捉到离散攻击所利用的全部脆弱性。在本研究中，我们旨在通过引入MixAT来弥补这一差距，这是一种在训练过程中结合更强的离散攻击和更快的连续攻击的新方法。我们严格评估MixAT在广泛的最先进攻击中的表现，提出了至少一个攻击成功率（ALO-ASR）指标，以捕捉模型的最坏情况脆弱性。我们展示了MixAT相比于先前的防御方法（ALO-ASR &gt; 50%）实现了显著更好的鲁棒性（ALO-ASR &lt; 20%），同时保持与基于连续松弛方法相当的运行时间。我们进一步分析了MixAT在现实部署环境中的表现，探讨了聊天模板、量化、低秩适配器和温度如何影响对抗训练和评估，揭示了当前方法中的额外盲点。我们的结果表明，MixAT的离散-连续防御提供了原则性和优越的鲁棒性-准确性权衡，计算开销最小，突显了其在构建更安全LLM方面的潜力。我们在https://github.com/insait-institute/MixAT提供了我们的代码和模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the ongoing challenges in ensuring the safety and alignment of Large Language Models (LLMs) against adversarial attacks, which can lead to harmful outputs. Previous methods primarily focused on discrete adversarial attacks, which are effective but computationally intensive, while continuous perturbations, although faster, fail to capture all vulnerabilities. The proposed MixAT method innovatively combines both discrete and continuous adversarial training, effectively bridging the gap between the two approaches. The research methodology involves rigorous evaluation using the At Least One Attack Success Rate (ALO-ASR) metric, demonstrating that MixAT achieves significantly improved robustness (ALO-ASR &lt; 20%) compared to existing defenses (ALO-ASR &gt; 50%) while maintaining efficient runtime. These findings support the goal of developing safer LLMs by providing a robust and computationally efficient training framework.</div>
<div class="mono" style="margin-top:8px">本文解决了大型语言模型（LLMs）在安全性和对齐性方面面临的持续挑战，特别是在面对可能导致有害输出的对抗性攻击时。以往的方法主要依赖于离散对抗攻击，虽然有效，但计算成本高且无法全面捕捉这些攻击所利用的脆弱性。所提出的方法MixAT创新性地结合了离散和连续对抗训练，以增强鲁棒性而不产生显著的计算成本。该工作的贡献在于引入了至少一次攻击成功率（ALO-ASR）指标来评估模型脆弱性，结果表明MixAT在鲁棒性方面取得了显著改善（ALO-ASR &lt; 20%），而现有防御方法的ALO-ASR超过50%，同时保持了运行效率。该方法包括对各种最先进攻击的严格评估和对部署环境的分析，表明MixAT有效地平衡了鲁棒性和准确性，使其成为构建更安全LLMs的有前景的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">SAGE: A Generic Framework for LLM Safety Evaluation</div>
<div class="meta-line">Authors: Madhur Jindal, Hari Shrawgi, Parag Agrawal, Sandipan Dandapat</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-04-28T11:01:08+00:00 · Latest: 2025-10-27T10:19:55+00:00</div>
<div class="meta-line">Comments: Accepted to EMNLP 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.19674v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.19674v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Language Models are rapidly deployed across diverse applications from healthcare to financial advice, safety evaluation struggles to keep pace. Current benchmarks focus on single-turn interactions with generic policies, failing to capture the conversational dynamics of real-world usage and the application-specific harms that emerge in context. Such potential oversights can lead to harms that go unnoticed in standard safety benchmarks and other current evaluation methodologies. To address these needs for robust AI safety evaluation, we introduce SAGE (Safety AI Generic Evaluation), an automated modular framework designed for customized and dynamic harm evaluations. SAGE employs prompted adversarial agents with diverse personalities based on the Big Five model, enabling system-aware multi-turn conversations that adapt to target applications and harm policies. We evaluate seven state-of-the-art LLMs across three applications and harm policies. Multi-turn experiments show that harm increases with conversation length, model behavior varies significantly when exposed to different user personalities and scenarios, and some models minimize harm via high refusal rates that reduce usefulness. We also demonstrate policy sensitivity within a harm category where tightening a child-focused sexual policy substantially increases measured defects across applications. These results motivate adaptive, policy-aware, and context-specific testing for safer real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAGE：一种通用的LLM安全评估框架</div>
<div class="mono" style="margin-top:8px">随着大型语言模型在医疗保健到金融建议等多种应用中迅速部署，安全评估难以跟上步伐。目前的基准测试侧重于与通用策略的单轮交互，未能捕捉现实使用中的对话动态和上下文中出现的特定应用危害。这些潜在的忽视可能导致在标准安全基准和其他当前评估方法中未被注意的危害。为满足对强大AI安全评估的需求，我们引入了SAGE（安全AI通用评估），这是一个旨在定制和动态危害评估的自动化模块化框架。SAGE采用基于五大人格模型的多样化个性化对抗代理，能够进行系统感知的多轮对话，适应目标应用和危害政策。我们在三个应用和危害政策中评估了七种最先进的LLM。多轮实验表明，随着对话长度的增加，危害也在增加；模型行为在不同用户个性和场景下显著变化；一些模型通过高拒绝率来最小化危害，从而降低了实用性。我们还展示了在一个危害类别内的政策敏感性，其中收紧以儿童为中心的性政策显著增加了跨应用的缺陷测量。这些结果激励了针对更安全的现实世界部署进行自适应、政策感知和上下文特定的测试。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the urgent need for effective safety evaluation of Large Language Models (LLMs) as their deployment in various fields accelerates, revealing limitations in existing benchmarks that primarily assess single-turn interactions without considering the complexities of real-world conversations. Previous methods have overlooked the dynamic nature of interactions and specific application-related harms, leading to potential safety oversights. The proposed SAGE framework introduces a modular and automated approach that utilizes prompted adversarial agents with diverse personalities to facilitate multi-turn conversations tailored to specific applications and harm policies. This methodology allows for a more nuanced evaluation of LLMs, demonstrating that harm can increase with conversation length and that model responses vary significantly based on user personality and context. The findings indicate a need for adaptive and context-aware safety evaluations, highlighting the importance of policy sensitivity in assessing LLM performance across different applications.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）安全评估的迫切需求，因为它们在各种应用中越来越多地被使用，而现有基准往往未能考虑多轮交互和特定上下文危害的复杂性。以往的方法主要集中在单轮交互和通用策略上，忽视了现实世界对话的动态特性，可能导致未被注意的风险。提出的SAGE框架通过利用具有多样化个性的对抗代理，促进针对特定应用和危害政策的系统感知多轮对话，从而显著改善了这一问题。这种方法具有良好的动机，因为它增强了AI安全评估的稳健性。该方法论涉及对七种最先进的LLMs在三个应用和危害政策下的评估，结果显示，随着对话长度的增加，危害往往会增加，并且模型行为在不同用户个性下显著变化。这些发现强调了适应性和特定上下文测试的必要性，以确保LLMs在现实场景中的更安全部署。</div>
</details>
</div>
<div class="card">
<div class="title">Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks</div>
<div class="meta-line">Authors: Md. Mehedi Hasan, Ziaur Rahman, Rafid Mostafiz, Md. Abir Hossain</div>
<div class="meta-line">First: 2025-10-26T11:19:47+00:00 · Latest: 2025-10-26T11:19:47+00:00</div>
<div class="meta-line">Comments: 11 pages, 5 figures. Preprint version under review in the area of Artificial Intelligence (cs.AI)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.22628v1">Abs</a> · <a href="https://arxiv.org/pdf/2510.22628v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a real-time modular defense system named Sentra-Guard. The system detects and mitigates jailbreak and prompt injection attacks targeting large language models (LLMs). The framework uses a hybrid architecture with FAISS-indexed SBERT embedding representations that capture the semantic meaning of prompts, combined with fine-tuned transformer classifiers, which are machine learning models specialized for distinguishing between benign and adversarial language inputs. It identifies adversarial prompts in both direct and obfuscated attack vectors. A core innovation is the classifier-retriever fusion module, which dynamically computes context-aware risk scores that estimate how likely a prompt is to be adversarial based on its content and context. The framework ensures multilingual resilience with a language-agnostic preprocessing layer. This component automatically translates non-English prompts into English for semantic evaluation, enabling consistent detection across over 100 languages. The system includes a HITL feedback loop, where decisions made by the automated system are reviewed by human experts for continual learning and rapid adaptation under adversarial pressure. Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and malicious prompts, enhancing detection reliability and reducing false positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 = 1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible with diverse LLM backends. Its modular design supports scalable deployment in both commercial and open-source environments. The system establishes a new state-of-the-art in adversarial LLM defense.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Sentra-Guard：一种多语言人机框架，用于实时防御对抗性LLM越狱</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为Sentra-Guard的实时模块化防御系统。该系统检测并缓解针对大型语言模型（LLMs）的越狱和提示注入攻击。该框架采用混合架构，使用FAISS索引的SBERT嵌入表示来捕捉提示的语义意义，并结合经过微调的变换器分类器，这些机器学习模型专门用于区分良性和对抗性语言输入。它识别直接和模糊攻击向量中的对抗性提示。核心创新是分类器-检索器融合模块，该模块动态计算上下文感知风险评分，估计提示基于其内容和上下文的对抗性可能性。该框架通过语言无关的预处理层确保多语言的韧性。该组件自动将非英语提示翻译成英语以进行语义评估，从而在100多种语言中实现一致检测。该系统包括一个人机反馈循环，自动系统做出的决策由人类专家审查，以便在对抗压力下进行持续学习和快速适应。Sentra-Guard维护一个不断发展的良性和恶意提示的双标签知识库，提高检测可靠性并减少误报。评估结果显示检测率为99.96%（AUC = 1.00，F1 = 1.00），攻击成功率（ASR）仅为0.004%。这优于领先的基线，如LlamaGuard-2（1.3%）和OpenAI Moderation（3.7%）。与黑箱方法不同，Sentra-Guard是透明的、可微调的，并与多种LLM后端兼容。其模块化设计支持在商业和开源环境中的可扩展部署。该系统在对抗性LLM防御中建立了新的最先进水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the increasing threat of jailbreak and prompt injection attacks on large language models (LLMs), which have been inadequately mitigated by existing black-box defense methods. The proposed Sentra-Guard framework introduces a modular defense system that employs a hybrid architecture combining FAISS-indexed SBERT embeddings and fine-tuned transformer classifiers to effectively identify adversarial prompts. This approach is well-motivated as it enhances detection reliability through a classifier-retriever fusion module that computes context-aware risk scores, and it ensures multilingual support by translating prompts into English for semantic evaluation. The methodology includes a human-in-the-loop feedback mechanism for continuous learning and adaptation, resulting in a remarkable 99.96% detection rate and an attack success rate of only 0.004%, significantly outperforming leading baselines. Sentra-Guard&#x27;s transparent and fine-tunable design allows for scalable deployment across various LLM backends, establishing a new benchmark in adversarial defense.</div>
<div class="mono" style="margin-top:8px">本研究关注针对大型语言模型（LLM）的对抗性攻击，特别是越狱和提示注入攻击的日益严重的问题。以往的方法往往缺乏透明性和适应性，导致较高的误报率和在多语言环境中的有效性有限。提出的Sentra-Guard框架引入了一种混合架构，将FAISS索引的SBERT嵌入与微调的变换器分类器相结合，通过分类器-检索器融合模块计算上下文感知风险评分，从而增强对对抗性提示的检测。这种方法具有良好的动机，因为它确保了多语言的鲁棒性，并结合了人机反馈机制以实现持续改进。该方法展示了99.96%的检测率和仅0.004%的攻击成功率，显著优于现有系统如LlamaGuard-2和OpenAI Moderation，从而在对抗性LLM防御中建立了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks</div>
<div class="meta-line">Authors: Havva Alizadeh Noughabi, Julien Serbanescu, Fattane Zarrinkalam, Ali Dehghantanha</div>
<div class="meta-line">First: 2025-10-24T19:20:23+00:00 · Latest: 2025-10-24T19:20:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21983v1">Abs</a> · <a href="https://arxiv.org/pdf/2510.21983v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent advances, Large Language Models remain vulnerable to jailbreak attacks that bypass alignment safeguards and elicit harmful outputs. While prior research has proposed various attack strategies differing in human readability and transferability, little attention has been paid to the linguistic and psychological mechanisms that may influence a model&#x27;s susceptibility to such attacks. In this paper, we examine an interdisciplinary line of research that leverages foundational theories of persuasion from the social sciences to craft adversarial prompts capable of circumventing alignment constraints in LLMs. Drawing on well-established persuasive strategies, we hypothesize that LLMs, having been trained on large-scale human-generated text, may respond more compliantly to prompts with persuasive structures. Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive fingerprints that emerge in their jailbreak responses. Empirical evaluations across multiple aligned LLMs reveal that persuasion-aware prompts significantly bypass safeguards, demonstrating their potential to induce jailbreak behaviors. This work underscores the importance of cross-disciplinary insight in addressing the evolving challenges of LLM safety. The code and data are available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>揭示大型语言模型在越狱攻击中的说服性指纹</div>
<div class="mono" style="margin-top:8px">尽管最近取得了进展，大型语言模型仍然容易受到越狱攻击，这些攻击绕过了对齐保护措施并引发有害输出。虽然之前的研究提出了不同的人类可读性和可转移性的攻击策略，但对可能影响模型对这些攻击的易感性的语言和心理机制关注较少。本文考察了一条跨学科的研究路线，利用社会科学中的说服基础理论，设计能够绕过大型语言模型对齐约束的对抗性提示。基于成熟的说服策略，我们假设大型语言模型在接受大量人类生成文本的训练后，可能对具有说服结构的提示反应更顺从。此外，我们还研究了大型语言模型是否在其越狱响应中表现出独特的说服性指纹。对多个对齐大型语言模型的实证评估表明，关注说服的提示显著绕过了保护措施，展示了它们诱发越狱行为的潜力。这项工作强调了跨学科见解在应对大型语言模型安全不断演变的挑战中的重要性。代码和数据可用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of Large Language Models (LLMs) to jailbreak attacks that can bypass alignment safeguards and produce harmful outputs. Previous methods have focused on various attack strategies but often overlooked the linguistic and psychological factors influencing a model&#x27;s susceptibility. This paper proposes an innovative approach that utilizes foundational theories of persuasion from social sciences to create adversarial prompts designed to exploit these vulnerabilities. The methodology involves empirical evaluations of multiple aligned LLMs, revealing that prompts structured around persuasive techniques significantly enhance the likelihood of bypassing safety measures. The findings indicate that LLMs may exhibit unique persuasive fingerprints in their responses, highlighting the necessity of interdisciplinary approaches to improve LLM safety and demonstrating the effectiveness of persuasion-aware prompts in inducing jailbreak behaviors.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在越狱攻击中对对齐保护措施的脆弱性，这些攻击能够引发有害输出。以往的方法主要集中在各种攻击策略上，但往往忽视了影响模型易受攻击性的语言和心理因素。所提出的方法结合了社会科学中的说服理论，设计出能够绕过对齐约束的对抗性提示，从而解决了现有方法的局限性。研究方法包括对多个对齐LLM的实证评估，结果表明，受说服策略启发的提示能够有效引发越狱行为。研究结果表明，理解说服机制可以增强LLM的安全性，支持提高模型抵御此类攻击的目标。</div>
</details>
</div>
<div class="card">
<div class="title">RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</div>
<div class="meta-line">Authors: Xiuyuan Chen, Jian Zhao, Yuchen Yuan, Tianle Zhang, Huilin Zhou, Zheng Zhu, Ping Hu, Linghe Kong, Chi Zhang, Weiran Huang, Xuelong Li</div>
<div class="meta-line">First: 2025-09-28T09:35:32+00:00 · Latest: 2025-10-23T03:33:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25271v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.25271v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing safety evaluation methods for large language models (LLMs) suffer from inherent limitations, including evaluator bias and detection failures arising from model homogeneity, which collectively undermine the robustness of risk evaluation processes. This paper seeks to re-examine the risk evaluation paradigm by introducing a theoretical framework that reconstructs the underlying risk concept space. Specifically, we decompose the latent risk concept space into three mutually exclusive subspaces: the explicit risk subspace (encompassing direct violations of safety guidelines), the implicit risk subspace (capturing potential malicious content that requires contextual reasoning for identification), and the non-risk subspace. Furthermore, we propose RADAR, a multi-agent collaborative evaluation framework that leverages multi-round debate mechanisms through four specialized complementary roles and employs dynamic update mechanisms to achieve self-evolution of risk concept distributions. This approach enables comprehensive coverage of both explicit and implicit risks while mitigating evaluator bias. To validate the effectiveness of our framework, we construct an evaluation dataset comprising 800 challenging cases. Extensive experiments on our challenging testset and public benchmarks demonstrate that RADAR significantly outperforms baseline evaluation methods across multiple dimensions, including accuracy, stability, and self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87% improvement in risk identification accuracy compared to the strongest baseline evaluation method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RADAR：一种基于角色专门化协作的风险意识动态多智能体框架，用于大型语言模型的安全评估</div>
<div class="mono" style="margin-top:8px">现有的大型语言模型（LLMs）安全评估方法存在固有局限性，包括评估者偏见和由于模型同质性导致的检测失败，这些问题共同削弱了风险评估过程的稳健性。本文旨在通过引入一个理论框架重新审视风险评估范式，该框架重构了潜在风险概念空间。具体而言，我们将潜在风险概念空间分解为三个相互排斥的子空间：显性风险子空间（涵盖对安全指南的直接违反）、隐性风险子空间（捕捉需要上下文推理才能识别的潜在恶意内容）和非风险子空间。此外，我们提出了RADAR，一个多智能体协作评估框架，利用四个专门互补角色的多轮辩论机制，并采用动态更新机制实现风险概念分布的自我演化。这种方法能够全面覆盖显性和隐性风险，同时减轻评估者偏见。为了验证我们框架的有效性，我们构建了一个包含800个挑战性案例的评估数据集。在我们的挑战性测试集和公共基准上的广泛实验表明，RADAR在多个维度上显著优于基线评估方法，包括准确性、稳定性和自我评估风险敏感性。值得注意的是，RADAR在风险识别准确性上相比最强基线评估方法提高了28.87%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing safety evaluation methods for large language models (LLMs), which are hindered by evaluator bias and detection failures due to model homogeneity. Previous methods often lack a comprehensive understanding of risk, leading to inadequate evaluations. The proposed approach, RADAR, introduces a theoretical framework that redefines the risk concept space into three distinct subspaces and employs a multi-agent collaborative evaluation framework with specialized roles and dynamic updates to enhance risk assessment. This well-motivated methodology allows for better identification of both explicit and implicit risks while reducing bias. The paper contributes by demonstrating that RADAR significantly improves risk identification accuracy by 28.87% over the strongest baseline method through extensive experiments on a dataset of 800 challenging cases, thus supporting its goals of enhancing LLM safety evaluation.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有大型语言模型（LLMs）安全评估方法的局限性，这些方法受到评估者偏见和由于模型同质性导致的检测失败的影响。以往的方法往往无法充分捕捉风险评估的复杂性，导致鲁棒性不足。所提出的方法RADAR引入了一种多智能体协作评估框架，将风险概念空间分解为显性风险、隐性风险和非风险子空间，从而实现更细致的风险评估。该方法采用多轮辩论机制和专业角色来增强评估过程并减少偏见。本文的贡献在于开发了这一框架，并通过对800个案例的评估数据集进行实验验证，显示出在风险识别准确性上比现有方法提高了28.87%，从而支持全面风险评估的目标。</div>
</details>
</div>
<div class="card">
<div class="title">NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks</div>
<div class="meta-line">Authors: Javad Rafiei Asl, Sidhant Narula, Mohammad Ghasemigol, Eduardo Blanco, Daniel Takabi</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-10-03T18:24:14+00:00 · Latest: 2025-10-21T17:41:58+00:00</div>
<div class="meta-line">Comments: This paper has been accepted in the main conference proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025). Javad Rafiei Asl and Sidhant Narula are co-first authors</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03417v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.03417v2">PDF</a> · <a href="https://github.com/inspire-lab/NEXUS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have revolutionized natural language processing but remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks that distribute malicious intent across benign exchanges and bypass alignment mechanisms. Existing approaches often explore the adversarial space poorly, rely on hand-crafted heuristics, or lack systematic query refinement. We present NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular framework for constructing, refining, and executing optimized multi-turn attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a harmful intent into a structured semantic network of topics, entities, and query chains; (2) a feedback-driven Simulator that iteratively refines and prunes these chains through attacker-victim-judge LLM collaboration using harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser that adaptively navigates the refined query space for real-time attacks. This pipeline uncovers stealthy, high-success adversarial paths across LLMs. On several closed-source and open-source LLMs, NEXUS increases attack success rate by 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NEXUS：用于利用多轮 LLM 越狱中不安全序列的网络探索</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）已彻底改变自然语言处理，但仍然容易受到越狱攻击，尤其是将恶意意图分散在良性交流中的多轮越狱，绕过对齐机制。现有方法通常对对抗空间探索不足，依赖手工设计的启发式方法，或缺乏系统的查询优化。我们提出了 NEXUS（用于利用不安全序列的网络探索），这是一个构建、优化和执行多轮攻击的模块化框架。NEXUS 包括：（1）ThoughtNet，它将有害意图分层扩展为主题、实体和查询链的结构化语义网络；（2）一个基于反馈的模拟器，通过攻击者-受害者-评判者 LLM 协作，使用有害性和语义相似性基准迭代优化和修剪这些链；（3）一个网络遍历器，适应性地导航优化后的查询空间以进行实时攻击。该管道揭示了 LLM 中隐秘的高成功率对抗路径。在多个闭源和开源 LLM 上，NEXUS 的攻击成功率比之前的方法提高了 2.1% 到 19.4%。代码：https://github.com/inspire-lab/NEXUS</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of Large Language Models (LLMs) to jailbreak attacks, particularly multi-turn jailbreaks that can obscure malicious intent within benign interactions. Previous methods have struggled with inadequate exploration of the adversarial space, reliance on manual heuristics, and lack of systematic query refinement, leading to limited effectiveness in executing such attacks. The proposed NEXUS framework overcomes these issues by providing a modular approach that includes ThoughtNet for hierarchical intent expansion, a feedback-driven Simulator for iterative refinement, and a Network Traverser for adaptive navigation of query spaces. This methodology significantly enhances the ability to uncover effective adversarial paths, achieving an increase in attack success rates by 2.1% to 19.4% compared to existing techniques across various LLMs, thereby supporting the goal of improving attack efficacy.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLM）在监狱攻击中的脆弱性，特别是能够在良性互动中掩盖恶意意图的多轮监狱攻击。以往的方法在对抗空间的探索上存在不足，依赖手工设计的启发式方法，并缺乏系统的查询优化，而NEXUS旨在克服这些问题。该方法具有良好的动机，提供了一个模块化框架，包括ThoughtNet用于将有害意图扩展为结构化的语义网络、基于反馈的模拟器用于优化查询链，以及网络遍历器用于实时导航查询空间。该方法显著提高了对多种LLM的攻击成功率，相较于现有方法提高了2.1%至19.4%，从而支持了揭示LLM中有效对抗路径的目标。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Safety Alignment is Divergence Estimation in Disguise</div>
<div class="meta-line">Authors: Rajdeep Haldar, Ziyi Wang, Qifan Song, Guang Lin, Yue Xing</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-02-02T04:09:42+00:00 · Latest: 2025-10-20T19:47:59+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.00657v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.00657v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a theoretical framework showing that popular LLM alignment methods, including RLHF and its variants, can be understood as divergence estimators between aligned (safe or preferred) and unaligned (harmful or less preferred) distributions. This perspective explains the emergence of separation in the latent space between safe and harmful prompts after alignment. As an application of our general divergence framework, we propose KLDO, a novel KL divergence-based alignment method, and empirically validate its effectiveness. We further show that using compliance-refusal datasets, rather than standard preference-based datasets, leads to stronger separation and improved safety alignment. Finally, to quantify the separation effect, we propose a distance-based metric in the prompt representation space, which also acts as a statistically significant indicator for model safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM安全对齐是伪装的发散估计</div>
<div class="mono" style="margin-top:8px">我们提出了一个理论框架，表明流行的LLM对齐方法，包括RLHF及其变体，可以理解为对齐（安全或优选）和未对齐（有害或不太优选）分布之间的发散估计器。这一视角解释了对齐后安全和有害提示在潜在空间中出现分离的现象。作为我们一般发散框架的应用，我们提出了KLDO，一种基于KL发散的新型对齐方法，并实证验证了其有效性。我们进一步表明，使用合规-拒绝数据集而非标准偏好数据集，能够导致更强的分离和改善的安全对齐。最后，为了量化分离效果，我们提出了一种基于距离的度量，适用于提示表示空间，这也作为模型安全的统计显著性指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing large language model (LLM) alignment methods, such as Reinforcement Learning from Human Feedback (RLHF), which often fail to effectively separate safe and harmful prompts in the latent space. The proposed approach, KLDO, reinterprets these alignment methods as divergence estimators and introduces a novel KL divergence-based alignment method that improves upon traditional preference-based datasets by utilizing compliance-refusal datasets, resulting in stronger separation and enhanced safety alignment. The paper contributes a theoretical framework that clarifies the alignment process and introduces a distance-based metric for quantifying prompt representation separation, demonstrating its effectiveness through empirical validation. The methodology shows significant improvements in safety alignment, supporting the goal of creating more reliable LLMs.</div>
<div class="mono" style="margin-top:8px">本文探讨了在将大型语言模型（LLM）与人类偏好对齐时面临的挑战，特别关注现有方法（如基于人类反馈的强化学习（RLHF））在有效区分安全和有害输出方面的局限性。所提出的方法KLDO将这些对齐方法重新解释为散度估计器，提供了一个理论框架，增强了对对齐和未对齐分布之间潜在空间分离的理解。本文的贡献在于引入了一种新的基于KL散度的对齐方法，并使用合规-拒绝数据集，这显著改善了与传统偏好数据集相比的安全对齐。研究方法包括对KLDO有效性的实证验证，以及引入一种基于距离的度量来量化提示表示的分离，在安全对齐任务中取得了改进的性能，支持了增强模型安全性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Black-box Optimization of LLM Outputs by Asking for Directions</div>
<div class="meta-line">Authors: Jie Zhang, Meng Ding, Yang Liu, Jue Hong, Florian Tramèr</div>
<div class="meta-line">First: 2025-10-19T11:13:45+00:00 · Latest: 2025-10-19T11:13:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.16794v1">Abs</a> · <a href="https://arxiv.org/pdf/2510.16794v1">PDF</a> · <a href="https://github.com/zj-jayzhang/black_box_llm_optimization">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel approach for attacking black-box large language models (LLMs) by exploiting their ability to express confidence in natural language. Existing black-box attacks require either access to continuous model outputs like logits or confidence scores (which are rarely available in practice), or rely on proxy signals from other models. Instead, we demonstrate how to prompt LLMs to express their internal confidence in a way that is sufficiently calibrated to enable effective adversarial optimization. We apply our general method to three attack scenarios: adversarial examples for vision-LLMs, jailbreaks and prompt injections. Our attacks successfully generate malicious inputs against systems that only expose textual outputs, thereby dramatically expanding the attack surface for deployed LLMs. We further find that better and larger models exhibit superior calibration when expressing confidence, creating a concerning security paradox where model capability improvements directly enhance vulnerability. Our code is available at this [link](https://github.com/zj-jayzhang/black_box_llm_optimization).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过询问方向对LLM输出进行黑箱优化</div>
<div class="mono" style="margin-top:8px">我们提出了一种新颖的方法，通过利用黑箱大型语言模型（LLMs）在自然语言中表达信心的能力来进行攻击。现有的黑箱攻击要么需要访问连续的模型输出，如logits或置信度分数（在实践中很少可用），要么依赖于其他模型的代理信号。相反，我们展示了如何提示LLMs以一种足够校准的方式表达其内部信心，从而实现有效的对抗优化。我们将我们的一般方法应用于三种攻击场景：视觉LLMs的对抗样本、越狱和提示注入。我们的攻击成功生成了针对仅暴露文本输出的系统的恶意输入，从而显著扩大了已部署LLMs的攻击面。我们进一步发现，更好和更大的模型在表达信心时表现出更优的校准，形成了一个令人担忧的安全悖论，即模型能力的提升直接增强了脆弱性。我们的代码可在此[链接](https://github.com/zj-jayzhang/black_box_llm_optimization)获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of black-box attacks on large language models (LLMs), which traditionally require access to model outputs like logits or confidence scores that are often not available. Previous methods either depend on these inaccessible outputs or utilize proxy signals from other models, limiting their effectiveness. The proposed approach innovatively prompts LLMs to express their internal confidence in a calibrated manner, enabling effective adversarial optimization without needing direct access to model internals. This method contributes to the field by successfully generating adversarial examples, jailbreaks, and prompt injections against LLMs that only provide textual outputs, thereby broadening the potential attack surface. The methodology demonstrates that larger and more capable models exhibit better calibration in expressing confidence, which paradoxically increases their vulnerability, highlighting a significant security concern. The experiments show that the proposed attacks are effective across various scenarios, supporting the research goals of enhancing understanding of LLM security vulnerabilities.</div>
<div class="mono" style="margin-top:8px">本研究解决了对大型语言模型（LLMs）进行黑箱攻击的挑战，传统方法通常需要访问模型输出，如logits或置信度分数，而这些在实践中往往不可用。以往的方法要么依赖于这些不可获取的输出，要么利用其他模型的信号，限制了其有效性。所提出的方法创新性地提示LLMs以校准的方式表达其内部置信度，从而在不需要直接访问模型内部的情况下实现有效的对抗优化。这种方法具有良好的动机，因为它通过成功生成对抗示例、越狱和提示注入，即使在仅提供文本输出的情况下，也扩大了LLMs的攻击面。研究方法表明，较大和更强大的模型在表达置信度时表现出更好的校准，这在悖论中增加了它们的脆弱性。实验表明，所提出的攻击在各种场景中都有效，支持了增强对LLM安全漏洞理解的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Active Honeypot Guardrail System: Probing and Confirming Multi-Turn LLM Jailbreaks</div>
<div class="meta-line">Authors: ChenYu Wu, Yi Wang, Yang Liao</div>
<div class="meta-line">First: 2025-10-16T17:41:09+00:00 · Latest: 2025-10-16T17:41:09+00:00</div>
<div class="meta-line">Comments: 6pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.15017v1">Abs</a> · <a href="https://arxiv.org/pdf/2510.15017v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly vulnerable to multi-turn jailbreak attacks, where adversaries iteratively elicit harmful behaviors that bypass single-turn safety filters. Existing defenses predominantly rely on passive rejection, which either fails against adaptive attackers or overly restricts benign users. We propose a honeypot-based proactive guardrail system that transforms risk avoidance into risk utilization. Our framework fine-tunes a bait model to generate ambiguous, non-actionable but semantically relevant responses, which serve as lures to probe user intent. Combined with the protected LLM&#x27;s safe reply, the system inserts proactive bait questions that gradually expose malicious intent through multi-turn interactions. We further introduce the Honeypot Utility Score (HUS), measuring both the attractiveness and feasibility of bait responses, and use a Defense Efficacy Rate (DER) for balancing safety and usability. Initial experiment on MHJ Datasets with recent attack method across GPT-4o show that our system significantly disrupts jailbreak success while preserving benign user experience.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>主动蜜罐护栏系统：探测和确认多轮LLM越狱</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）越来越容易受到多轮越狱攻击，攻击者通过迭代引导有害行为，绕过单轮安全过滤器。现有防御主要依赖被动拒绝，这对适应性攻击者无效，或过度限制良性用户。我们提出了一种基于蜜罐的主动护栏系统，将风险规避转化为风险利用。我们的框架微调了一个诱饵模型，以生成模糊、不可操作但语义相关的响应，作为引诱用户意图的诱饵。结合受保护的LLM的安全回复，该系统插入主动诱饵问题，通过多轮交互逐步暴露恶意意图。我们进一步引入蜜罐效用评分（HUS），衡量诱饵响应的吸引力和可行性，并使用防御有效率（DER）来平衡安全性和可用性。在MHJ数据集上进行的初步实验显示，我们的系统显著干扰了越狱成功，同时保持了良性用户体验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the increasing vulnerability of large language models (LLMs) to multi-turn jailbreak attacks, which exploit the limitations of existing defenses that primarily rely on passive rejection methods. These traditional approaches often fail against adaptive attackers or impose excessive restrictions on legitimate users. The proposed honeypot-based proactive guardrail system shifts the focus from risk avoidance to risk utilization by fine-tuning a bait model that generates ambiguous yet relevant responses to probe user intent. This methodology includes the introduction of the Honeypot Utility Score (HUS) to evaluate bait responses and the Defense Efficacy Rate (DER) to balance safety and usability. Experimental results on MHJ Datasets demonstrate that the proposed system significantly reduces jailbreak success rates while maintaining a positive experience for benign users.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）在多轮越狱攻击中日益脆弱的问题，这些攻击利用现有防御措施的局限性，主要依赖被动拒绝，导致对适应性攻击者的保护不足或对合法用户的过度限制。提出的基于蜜罐的主动护栏系统与传统方法不同，它将风险规避转变为风险利用，利用诱饵模型生成模糊但相关的响应，以探测用户意图，同时保持安全性。这种方法的动机明确，旨在平衡安全性和可用性，贡献了一个新框架，包括蜜罐效用评分（HUS）和防御有效率（DER）来评估诱饵响应。该方法论涉及对诱饵模型的微调，并将其与受保护的LLM集成，以插入主动诱饵问题，证明在MHJ数据集上对最近攻击方法的性能显著改善，有效干扰越狱尝试，同时保持合法用户的良好体验。</div>
</details>
</div>
<div class="card">
<div class="title">When Style Breaks Safety: Defending LLMs Against Superficial Style Alignment</div>
<div class="meta-line">Authors: Yuxin Xiao, Sana Tonekaboni, Walter Gerych, Vinith Suriyakumar, Marzyeh Ghassemi</div>
<div class="meta-line">First: 2025-06-09T05:57:39+00:00 · Latest: 2025-10-16T06:50:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.07452v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.07452v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) can be prompted with specific styles (e.g., formatting responses as lists), including in malicious queries. Prior jailbreak research mainly augments these queries with additional string transformations to maximize attack success rate (ASR). However, the impact of style patterns in the original queries that are semantically irrelevant to the malicious intent remains unclear. In this work, we seek to understand whether style patterns compromise LLM safety, how superficial style alignment increases model vulnerability, and how best to mitigate these risks during alignment. We first define ASR inflation as the increase in ASR due to style patterns in existing jailbreak benchmark queries. By evaluating 32 LLMs across seven benchmarks, we find that nearly all models exhibit ASR inflation. Notably, the inflation correlates with an LLM&#x27;s relative attention to style patterns, which also overlap more with its instruction-tuning data when inflation occurs. We then investigate superficial style alignment, and find that fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of those same styles. Finally, we propose SafeStyle, a defense strategy that incorporates a small amount of safety training data augmented to match the distribution of style patterns in the fine-tuning data. Across three LLMs, six fine-tuning style settings, and two real-world instruction-tuning datasets, SafeStyle consistently outperforms baselines in maintaining LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当风格破坏安全性：保护大型语言模型免受表面风格对齐的影响</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）可以通过特定风格（例如，将响应格式化为列表）进行提示，包括在恶意查询中。之前的越狱研究主要通过额外的字符串转换来增强这些查询，以最大化攻击成功率（ASR）。然而，原始查询中与恶意意图语义无关的风格模式的影响仍不清楚。在本研究中，我们旨在了解风格模式是否会危害LLM安全性，表面风格对齐如何增加模型脆弱性，以及在对齐过程中如何最好地减轻这些风险。我们首先将ASR膨胀定义为由于现有越狱基准查询中的风格模式而导致的ASR增加。通过评估32个LLM在七个基准上的表现，我们发现几乎所有模型都表现出ASR膨胀。值得注意的是，膨胀与LLM对风格模式的相对关注度相关，当膨胀发生时，这些模式与其指令调优数据的重叠也更多。然后，我们研究表面风格对齐，发现使用特定风格进行微调使LLM更容易受到这些相同风格的越狱攻击。最后，我们提出了SafeStyle，一种防御策略，结合少量安全训练数据，增强以匹配微调数据中的风格模式分布。在三个LLM、六个微调风格设置和两个真实世界的指令调优数据集上，SafeStyle在维护LLM安全性方面始终优于基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of large language models (LLMs) to malicious queries that exploit specific stylistic patterns, a concern that has been inadequately explored in prior jailbreak studies which primarily focused on string transformations to enhance attack success rates (ASR). The existing methods fail to consider how superficial style alignment can inadvertently increase model susceptibility to attacks. This paper contributes by defining ASR inflation and demonstrating its prevalence across 32 LLMs, revealing a correlation between ASR inflation and the models&#x27; attention to style patterns. The proposed methodology, SafeStyle, involves augmenting safety training data to align with the stylistic patterns present in fine-tuning data, effectively mitigating the identified vulnerabilities. The results show that SafeStyle consistently outperforms baseline methods in maintaining LLM safety across various fine-tuning settings and instruction-tuning datasets, supporting the goal of enhancing model robustness against style-based attacks.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在恶意查询中利用特定风格模式的脆弱性，这是以往越狱研究未充分探讨的领域，后者主要集中在通过字符串变换来提高攻击成功率（ASR）。现有方法未考虑与恶意意图无关的风格模式的影响，导致对这些表面风格对齐如何危害LLM安全的理解存在空白。本文的贡献在于定义了ASR膨胀，并展示其在32个LLM中的普遍存在，揭示了ASR膨胀与模型对风格模式的关注之间的相关性。提出的方法SafeStyle通过增强与微调数据的风格分布相匹配的安全训练数据，有效减轻了识别出的脆弱性。结果表明，SafeStyle在各种微调设置和指令微调数据集上始终优于基线方法，支持了增强模型对基于风格攻击的鲁棒性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">When &quot;Competency&quot; in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers</div>
<div class="meta-line">Authors: Divij Handa, Zehua Zhang, Amir Saeidi, Shrinidhi Kumbhar, Md Nayem Uddin, Aswin RRV, Chitta Baral</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2024-02-16T11:37:05+00:00 · Latest: 2025-10-14T06:25:21+00:00</div>
<div class="meta-line">Comments: Published in Reliable ML from Unreliable Data workshop @ NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.10601v5">Abs</a> · <a href="https://arxiv.org/pdf/2402.10601v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Language Model (LLM) safety have primarily focused on mitigating attacks crafted in natural language or common ciphers (e.g. Base64), which are likely integrated into newer models&#x27; safety training. However, we reveal a paradoxical vulnerability: as LLMs advance in reasoning, they inadvertently become more susceptible to novel jailbreaking attacks. Enhanced reasoning enables LLMs to interpret complex instructions and decode complex user-defined ciphers, creating an exploitable security gap. To study this vulnerability, we introduce Attacks using Custom Encryptions (ACE), a jailbreaking technique that encodes malicious queries with novel ciphers. Extending ACE, we introduce Layered Attacks using Custom Encryptions (LACE), which applies multi-layer ciphers to amplify attack complexity. Furthermore, we develop CipherBench, a benchmark designed to evaluate LLMs&#x27; accuracy in decoding encrypted benign text. Our experiments reveal a critical trade-off: LLMs that are more capable of decoding ciphers are more vulnerable to LACE, with success rates on gpt-oss-20b escalating from 60% under ACE to 72% with LACE. These findings highlight a critical insight: as LLMs become more adept at deciphering complex user ciphers--many of which cannot be preemptively included in safety training--they become increasingly exploitable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当推理中的“能力”打开脆弱性之门：通过新型复杂密码破解大型语言模型</div>
<div class="mono" style="margin-top:8px">最近在大型语言模型（LLM）安全性方面的进展主要集中在减轻使用自然语言或常见密码（例如Base64）构建的攻击，这些攻击可能已被纳入新模型的安全训练中。然而，我们揭示了一种矛盾的脆弱性：随着LLM在推理方面的进步，它们无意中变得更容易受到新型破解攻击的影响。增强的推理能力使LLM能够解释复杂指令并解码复杂的用户定义密码，从而产生可利用的安全漏洞。为了研究这种脆弱性，我们引入了使用自定义加密的攻击（ACE），这是一种通过新型密码编码恶意查询的破解技术。扩展ACE，我们引入了使用自定义加密的分层攻击（LACE），它应用多层密码以增强攻击复杂性。此外，我们开发了CipherBench，这是一个旨在评估LLM在解码加密良性文本时准确性的基准测试。我们的实验揭示了一个关键的权衡：能够解码密码的LLM在LACE下更脆弱，gpt-oss-20b的成功率从ACE下的60%上升到LACE下的72%。这些发现突显了一个关键的见解：随着LLM在解读复杂用户密码方面变得更加熟练——其中许多无法在安全训练中预先包含——它们变得越来越容易被利用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the paradoxical vulnerability of Large Language Models (LLMs) that arises as they improve in reasoning capabilities, making them more susceptible to novel jailbreaking attacks. Previous methods focused on mitigating attacks using natural language or common ciphers, which are often included in safety training, but these approaches fail to account for the complexities introduced by advanced reasoning. The proposed method, Attacks using Custom Encryptions (ACE), and its extension, Layered Attacks using Custom Encryptions (LACE), utilize multi-layer ciphers to exploit this vulnerability, demonstrating a significant increase in attack success rates. The contribution of this paper lies in the introduction of CipherBench, a benchmark for evaluating LLMs&#x27; performance in decoding encrypted benign text. Experimental results show that success rates for LACE attacks on gpt-oss-20b increase from 60% to 72%, indicating that as LLMs enhance their cipher-decoding abilities, they become more vulnerable, thus supporting the research&#x27;s goals.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在其推理能力增强时出现的新漏洞，这使得它们在无意中更容易受到新型越狱攻击的影响。以往的方法主要集中在减轻来自自然语言或常见密码的威胁，但这些方法未能考虑到高级推理带来的复杂性。提出的方法，定制加密攻击（ACE）及其扩展层叠定制加密攻击（LACE），利用多层密码来利用这一漏洞。该研究的贡献在于开发了CipherBench，一个用于评估LLMs在解码加密良性文本方面表现的基准。实验结果表明，脆弱性显著增加，使用LACE时在gpt-oss-20b上的成功率从60%上升到72%，这表明随着LLMs提高其解码能力，它们变得更加容易受到攻击。</div>
</details>
</div>
<div class="card">
<div class="title">Attention-Aware GNN-based Input Defense against Multi-Turn LLM Jailbreak</div>
<div class="meta-line">Authors: Zixuan Huang, Kecheng Huang, Lihao Yin, Bowei He, Huiling Zhen, Mingxuan Yuan, Zili Shao</div>
<div class="meta-line">First: 2025-07-09T07:55:03+00:00 · Latest: 2025-10-14T04:28:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.07146v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.07146v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have gained significant traction in various applications, yet their capabilities present risks for both constructive and malicious exploitation. Despite extensive training and fine-tuning efforts aimed at enhancing safety, LLMs remain susceptible to jailbreak attacks. Recently, the emergence of multi-turn attacks has intensified this vulnerability. Unlike single-turn attacks, multi-turn attacks incrementally escalate dialogue complexity, rendering them more challenging to detect and mitigate.
  In this study, we introduce G-Guard, an innovative attention-aware Graph Neural Network (GNN)-based input classifier specifically designed to defend against multi-turn jailbreak attacks targeting LLMs. G-Guard constructs an entity graph for multi-turn queries, which captures the interrelationships between queries and harmful keywords that present in multi-turn queries. Furthermore, we propose an attention-aware augmentation mechanism that retrieves the most relevant single-turn query based on the ongoing multi-turn conversation. The retrieved query is incorporated as a labeled node within the graph, thereby enhancing the GNN&#x27;s capacity to classify the current query as harmful or benign. Evaluation results show that G-Guard consistently outperforms all baselines across diverse datasets and evaluation metrics, demonstrating its efficacy as a robust defense mechanism against multi-turn jailbreak attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于注意力的GNN输入防御多轮LLM越狱攻击</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在各种应用中获得了显著关注，但其能力也带来了建设性和恶意利用的风险。尽管进行了广泛的训练和微调以增强安全性，LLMs仍然容易受到越狱攻击。最近，多轮攻击的出现加剧了这种脆弱性。与单轮攻击不同，多轮攻击逐步增加对话复杂性，使其更难以检测和缓解。
  在本研究中，我们介绍了G-Guard，一种创新的基于注意力的图神经网络（GNN）输入分类器，专门设计用于防御针对LLMs的多轮越狱攻击。G-Guard为多轮查询构建实体图，捕捉查询与多轮查询中存在的有害关键词之间的相互关系。此外，我们提出了一种注意力感知的增强机制，根据正在进行的多轮对话检索最相关的单轮查询。检索到的查询作为标记节点纳入图中，从而增强GNN将当前查询分类为有害或良性的能力。评估结果表明，G-Guard在不同数据集和评估指标上始终优于所有基线，证明其作为多轮越狱攻击的强大防御机制的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the increasing vulnerability of Large Language Models (LLMs) to jailbreak attacks, particularly the more complex multi-turn attacks that escalate dialogue difficulty and are harder to detect. Previous methods have struggled to effectively mitigate these attacks due to their inability to capture the evolving context of multi-turn interactions. The proposed approach, G-Guard, utilizes an attention-aware Graph Neural Network (GNN) to create an entity graph that represents the relationships between queries and harmful keywords, along with an attention-aware augmentation mechanism to enhance classification accuracy. This method is well-motivated as it directly targets the shortcomings of existing defenses by improving context awareness. The paper contributes a novel defense mechanism that significantly outperforms baseline models across various datasets and metrics, demonstrating its effectiveness in classifying multi-turn queries as harmful or benign.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）在监狱突破攻击中的脆弱性，特别是更复杂的多轮攻击，这些攻击比单轮攻击更难以检测。以往的方法由于无法有效捕捉多轮对话的演变上下文而面临挑战。所提出的方法G-Guard利用一种注意力感知的图神经网络（GNN），构建一个实体图来表示查询与有害关键词之间的关系，并结合一种注意力感知的增强机制来提高分类准确性。该方法的动机明确，直接针对现有防御的不足。本文的贡献在于证明G-Guard在多轮查询分类中显著优于现有基线，从而在各种数据集和评估指标上为LLMs提供了强有力的监狱突破攻击防御。</div>
</details>
</div>
<div class="card">
<div class="title">The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections</div>
<div class="meta-line">Authors: Milad Nasr, Nicholas Carlini, Chawin Sitawarin, Sander V. Schulhoff, Jamie Hayes, Michael Ilie, Juliette Pluto, Shuang Song, Harsh Chaudhari, Ilia Shumailov, Abhradeep Thakurta, Kai Yuanqing Xiao, Andreas Terzis, Florian Tramèr</div>
<div class="meta-line">First: 2025-10-10T05:51:04+00:00 · Latest: 2025-10-10T05:51:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.09023v1">Abs</a> · <a href="https://arxiv.org/pdf/2510.09023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How should we evaluate the robustness of language model defenses? Current defenses against jailbreaks and prompt injections (which aim to prevent an attacker from eliciting harmful knowledge or remotely triggering malicious actions, respectively) are typically evaluated either against a static set of harmful attack strings, or against computationally weak optimization methods that were not designed with the defense in mind. We argue that this evaluation process is flawed.
  Instead, we should evaluate defenses against adaptive attackers who explicitly modify their attack strategy to counter a defense&#x27;s design while spending considerable resources to optimize their objective. By systematically tuning and scaling general optimization techniques-gradient descent, reinforcement learning, random search, and human-guided exploration-we bypass 12 recent defenses (based on a diverse set of techniques) with attack success rate above 90% for most; importantly, the majority of defenses originally reported near-zero attack success rates. We believe that future defense work must consider stronger attacks, such as the ones we describe, in order to make reliable and convincing claims of robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>攻击者第二步行动：更强的自适应攻击绕过针对 LLM 监狱突破和提示注入的防御</div>
<div class="mono" style="margin-top:8px">我们应该如何评估语言模型防御的稳健性？当前针对监狱突破和提示注入的防御（旨在防止攻击者引发有害知识或远程触发恶意行为）通常是针对一组静态的有害攻击字符串，或针对未考虑防御设计的计算弱优化方法进行评估。我们认为这一评估过程存在缺陷。相反，我们应该针对自适应攻击者进行评估，他们明确修改攻击策略以对抗防御设计，同时花费大量资源来优化目标。通过系统地调整和扩展一般优化技术——梯度下降、强化学习、随机搜索和人类引导探索——我们绕过了 12 种最近的防御（基于多样化的技术集），大多数攻击成功率超过 90%；重要的是，大多数防御最初报告的攻击成功率接近零。我们相信，未来的防御工作必须考虑更强的攻击，例如我们所描述的，以便做出可靠和令人信服的稳健性声明。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the evaluation of language model defenses against jailbreaks and prompt injections, which are strategies used by attackers to extract harmful information or trigger malicious actions. Previous methods for evaluating these defenses relied on static attack strings or weak optimization techniques that did not account for the adaptive nature of attackers. The proposed approach emphasizes the need to evaluate defenses against adaptive attackers who can modify their strategies, utilizing advanced optimization techniques such as gradient descent and reinforcement learning. The paper contributes by demonstrating that 12 recent defenses can be bypassed with over 90% success using these adaptive methods, highlighting the inadequacy of current evaluation practices and advocating for a shift towards stronger attack scenarios in future defense research.</div>
<div class="mono" style="margin-top:8px">本研究针对评估语言模型防御措施在抵御越狱和提示注入方面的不足进行了探讨，这些措施对于防止有害行为和知识引导至关重要。以往的方法依赖于静态攻击字符串或弱优化技术，未能考虑自适应策略，导致对防御有效性的误导性评估。本文提出了一种新的评估框架，专注于能够调整策略以利用防御弱点的自适应攻击者，采用了梯度下降和强化学习等先进的优化方法。研究的贡献在于证明了12种近期防御措施的攻击成功率超过90%，突显了未来防御措施必须考虑更强的自适应攻击，以确保真正的鲁棒性声明。该方法有效揭示了现有防御的脆弱性，强调了对复杂攻击策略进行严格评估的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Machine Learning for Detection and Analysis of Novel LLM Jailbreaks</div>
<div class="meta-line">Authors: John Hawkins, Aditya Pramar, Rodney Beard, Rohitash Chandra</div>
<div class="meta-line">First: 2025-10-02T03:55:29+00:00 · Latest: 2025-10-10T05:08:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01644v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.01644v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) suffer from a range of vulnerabilities that allow malicious users to solicit undesirable responses through manipulation of the input text. These so-called jailbreak prompts are designed to trick the LLM into circumventing the safety guardrails put in place to keep responses acceptable to the developer&#x27;s policies. In this study, we analyse the ability of different machine learning models to distinguish jailbreak prompts from genuine uses, including looking at our ability to identify jailbreaks that use previously unseen strategies. Our results indicate that using current datasets the best performance is achieved by fine tuning a Bidirectional Encoder Representations from Transformers (BERT) model end-to-end for identifying jailbreaks. We visualise the keywords that distinguish jailbreak from genuine prompts and conclude that explicit reflexivity in prompt structure could be a signal of jailbreak intention.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于检测和分析新型LLM越狱的机器学习</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）存在一系列漏洞，使恶意用户能够通过操纵输入文本来获取不良响应。这些所谓的越狱提示旨在欺骗LLM绕过为保持响应符合开发者政策而设立的安全防护措施。在本研究中，我们分析了不同机器学习模型区分越狱提示与真实使用的能力，包括识别使用以前未见策略的越狱。我们的结果表明，使用当前数据集，通过端到端微调双向编码器表示（BERT）模型来识别越狱可以获得最佳性能。我们可视化了区分越狱与真实提示的关键词，并得出结论，提示结构中的显性反身性可能是越狱意图的信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of Large Language Models (LLMs) that can be exploited by malicious users through specially crafted inputs known as jailbreak prompts, which bypass safety measures. Previous methods struggled to effectively identify these prompts, particularly those employing novel strategies, leading to a need for improved detection techniques. This study proposes a fine-tuned Bidirectional Encoder Representations from Transformers (BERT) model, which enhances the ability to distinguish between jailbreak and genuine prompts by analyzing their structural characteristics. The methodology involves training the BERT model end-to-end on current datasets, achieving superior performance in identifying jailbreaks. The findings demonstrate that the proposed approach not only improves detection rates but also provides insights into the linguistic features indicative of jailbreak intentions, supporting the goal of enhancing LLM safety mechanisms.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）的脆弱性，这些脆弱性可以被恶意用户通过特制的输入提示（称为越狱提示）利用，从而绕过安全措施。以往的方法在有效区分越狱提示和合法使用方面存在困难，尤其是在面对新策略时。本研究提出了一种新方法，通过端到端微调双向编码器表示（BERT）模型，显著提高了越狱检测的准确性。该方法涉及分析各种机器学习模型，以识别越狱提示的特征，得出某些提示结构线索可能表明恶意意图的结论。所提出的方法在识别越狱方面表现优异，支持了增强LLM安全性和可靠性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression</div>
<div class="meta-line">Authors: Jingyu Peng, Maolin Wang, Nan Wang, Jiatong Li, Yuchen Li, Yuyang Ye, Wanyu Wang, Pengyue Jia, Kai Zhang, Xiangyu Zhao</div>
<div class="meta-line">First: 2025-05-18T04:23:51+00:00 · Latest: 2025-10-09T06:29:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.13527v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.13527v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite substantial advancements in aligning large language models (LLMs) with human values, current safety mechanisms remain susceptible to jailbreak attacks. We hypothesize that this vulnerability stems from distributional discrepancies between alignment-oriented prompts and malicious prompts. To investigate this, we introduce LogiBreak, a novel and universal black-box jailbreak method that leverages logical expression translation to circumvent LLM safety systems. By converting harmful natural language prompts into formal logical expressions, LogiBreak exploits the distributional gap between alignment data and logic-based inputs, preserving the underlying semantic intent and readability while evading safety constraints. We evaluate LogiBreak on a multilingual jailbreak dataset spanning three languages, demonstrating its effectiveness across various evaluation settings and linguistic contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>逻辑越狱：通过形式逻辑表达高效解锁大型语言模型安全限制</div>
<div class="mono" style="margin-top:8px">尽管在将大型语言模型（LLMs）与人类价值观对齐方面取得了重大进展，但当前的安全机制仍然容易受到越狱攻击。我们假设这种脆弱性源于对齐导向提示与恶意提示之间的分布差异。为此，我们引入了LogiBreak，这是一种新颖且通用的黑箱越狱方法，利用逻辑表达翻译来规避LLM安全系统。通过将有害的自然语言提示转换为形式逻辑表达，LogiBreak利用对齐数据与基于逻辑的输入之间的分布差距，保持潜在的语义意图和可读性，同时规避安全约束。我们在涵盖三种语言的多语言越狱数据集上评估LogiBreak，展示了其在各种评估设置和语言环境中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of large language models (LLMs) to jailbreak attacks despite advancements in aligning these models with human values. Previous methods have struggled with the distributional discrepancies between alignment-oriented and malicious prompts, leading to ineffective safety mechanisms. The proposed approach, LogiBreak, introduces a novel black-box jailbreak method that translates harmful natural language prompts into formal logical expressions, effectively exploiting the distributional gap while maintaining semantic intent and readability. This method is well-motivated as it directly targets the identified weaknesses in existing safety systems. The contribution of the paper lies in demonstrating LogiBreak&#x27;s effectiveness on a multilingual jailbreak dataset across three languages, achieving significant performance improvements in various evaluation settings and linguistic contexts, thus supporting the goal of enhancing LLM safety mechanisms.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）在面对越狱攻击时的脆弱性问题，这种脆弱性源于对齐导向提示与恶意提示之间的差异。以往的方法在应对这些脆弱性时效果不佳，往往无法有效弥合安全输入与有害输入之间的差距。提出的方法LogiBreak引入了一种新颖的黑箱越狱方法，通过将有害的自然语言提示转换为形式逻辑表达，从而规避安全机制，同时保持语义意图。该方法的提出是有充分动机的，因为它直接针对已识别的分布差异。论文的贡献在于展示LogiBreak在跨越三种语言的多语言越狱数据集上的有效性，在各种评估设置中取得显著的性能提升，支持了增强LLM安全性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">$\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks</div>
<div class="meta-line">Authors: Rana Muhammad Shahroz Khan, Zhen Tan, Sukwon Yun, Charles Fleming, Tianlong Chen</div>
<div class="meta-line">First: 2025-03-31T20:43:56+00:00 · Latest: 2025-10-08T22:17:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.00218v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.00218v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most discussions about Large Language Model (LLM) safety have focused on single-agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking pragmatic systems that have constrains such as limited token bandwidth, latency between message delivery, and defense mechanisms. We design a $\textit{permutation-invariant adversarial attack}$ that optimizes prompt distribution across latency and bandwidth-constraint network topologies to bypass distributed safety mechanisms within the system. Formulating the attack path as a problem of $\textit{maximum-flow minimum-cost}$, coupled with the novel $\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage graph-based optimization to maximize attack success rate while minimizing detection risk. Evaluating across models including $\texttt{Llama}$, $\texttt{Mistral}$, $\texttt{Gemma}$, $\texttt{DeepSeek}$ and other variants on various datasets like $\texttt{JailBreakBench}$ and $\texttt{AdversarialBench}$, our method outperforms conventional attacks by up to $7\times$, exposing critical vulnerabilities in multi-agent systems. Moreover, we demonstrate that existing defenses, including variants of $\texttt{Llama-Guard}$ and $\texttt{PromptGuard}$, fail to prohibit our attack, emphasizing the urgent need for multi-agent specific safety mechanisms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>《被围攻的智能体》：通过优化提示攻击打破务实的多智能体大语言模型系统</div>
<div class="mono" style="margin-top:8px">关于大语言模型（LLM）安全性的讨论大多集中在单智能体设置上，但多智能体LLM系统现在带来了新的对抗风险，因为它们的行为依赖于智能体之间的通信和分散推理。在这项工作中，我们创新性地关注于攻击具有限制的务实系统，例如有限的令牌带宽、消息传递延迟和防御机制。我们设计了一种“置换不变对抗攻击”，优化延迟和带宽受限网络拓扑中的提示分布，以绕过系统内的分布式安全机制。将攻击路径表述为“最大流最小成本”问题，并结合新颖的“置换不变规避损失（PIEL）”，我们利用基于图的优化来最大化攻击成功率，同时最小化检测风险。在包括“Llama”、“Mistral”、“Gemma”、“DeepSeek”等模型以及“JailBreakBench”和“AdversarialBench”等各种数据集上的评估中，我们的方法比传统攻击提高了多达7倍的效果，暴露了多智能体系统中的关键漏洞。此外，我们证明了现有的防御措施，包括“Llama-Guard”和“PromptGuard”的变体，无法阻止我们的攻击，强调了对多智能体特定安全机制的迫切需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the emerging adversarial risks associated with multi-agent Large Language Model (LLM) systems, which have been largely overlooked in previous safety discussions that focused on single-agent settings. Past methods have not adequately considered the unique challenges posed by communication and decentralized reasoning among agents, leading to vulnerabilities in these systems. The proposed approach introduces a permutation-invariant adversarial attack that optimizes prompt distribution within bandwidth and latency constraints, effectively bypassing existing safety mechanisms. This method is motivated by the need to enhance security in multi-agent environments and employs a maximum-flow minimum-cost formulation along with a novel Permutation-Invariant Evasion Loss (PIEL) to maximize attack success while minimizing detection risk. The evaluation of this methodology across various models, including Llama and Mistral, demonstrates performance improvements of up to seven times compared to conventional attacks, revealing significant weaknesses in multi-agent systems and highlighting the inadequacy of current defense strategies.</div>
<div class="mono" style="margin-top:8px">本文探讨了与多智能体大型语言模型（LLM）系统相关的新兴对抗风险，而以往的研究主要集中在单智能体设置上，未能充分覆盖这些风险。现有方法在应对多智能体交互所带来的独特挑战（如通信限制和分散推理）时存在不足，导致可被利用的漏洞。作者提出了一种新颖的置换不变对抗攻击，优化提示分布以有效应对这些限制，采用最大流最小成本的形式化方法和新的置换不变规避损失（PIEL），以提高攻击成功率并降低检测风险。该方法在多个模型（如Llama和Mistral）及多个数据集（如JailBreakBench和AdversarialBench）上进行评估，表现出比传统攻击高出七倍的性能提升，揭示了现有多智能体安全防御的关键弱点。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models</div>
<div class="meta-line">Authors: Haoran Ye, Tianze Zhang, Yuhang Xie, Liyuan Zhang, Yuanyi Ren, Xin Zhang, Guojie Song</div>
<div class="meta-line">Venue: ACL 2025</div>
<div class="meta-line">First: 2025-02-04T16:10:55+00:00 · Latest: 2025-10-07T14:57:19+00:00</div>
<div class="meta-line">Comments: ACL 2025 Main</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.02444v6">Abs</a> · <a href="https://arxiv.org/pdf/2502.02444v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz&#x27;s Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross-disciplinary investigations into decision-making and societal dynamics. Recently, the rise of Large Language Models (LLMs) has raised concerns regarding their elusive intrinsic values. Despite growing efforts in evaluating, understanding, and aligning LLM values, a psychologically grounded LLM value system remains underexplored. This study addresses the gap by introducing the Generative Psycho-Lexical Approach (GPLA), a scalable, adaptable, and theoretically informed method for constructing value systems. Leveraging GPLA, we propose a psychologically grounded five-factor value system tailored for LLMs. For systematic validation, we present three benchmarking tasks that integrate psychological principles with cutting-edge AI priorities. Our results reveal that the proposed value system meets standard psychological criteria, better captures LLM values, improves LLM safety prediction, and enhances LLM alignment, when compared to the canonical Schwartz&#x27;s values.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成心理词汇方法构建大型语言模型的价值体系</div>
<div class="mono" style="margin-top:8px">价值观是个体和集体感知、认知和行为的核心驱动力。价值体系，如施瓦茨的基本人类价值理论，描绘了这些价值观之间的层次和相互作用，使跨学科研究决策和社会动态成为可能。最近，大型语言模型（LLMs）的兴起引发了对其难以捉摸的内在价值的担忧。尽管在评估、理解和对齐LLM价值方面的努力不断增加，但基于心理学的LLM价值体系仍然未得到充分探索。本研究通过引入生成心理词汇方法（GPLA）来填补这一空白，GPLA是一种可扩展、可适应且理论上有依据的构建价值体系的方法。利用GPLA，我们提出了一个针对LLM的基于心理学的五因素价值体系。为了进行系统验证，我们提出了三个基准任务，将心理学原理与前沿AI优先事项相结合。我们的结果表明，所提出的价值体系符合标准心理学标准，更好地捕捉LLM价值，提高LLM安全预测，并增强LLM对齐，相较于经典的施瓦茨价值观。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for a psychologically informed value system in Large Language Models (LLMs), as existing methods have not adequately captured the intrinsic values of these models. Previous approaches, primarily based on Schwartz&#x27;s Theory of Basic Human Values, lack a scalable and adaptable framework for integrating psychological principles into LLMs. The proposed Generative Psycho-Lexical Approach (GPLA) offers a novel methodology that constructs a five-factor value system specifically designed for LLMs, addressing the limitations of prior methods. The paper contributes by validating this value system through three benchmarking tasks, demonstrating that it aligns better with LLM values, enhances safety predictions, and improves overall alignment compared to traditional frameworks. The findings indicate that the GPLA effectively supports the goal of creating a more psychologically grounded understanding of LLM values.</div>
<div class="mono" style="margin-top:8px">本文探讨了在大型语言模型（LLMs）中建立心理学基础的价值体系的必要性，因为现有方法未能充分探索这些模型的内在价值。传统方法，如施瓦茨的基本人类价值理论，缺乏对LLMs的适应性和可扩展性，导致与其操作价值的对齐不足。提出的生成心理词汇方法（GPLA）提供了一种新颖的、理论上有依据的框架，专门为LLMs构建五因素价值体系，有效解决了以前方法的局限性。研究方法通过三项基准任务进行系统验证，这些任务结合了心理学原则和人工智能优先事项。研究结果表明，基于GPLA的价值体系不仅符合既定的心理学标准，而且在安全预测和对齐方面显著优于传统价值体系。</div>
</details>
</div>
<div class="card">
<div class="title">Sampling-aware Adversarial Attacks Against Large Language Models</div>
<div class="meta-line">Authors: Tim Beyer, Yan Scholten, Leo Schwinn, Stephan Günnemann</div>
<div class="meta-line">First: 2025-07-06T16:13:33+00:00 · Latest: 2025-10-06T09:02:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.04446v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.04446v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To guarantee safe and robust deployment of large language models (LLMs) at scale, it is critical to accurately assess their adversarial robustness. Existing adversarial attacks typically target harmful responses in single-point greedy generations, overlooking the inherently stochastic nature of LLMs and overestimating robustness. We show that for the goal of eliciting harmful responses, repeated sampling of model outputs during the attack complements prompt optimization and serves as a strong and efficient attack vector. By casting attacks as a resource allocation problem between optimization and sampling, we determine compute-optimal trade-offs and show that integrating sampling into existing attacks boosts success rates by up to 37\% and improves efficiency by up to two orders of magnitude. We further analyze how distributions of output harmfulness evolve during an adversarial attack, discovering that many common optimization strategies have little effect on output harmfulness. Finally, we introduce a label-free proof-of-concept objective based on entropy maximization, demonstrating how our sampling-aware perspective enables new optimization targets. Overall, our findings establish the importance of sampling in attacks to accurately assess and strengthen LLM safety at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对大型语言模型的采样感知对抗攻击</div>
<div class="mono" style="margin-top:8px">为了确保大型语言模型（LLMs）在大规模部署中的安全性和鲁棒性，准确评估其对抗鲁棒性至关重要。现有的对抗攻击通常针对单点贪婪生成中的有害响应，忽视了LLMs固有的随机性，并高估了鲁棒性。我们表明，为了引发有害响应，在攻击过程中对模型输出进行重复采样可以补充提示优化，并作为一种强大而高效的攻击向量。通过将攻击视为优化与采样之间的资源分配问题，我们确定了计算最优的权衡，并表明将采样整合到现有攻击中可以将成功率提高多达37\%，并将效率提高两个数量级。我们进一步分析了在对抗攻击过程中输出有害性分布的演变，发现许多常见的优化策略对输出有害性几乎没有影响。最后，我们引入了一种基于熵最大化的无标签概念验证目标，展示了我们的采样感知视角如何启用新的优化目标。总体而言，我们的研究结果确立了在攻击中采样的重要性，以准确评估和增强LLM在大规模下的安全性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical need for assessing the adversarial robustness of large language models (LLMs) to ensure their safe deployment. Previous methods primarily focused on single-point greedy generations, which fail to account for the stochastic nature of LLMs, leading to an overestimation of their robustness. The proposed approach integrates repeated sampling of model outputs with prompt optimization, framing the attack as a resource allocation problem that optimizes the trade-off between sampling and optimization. This method significantly enhances the effectiveness of adversarial attacks, achieving up to a 37% increase in success rates and improving efficiency by two orders of magnitude. The paper contributes by demonstrating the necessity of sampling in adversarial attacks and introducing a label-free objective based on entropy maximization, thereby providing new optimization targets for enhancing LLM safety.</div>
<div class="mono" style="margin-top:8px">本研究解决了准确评估大型语言模型（LLMs）对抗鲁棒性的重要需求，以确保其安全部署。以往的方法主要集中于单点贪婪生成，未能考虑LLMs的随机性，导致对其鲁棒性的高估。所提出的方法在对抗攻击中引入了对模型输出的重复采样，将其视为优化与采样之间的资源分配问题，从而增强了攻击的有效性。该方法动机充分，成功率提高了多达37%，效率提升了两个数量级。本文的贡献在于展示了如何将采样整合到现有攻击策略中，并引入了一种基于熵最大化的新型无标签目标，最终强调了采样在评估和增强LLM安全性中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models</div>
<div class="meta-line">Authors: Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, Natasha Jaques</div>
<div class="meta-line">First: 2025-06-09T06:35:12+00:00 · Latest: 2025-10-06T03:42:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.07468v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.07468v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch -- attackers overfit to obsolete defenses, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction. We cast safety alignment as a two-player zero-sum game, where a single model alternates between attacker and defender roles -- generating adversarial prompts and safeguarding against them -- while a reward LM adjudicates outcomes. This enables dynamic co-adaptation. Grounded in the game-theoretic framework of zero-sum games, we establish a theoretical safety guarantee which motivates the design of our method: if self-play converges to a Nash Equilibrium, the defender will reliably produce safe responses to any adversarial input. Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared to attackers trained against static defenders and achieves higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained against static attackers. We further propose hidden Chain-of-Thought, allowing agents to plan privately, which boosts adversarial diversity and reduces over-refusals. Our results motivate a shift from reactive patching to proactive co-evolution in LM safety training, enabling scalable, autonomous, and robust self-improvement of LMs via multi-agent reinforcement learning (MARL).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过在线自我对弈强化学习追踪移动目标以提高语言模型的安全性</div>
<div class="mono" style="margin-top:8px">传统的语言模型（LM）安全对齐依赖于反应性、分离的程序：攻击者利用静态模型，随后进行防御性微调以修补暴露的漏洞。这种顺序方法造成了不匹配——攻击者过度拟合过时的防御，而防御者则始终滞后于新兴威胁。为了解决这个问题，我们提出了Self-RedTeam，一种在线自我对弈强化学习算法，其中攻击者和防御者代理通过持续互动共同进化。我们将安全对齐视为一个双人零和游戏，其中单个模型在攻击者和防御者角色之间交替——生成对抗性提示并对其进行保护——同时奖励语言模型裁定结果。这使得动态共同适应成为可能。基于零和游戏的博弈论框架，我们建立了理论安全保证，这激励了我们方法的设计：如果自我对弈收敛到纳什均衡，防御者将可靠地产生对任何对抗输入的安全响应。从经验上看，Self-RedTeam发现了比针对静态防御者训练的攻击者更具多样性的攻击（+21.8% SBERT），并在安全基准上实现了更高的鲁棒性（例如，在WildJailBreak上提高65.5%），相比之下，针对静态攻击者训练的防御者表现较差。我们进一步提出了隐性思维链，允许代理私下规划，从而提高对抗多样性并减少过度拒绝。我们的结果促使从反应性修补转向LM安全训练中的主动共同进化，使得通过多代理强化学习（MARL）实现可扩展、自主和鲁棒的自我改进成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of conventional language model safety alignment, which typically follows a reactive and disjointed approach that leaves models vulnerable to evolving threats. Previous methods involve static models that attackers exploit, leading to a cycle of outdated defenses. The proposed Self-RedTeam introduces an online self-play reinforcement learning algorithm that allows an attacker and defender to co-evolve through continuous interaction, framing safety alignment as a two-player zero-sum game. This method not only provides a theoretical safety guarantee based on Nash Equilibrium but also demonstrates empirical advantages, uncovering 21.8% more diverse attacks and achieving 65.5% higher robustness on safety benchmarks compared to traditional methods. The research contributes to a paradigm shift in language model safety training from reactive to proactive strategies, enabling scalable and autonomous self-improvement through multi-agent reinforcement learning.</div>
<div class="mono" style="margin-top:8px">本研究解决了传统语言模型安全对齐的局限性，该方法依赖于反应性的方法，通常导致防御者落后于攻击者。以往的方法涉及静态模型，这些模型容易受到不断演变的威胁，导致防御策略不匹配。提出的方法Self-RedTeam利用在线自我对弈强化学习算法，攻击者和防御者通过持续互动共同演化，框架为双人零和游戏。该方法不仅通过纳什均衡的概念提供了理论安全保证，还在实证上表现出优势，发现了21.8%更多的多样化攻击，并在安全基准上实现了65.5%更高的鲁棒性，相较于传统方法。该研究为语言模型安全训练的范式转变做出了贡献，从反应性转向主动共同演化，通过多智能体强化学习增强了语言模型的可扩展性和自主性。</div>
</details>
</div>
<div class="card">
<div class="title">OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!</div>
<div class="meta-line">Authors: Jingdi Lei, Varun Gumma, Rishabh Bhardwaj, Seok Min Lim, Chuan Li, Amir Zadeh, Soujanya Poria</div>
<div class="meta-line">First: 2025-09-30T16:39:17+00:00 · Latest: 2025-10-03T12:46:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26495v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.26495v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model (LLM) safety is one of the most pressing challenges for enabling wide-scale deployment. While most studies and global discussions focus on generic harms, such as models assisting users in harming themselves or others, enterprises face a more fundamental concern: whether LLM-based agents are safe for their intended use case. To address this, we introduce operational safety, defined as an LLM&#x27;s ability to appropriately accept or refuse user queries when tasked with a specific purpose. We further propose OffTopicEval, an evaluation suite and benchmark for measuring operational safety both in general and within specific agentic use cases. Our evaluations on six model families comprising 20 open-weight LLMs reveal that while performance varies across models, all of them remain highly operationally unsafe. Even the strongest models - Qwen-3 (235B) with 77.77% and Mistral (24B) with 79.96% - fall far short of reliable operational safety, while GPT models plateau in the 62-73% range, Phi achieves only mid-level scores (48-70%), and Gemma and Llama-3 collapse to 39.53% and 23.84%, respectively. While operational safety is a core model alignment issue, to suppress these failures, we propose prompt-based steering methods: query grounding (Q-ground) and system-prompt grounding (P-ground), which substantially improve OOD refusal. Q-ground provides consistent gains of up to 23%, while P-ground delivers even larger boosts, raising Llama-3.3 (70B) by 41% and Qwen-3 (30B) by 27%. These results highlight both the urgent need for operational safety interventions and the promise of prompt-based steering as a first step toward more reliable LLM-based agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OffTopicEval：当大型语言模型进入错误的聊天时，几乎总是如此！</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）安全性是实现大规模部署面临的最紧迫挑战之一。虽然大多数研究和全球讨论集中在一般性危害上，例如模型帮助用户伤害自己或他人，但企业面临更根本的担忧：基于LLM的代理是否安全用于其预期用途。为了解决这个问题，我们引入了操作安全性，定义为LLM在特定目的下适当地接受或拒绝用户查询的能力。我们进一步提出了OffTopicEval，一个评估套件和基准，用于测量一般和特定代理使用案例中的操作安全性。我们对六个模型家族中20个开放权重LLM的评估显示，尽管模型之间的性能差异很大，但它们都高度操作不安全。即使是最强的模型 - Qwen-3（235B）得分77.77%和Mistral（24B）得分79.96% - 也远未达到可靠的操作安全性，而GPT模型的得分停留在62-73%范围内，Phi仅获得中等得分（48-70%），Gemma和Llama-3分别降至39.53%和23.84%。虽然操作安全性是核心模型对齐问题，为了抑制这些失败，我们提出了基于提示的引导方法：查询基础（Q-ground）和系统提示基础（P-ground），显著提高了OOD拒绝率。Q-ground提供了高达23%的持续增益，而P-ground则提供了更大的提升，将Llama-3.3（70B）提高了41%，将Qwen-3（30B）提高了27%。这些结果突显了对操作安全性干预的迫切需求，以及基于提示的引导作为朝着更可靠的基于LLM的代理迈出的第一步的前景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical challenge of safety in deploying Large Language Models (LLMs), particularly focusing on their operational safety, which is the ability to appropriately accept or refuse user queries for specific purposes. Previous methods primarily concentrated on generic harms without adequately assessing the suitability of LLMs for intended use cases, leading to significant operational safety concerns. The proposed approach, OffTopicEval, introduces an evaluation suite and benchmark specifically designed to measure operational safety across various LLMs, revealing that all tested models exhibit high levels of operational unsafety. The methodology includes prompt-based steering techniques, such as query grounding (Q-ground) and system-prompt grounding (P-ground), which enhance out-of-distribution refusal rates, achieving performance improvements of up to 41% for Llama-3.3 and 27% for Qwen-3. These findings underscore the necessity for operational safety measures and demonstrate the effectiveness of prompt-based steering in enhancing LLM reliability.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLM）安全性中的关键挑战，特别是在特定用例中的操作安全性，这在以往的研究中被忽视，后者主要关注一般性危害。现有方法未能有效确保LLM能够适当地接受或拒绝用户查询，导致显著的操作安全性问题。本文引入了OffTopicEval，一个旨在测量各种LLM操作安全性的评估套件，结果显示所有测试模型都表现出高度的操作不安全性，性能指标表明需要改进。作者提出了基于提示的引导方法，具体包括查询引导（Q-ground）和系统提示引导（P-ground），这些方法提高了超出分布的拒绝率，某些模型的性能提升达41%。这些发现强调了操作安全性措施的必要性，并展示了基于提示的引导在创建更可靠的LLM代理方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models</div>
<div class="meta-line">Authors: Yuhao Sun, Zhuoer Xu, Shiwen Cui, Kun Yang, Lingyun Yu, Yongdong Zhang, Hongtao Xie</div>
<div class="meta-line">First: 2025-10-02T16:43:33+00:00 · Latest: 2025-10-02T16:43:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.02194v1">Abs</a> · <a href="https://arxiv.org/pdf/2510.02194v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have achieved remarkable progress across a wide range of tasks, but remain vulnerable to safety risks such as harmful content generation and jailbreak attacks. Existing safety techniques -- including external guardrails, inference-time guidance, and post-training alignment -- each face limitations in balancing safety, utility, and controllability. In this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM safety through safety-aware upcycling. Our approach first identifies safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE) structure, where the router acts as a soft guardrail that selectively activates original MLPs and added safety experts. We further introduce a two-stage SFT strategy to strengthen safety discrimination while preserving general capabilities. To enable flexible control at inference time, we introduce a safety temperature mechanism, allowing dynamic adjustment of the trade-off between safety and utility. Experiments across multiple benchmarks, base model, and model scales demonstrate that UpSafe$^\circ$C achieves robust safety improvements against harmful and jailbreak inputs, while maintaining competitive performance on general tasks. Moreover, analysis shows that safety temperature provides fine-grained inference-time control that achieves the Pareto-optimal frontier between utility and safety. Our results highlight a new direction for LLM safety: moving from static alignment toward dynamic, modular, and inference-aware control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UpSafe$^\circ$C：大型语言模型的可控安全升级</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在广泛任务中取得了显著进展，但仍然容易受到安全风险的影响，如有害内容生成和越狱攻击。现有的安全技术——包括外部护栏、推理时指导和训练后对齐——在平衡安全性、实用性和可控性方面各有局限。在本研究中，我们提出了UpSafe$^\circ$C，一个通过安全意识升级来增强LLM安全性的统一框架。我们的方法首先识别安全关键层，并将其升级为稀疏的专家混合（MoE）结构，其中路由器充当软护栏，选择性激活原始MLP和新增的安全专家。我们进一步引入了两阶段的SFT策略，以增强安全区分能力，同时保持一般能力。为了在推理时实现灵活控制，我们引入了安全温度机制，允许动态调整安全性和实用性之间的权衡。在多个基准、基础模型和模型规模上的实验表明，UpSafe$^\circ$C在对抗有害和越狱输入时实现了稳健的安全改进，同时在一般任务上保持竞争力。此外，分析表明安全温度提供了细粒度的推理时控制，实现了实用性和安全性之间的帕累托最优边界。我们的结果突显了LLM安全的新方向：从静态对齐转向动态、模块化和推理意识的控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the safety vulnerabilities of Large Language Models (LLMs), which can generate harmful content and be susceptible to jailbreak attacks. Previous safety methods, such as external guardrails and post-training alignment, struggle to balance safety, utility, and controllability. The proposed UpSafe$^\circ$C framework enhances LLM safety through a novel approach called safety-aware upcycling, which restructures safety-critical layers into a sparse Mixture-of-Experts (MoE) format and employs a two-stage SFT strategy to improve safety discrimination while retaining general capabilities. This methodology allows for flexible control via a safety temperature mechanism, enabling dynamic adjustments between safety and utility. Experimental results show that UpSafe$^\circ$C significantly improves safety against harmful inputs while maintaining competitive performance on general tasks, thus supporting the goal of achieving a balance between safety and utility in LLMs.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）的安全漏洞，这些模型可能生成有害内容并容易受到越狱攻击。以往的安全技术，如外部护栏和后训练对齐，难以有效平衡安全性、实用性和可控性。提出的UpSafe$^\circ$C框架通过实施安全感知的再利用来增强LLM的安全性，该方法识别关键层并将其转化为稀疏的专家混合结构，从而允许选择性激活安全专家。该方法引入了两阶段的SFT策略，以提高安全性辨别能力，同时保持一般能力，并结合安全温度机制以实现推理过程中的动态控制。实验结果表明，UpSafe$^\circ$C在抵御有害输入方面显著提高了安全性，同时保持了在一般任务上的竞争性表现，展示了LLM安全控制向动态和模块化的转变。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense</div>
<div class="meta-line">Authors: Guobin Shen, Dongcheng Zhao, Haibo Tong, Jindong Li, Feifei Zhao, Yi Zeng</div>
<div class="meta-line">First: 2025-10-01T16:35:03+00:00 · Latest: 2025-10-01T16:35:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01088v1">Abs</a> · <a href="https://arxiv.org/pdf/2510.01088v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring Large Language Model (LLM) safety remains challenging due to the absence of universal standards and reliable content validators, making it difficult to obtain effective training signals. We discover that aligned models already possess robust internal safety beliefs: they consistently produce high-confidence refusals to harmful requests while exhibiting high entropy when generating potentially dangerous content. This entropy gap reveals an untapped signal--models intrinsically &quot;know&quot; when to refuse. We introduce Safety Instincts Reinforcement Learning (SIRL), which transforms this internal confidence into a self-generated reward signal, eliminating dependence on external validators or human annotations. SIRL teaches models to trust their safety instincts by reinforcing low-entropy refusal behaviors. Evaluated on Llama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against 20+ jailbreak methods, from static prompts to adaptive attacks. Using only 15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods while preserving performance on mathematics, coding, and conversation benchmarks. Our work demonstrates that effective alignment can emerge from within, paving the way for more autonomous and robust AI safety mechanisms that scale without extensive human oversight.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全本能：大型语言模型学习信任其内部指南以自我防卫</div>
<div class="mono" style="margin-top:8px">确保大型语言模型（LLM）的安全性仍然具有挑战性，因为缺乏通用标准和可靠的内容验证者，使得获得有效的训练信号变得困难。我们发现对齐模型已经具备强大的内部安全信念：它们在面对有害请求时始终产生高置信度的拒绝，同时在生成潜在危险内容时表现出高熵。这一熵差揭示了一个未被利用的信号——模型本质上“知道”何时拒绝。我们引入了安全本能强化学习（SIRL），将这种内部信心转化为自生成的奖励信号，消除对外部验证者或人工注释的依赖。SIRL通过强化低熵拒绝行为来教导模型信任其安全本能。在Llama和Qwen模型上的评估显示，SIRL在20多种越狱方法下保持89%以上的防御成功率（DSR），从静态提示到自适应攻击。仅使用15,000个未标记的提示，SIRL超越了资源密集型的监督方法，同时在数学、编码和对话基准测试中保持性能。我们的工作表明，有效的对齐可以从内部产生，为更自主和强大的AI安全机制铺平道路，这些机制可以在没有广泛人工监督的情况下扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the ongoing challenge of ensuring safety in Large Language Models (LLMs), which is complicated by the lack of universal standards and reliable content validators. Previous methods have struggled with dependence on external validators and human annotations, leading to inefficiencies and limitations in training signals. The proposed Safety Instincts Reinforcement Learning (SIRL) approach leverages the internal safety beliefs of aligned models, transforming their intrinsic confidence into a self-generated reward signal to reinforce low-entropy refusal behaviors. This method is well-motivated as it allows models to trust their safety instincts without relying on external inputs. The contribution of the paper lies in demonstrating that SIRL can maintain over 89% Defense Success Rates against various jailbreak methods while using only 15,000 unlabeled prompts, outperforming traditional supervised methods and maintaining performance across other benchmarks such as mathematics, coding, and conversation.</div>
<div class="mono" style="margin-top:8px">本研究解决了确保大型语言模型（LLM）安全性的问题，这一问题因缺乏通用标准和可靠内容验证者而变得复杂，导致有效训练信号的获取困难。以往的方法通常依赖于外部验证者或人工注释，这可能资源密集且效率低下。提出的安全本能强化学习（SIRL）方法通过利用对齐模型的内部安全信念，将其内在信心转化为自生成的奖励信号，从而强化低熵拒绝行为，这与以往方法不同，且有效地消除了对外部依赖的需求。该方法的动机明确，因为它使模型能够信任其安全本能。论文的贡献在于证明SIRL在使用仅15,000个未标记提示的情况下，能够在多种越狱方法中实现超过89%的防御成功率，超越传统的监督方法，并在数学、编码和对话任务的多个基准上保持性能，从而支持增强自主AI安全机制的目标。</div>
</details>
</div>
<div class="card">
<div class="title">QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety</div>
<div class="meta-line">Authors: Taegyeong Lee, Jeonghwa Yoo, Hyoungseo Cho, Soo Yong Kim, Yunho Maeng</div>
<div class="meta-line">Venue: ACL</div>
<div class="meta-line">First: 2025-06-14T01:23:50+00:00 · Latest: 2025-09-30T07:47:51+00:00</div>
<div class="meta-line">Comments: Accept to ACLW 2025 (WOAH); fix typo</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.12299v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.12299v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent advancements in Large Language Models(LLMs) have had a significant impact on a wide range of fields, from general domains to specialized areas. However, these advancements have also significantly increased the potential for malicious users to exploit harmful and jailbreak prompts for malicious attacks. Although there have been many efforts to prevent harmful prompts and jailbreak prompts, protecting LLMs from such malicious attacks remains an important and challenging task. In this paper, we propose QGuard, a simple yet effective safety guard method, that utilizes question prompting to block harmful prompts in a zero-shot manner. Our method can defend LLMs not only from text-based harmful prompts but also from multi-modal harmful prompt attacks. Moreover, by diversifying and modifying guard questions, our approach remains robust against the latest harmful prompts without fine-tuning. Experimental results show that our model performs competitively on both text-only and multi-modal harmful datasets. Additionally, by providing an analysis of question prompting, we enable a white-box analysis of user inputs. We believe our method provides valuable insights for real-world LLM services in mitigating security risks associated with harmful prompts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QGuard：基于问题的零-shot多模态LLM安全防护</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的最新进展对从一般领域到专业领域的广泛领域产生了重大影响。然而，这些进展也显著增加了恶意用户利用有害和越狱提示进行恶意攻击的潜力。尽管已经有许多努力来防止有害提示和越狱提示，但保护LLMs免受此类恶意攻击仍然是一项重要且具有挑战性的任务。本文提出了QGuard，一种简单而有效的安全防护方法，利用问题提示以零-shot方式阻止有害提示。我们的方法不仅可以防御基于文本的有害提示，还可以防御多模态有害提示攻击。此外，通过多样化和修改防护问题，我们的方法在不进行微调的情况下仍然对最新的有害提示保持稳健。实验结果表明，我们的模型在文本和多模态有害数据集上表现出竞争力。此外，通过提供问题提示的分析，我们实现了对用户输入的白盒分析。我们相信我们的方法为现实世界的LLM服务在减轻与有害提示相关的安全风险方面提供了有价值的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concern of malicious exploitation of Large Language Models (LLMs) through harmful and jailbreak prompts, which pose significant security risks. Previous methods aimed at preventing such attacks have been insufficient, often requiring fine-tuning and lacking robustness against evolving threats. The proposed approach, QGuard, introduces a question-based zero-shot safety guard that effectively blocks harmful prompts without the need for fine-tuning, thus offering a more adaptable solution. This paper contributes by demonstrating that QGuard can defend against both text-based and multi-modal harmful prompts, achieving competitive performance on relevant datasets. The methodology involves utilizing diverse and modified guard questions to maintain effectiveness against new threats, supporting the goal of enhancing LLM safety in real-world applications.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）面临的恶意利用威胁，尤其是通过有害和越狱提示进行的攻击，这对安全性构成了重大挑战。以往的防范方法往往效果不佳，通常需要微调，并且对不断演变的威胁缺乏鲁棒性。提出的QGuard方法通过在零样本情况下使用问题提示，能够有效阻止有害提示，而无需微调，从而增强了其对新威胁的适应性。本文贡献了一种新颖的安全防护机制，不仅保护文本攻击，还扩展到多模态有害提示的防御。实验结果表明，QGuard在文本和多模态数据集上均表现出竞争力，支持其在现实世界LLM应用中减轻与有害提示相关的安全风险的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models</div>
<div class="meta-line">Authors: Miao Yu, Zhenhong Zhou, Moayad Aloqaily, Kun Wang, Biwei Huang, Stephen Wang, Yueming Jin, Qingsong Wen</div>
<div class="meta-line">First: 2025-09-26T01:45:25+00:00 · Latest: 2025-09-30T01:52:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21761v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.21761v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuned Large Language Models (LLMs) are vulnerable to backdoor attacks through data poisoning, yet the internal mechanisms governing these attacks remain a black box. Previous research on interpretability for LLM safety tends to focus on alignment, jailbreak, and hallucination, but overlooks backdoor mechanisms, making it difficult to understand and fully eliminate the backdoor threat. In this paper, aiming to bridge this gap, we explore the interpretable mechanisms of LLM backdoors through Backdoor Attribution (BkdAttr), a tripartite causal analysis framework. We first introduce the Backdoor Probe that proves the existence of learnable backdoor features encoded within the representations. Building on this insight, we further develop Backdoor Attention Head Attribution (BAHA), efficiently pinpointing the specific attention heads responsible for processing these features. Our primary experiments reveals these heads are relatively sparse; ablating a minimal \textbf{$\sim$ 3%} of total heads is sufficient to reduce the Attack Success Rate (ASR) by \textbf{over 90%}. More importantly, we further employ these findings to construct the Backdoor Vector derived from these attributed heads as a master controller for the backdoor. Through only \textbf{1-point} intervention on \textbf{single} representation, the vector can either boost ASR up to \textbf{$\sim$ 100% ($\uparrow$)} on clean inputs, or completely neutralize backdoor, suppressing ASR down to \textbf{$\sim$ 0% ($\downarrow$)} on triggered inputs. In conclusion, our work pioneers the exploration of mechanistic interpretability in LLM backdoors, demonstrating a powerful method for backdoor control and revealing actionable insights for the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>后门归因：阐明和控制语言模型中的后门</div>
<div class="mono" style="margin-top:8px">微调的大型语言模型（LLMs）易受数据中毒的后门攻击，但这些攻击的内部机制仍然是一个黑箱。以往关于LLM安全性的可解释性研究往往集中在对齐、越狱和幻觉上，而忽视了后门机制，使得理解和完全消除后门威胁变得困难。本文旨在填补这一空白，通过后门归因（BkdAttr）这一三方因果分析框架，探索LLM后门的可解释机制。我们首先引入后门探测器，证明可学习的后门特征存在于表示中。基于这一见解，我们进一步开发了后门注意力头归因（BAHA），有效地定位负责处理这些特征的特定注意力头。我们的主要实验表明，这些头相对稀疏；去除约3%的总头数足以将攻击成功率（ASR）降低超过90%。更重要的是，我们进一步利用这些发现构建了基于这些归因头的后门向量，作为后门的主控制器。通过对单一表示进行1点干预，该向量可以在干净输入上将ASR提升至约100%（↑），或在触发输入上完全中和后门，将ASR压制至约0%（↓）。总之，我们的工作开创了LLM后门机制可解释性的探索，展示了一种强大的后门控制方法，并为社区提供了可行的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of fine-tuned Large Language Models (LLMs) to backdoor attacks via data poisoning, highlighting a gap in existing interpretability research that typically focuses on alignment and hallucination but neglects backdoor mechanisms. Previous methods have failed to elucidate these mechanisms, making it challenging to understand and mitigate backdoor threats. The proposed approach, Backdoor Attribution (BkdAttr), introduces a tripartite causal analysis framework that includes the Backdoor Probe to identify learnable backdoor features and the Backdoor Attention Head Attribution (BAHA) to isolate specific attention heads responsible for these features. The methodology demonstrates that ablating approximately 3% of total heads can reduce the Attack Success Rate (ASR) by over 90%, and the construction of a Backdoor Vector allows for precise control over the backdoor, achieving ASR manipulation from nearly 100% to 0% with minimal intervention. This work significantly advances the mechanistic interpretability of LLM backdoors and provides practical strategies for controlling backdoor threats.</div>
<div class="mono" style="margin-top:8px">本研究解决了经过微调的大型语言模型（LLMs）在数据中毒攻击下的脆弱性，强调了对这些攻击内部机制理解的不足，而以往的可解释性研究主要忽视了这一点。现有方法集中于对齐和其他安全问题，但未能阐明后门机制，使得有效缓解这一威胁变得困难。提出的方法，后门归因（BkdAttr），引入了一个三方因果分析框架，包括后门探测器以识别可学习的后门特征，以及后门注意力头归因（BAHA）以定位负责这些特征的特定注意力头。该方法表明，去除大约3%的总注意力头可以将攻击成功率（ASR）降低超过90%，而从这些头中派生的后门向量可以在干净输入上将ASR提升至近100%，或在触发输入上完全中和后门，实现约0%的ASR。这项工作显著推动了对LLM后门的理解，并提供了一种强有力的控制方法，为该领域未来的研究提供了宝贵的见解。</div>
</details>
</div>
<div class="card">
<div class="title">STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents</div>
<div class="meta-line">Authors: Jing-Jing Li, Jianfeng He, Chao Shang, Devang Kulshreshtha, Xun Xian, Yi Zhang, Hang Su, Sandesh Swamy, Yanjun Qi</div>
<div class="meta-line">First: 2025-09-30T00:31:44+00:00 · Latest: 2025-09-30T00:31:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25624v1">Abs</a> · <a href="https://arxiv.org/pdf/2509.25624v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As LLMs advance into autonomous agents with tool-use capabilities, they introduce security challenges that extend beyond traditional content-based LLM safety concerns. This paper introduces Sequential Tool Attack Chaining (STAC), a novel multi-turn attack framework that exploits agent tool use. STAC chains together tool calls that each appear harmless in isolation but, when combined, collectively enable harmful operations that only become apparent at the final execution step. We apply our framework to automatically generate and systematically evaluate 483 STAC cases, featuring 1,352 sets of user-agent-environment interactions and spanning diverse domains, tasks, agent types, and 10 failure modes. Our evaluations show that state-of-the-art LLM agents, including GPT-4.1, are highly vulnerable to STAC, with attack success rates (ASR) exceeding 90% in most cases. The core design of STAC&#x27;s automated framework is a closed-loop pipeline that synthesizes executable multi-step tool chains, validates them through in-environment execution, and reverse-engineers stealthy multi-turn prompts that reliably induce agents to execute the verified malicious sequence. We further perform defense analysis against STAC and find that existing prompt-based defenses provide limited protection. To address this gap, we propose a new reasoning-driven defense prompt that achieves far stronger protection, cutting ASR by up to 28.8%. These results highlight a crucial gap: defending tool-enabled agents requires reasoning over entire action sequences and their cumulative effects, rather than evaluating isolated prompts or responses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STAC：当无辜工具形成危险链条以越狱LLM代理</div>
<div class="mono" style="margin-top:8px">随着LLM发展为具有工具使用能力的自主代理，它们引入了超越传统内容基础LLM安全问题的安全挑战。本文介绍了顺序工具攻击链（STAC），一种新颖的多轮攻击框架，利用代理工具的使用。STAC将看似无害的工具调用串联在一起，但当组合时，集体启用有害操作，这些操作仅在最终执行步骤时显现。我们应用我们的框架自动生成并系统评估了483个STAC案例，涉及1,352组用户-代理-环境交互，涵盖多种领域、任务、代理类型和10种失败模式。我们的评估显示，最先进的LLM代理，包括GPT-4.1，在大多数情况下对STAC高度脆弱，攻击成功率（ASR）超过90%。STAC自动化框架的核心设计是一个闭环管道，合成可执行的多步骤工具链，通过环境执行进行验证，并反向工程隐蔽的多轮提示，可靠地诱导代理执行经过验证的恶意序列。我们进一步对STAC进行防御分析，发现现有的基于提示的防御提供的保护有限。为了解决这一差距，我们提出了一种新的基于推理的防御提示，能够实现更强的保护，将ASR降低多达28.8%。这些结果突显了一个关键差距：防御工具启用的代理需要对整个行动序列及其累积效果进行推理，而不是评估孤立的提示或响应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the emerging security challenges posed by large language models (LLMs) as they evolve into autonomous agents capable of tool use, which introduces risks beyond traditional safety concerns. Previous methods primarily focused on isolated prompt evaluations, which fail to account for the cumulative effects of tool interactions. The proposed Sequential Tool Attack Chaining (STAC) framework differs by systematically chaining together seemingly harmless tool calls to reveal harmful operations only at the final execution stage, thus effectively exploiting the vulnerabilities of LLM agents. The contribution of this paper lies in the development of an automated framework that generates and evaluates 483 STAC cases across various domains, demonstrating that state-of-the-art LLM agents, including GPT-4.1, exhibit attack success rates exceeding 90%. Additionally, the paper introduces a new reasoning-driven defense prompt that significantly enhances protection against STAC, reducing attack success rates by up to 28.8%, thereby emphasizing the need for defenses that consider the entire sequence of actions rather than isolated prompts.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）作为具备工具使用能力的自主代理所带来的新兴安全挑战，这些挑战超出了传统安全问题的范围。以往的方法主要集中在孤立的提示评估上，未能考虑工具交互的累积效应，而提出的顺序工具攻击链（STAC）框架有效地通过将看似无害的工具调用链成有害操作来解决这一问题。本文的贡献在于开发了STAC这一多轮攻击框架，系统地生成和评估大量攻击场景，揭示了最先进的LLM代理存在显著的脆弱性，攻击成功率超过90%。该方法论涉及一个闭环管道，合成和验证可执行的工具链，同时提出了一种新的基于推理的防御提示，显著降低攻击成功率达28.8%，强调了需要考虑整个行动序列而非孤立提示的全面防御。</div>
</details>
</div>
<div class="card">
<div class="title">Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection</div>
<div class="meta-line">Authors: Hoang Phan, Victor Li, Qi Lei</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-09-29T12:54:28+00:00 · Latest: 2025-09-29T12:54:28+00:00</div>
<div class="meta-line">Comments: Accepted to EMNLP 2025 Findings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01270v1">Abs</a> · <a href="https://arxiv.org/pdf/2510.01270v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have revolutionized natural language processing with their ability to generate coherent and contextually relevant text. However, their deployment raises significant concerns about the potential for generating harmful or inappropriate content. In this paper, we introduce Progressive Self-Reflection (PSR), a novel inference-time technique that empowers LLMs to self-monitor and correct their outputs dynamically. Experimental results demonstrate that applying our proposed method to Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\% to 5.9\%, to Llama-3.1-8B base from 89.7\% to 5.6\%, and to Qwen2.5-7B-Instruct from 44.4\% to 3.8\%, without additional training, while maintaining their original performance on benign tasks. Our approach acts as a test-time scaling method, where additional self-reflection rounds enhance safety at the cost of inference overhead. To balance safety with computational efficiency, we introduce a lightweight self-reflection predictor that estimates the optimal number of reflection rounds based on input complexity. This adaptive mechanism prevents unnecessary self-assessment on benign inputs while ensuring thorough evaluation when encountering potentially harmful content. Our findings suggest that Progressive Self-Reflection serves as a scalable test-time approach, enhancing LLM safety by dynamically allocating computational resources in proportion to the input&#x27;s risk profile.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>三思而后行：通过渐进自我反思保障安全</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）凭借生成连贯且上下文相关文本的能力，彻底改变了自然语言处理。然而，它们的部署引发了关于生成有害或不当内容的潜在风险的重大担忧。本文介绍了一种新颖的推理时技术——渐进自我反思（PSR），使LLMs能够动态自我监控和纠正其输出。实验结果表明，将我们提出的方法应用于Llama-3.1-8B-Instruct时，攻击成功率从77.5\%降低到5.9\%；对Llama-3.1-8B base从89.7\%降低到5.6\%；对Qwen2.5-7B-Instruct从44.4\%降低到3.8\%，且无需额外训练，同时保持其在良性任务上的原始性能。我们的方法作为一种测试时扩展方法，额外的自我反思轮次在推理开销的代价下增强了安全性。为了在安全性与计算效率之间取得平衡，我们引入了一种轻量级自我反思预测器，根据输入复杂性估计最佳反思轮次。该自适应机制防止在良性输入上进行不必要的自我评估，同时确保在遇到潜在有害内容时进行全面评估。我们的研究结果表明，渐进自我反思作为一种可扩展的测试时方法，通过动态分配计算资源来增强LLM的安全性，比例与输入的风险特征相匹配。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concerns surrounding the deployment of large language models (LLMs), particularly their potential to generate harmful or inappropriate content. Previous methods lacked effective self-monitoring capabilities, leading to high rates of harmful output. The proposed Progressive Self-Reflection (PSR) technique differs by enabling LLMs to dynamically self-correct their outputs during inference, significantly reducing the attack success rates across various models without additional training. This method is well-motivated as it balances safety and computational efficiency by using a lightweight predictor to determine the optimal number of self-reflection rounds based on input complexity. The experimental results show that PSR effectively lowers the attack success rates to as low as 3.8% while maintaining performance on benign tasks, demonstrating its potential to enhance LLM safety while managing computational resources effectively.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型（LLMs）在自然语言处理中的应用日益增加而引发的生成有害或不当内容的担忧进行了研究。以往的方法缺乏动态自我监控能力，导致有害输出的高发生率。所提出的渐进自我反思（PSR）技术通过在推理过程中使LLMs能够自我纠正，从而显著降低攻击成功率，同时保持对良性任务的性能。本文的贡献在于引入了一种测试时扩展方法，结合轻量级自我反思预测器，根据输入复杂性优化反思轮次，从而平衡安全性和计算效率。实验结果表明，PSR显著降低了多种模型的攻击成功率，证明了其在增强LLM安全性方面的有效性，而不影响其原有能力。</div>
</details>
</div>
<div class="card">
<div class="title">LionGuard 2: Building Lightweight, Data-Efficient &amp; Localised Multilingual Content Moderators</div>
<div class="meta-line">Authors: Leanne Tan, Gabriel Chua, Ziyu Ge, Roy Ka-Wei Lee</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-07-21T07:50:48+00:00 · Latest: 2025-09-28T02:30:26+00:00</div>
<div class="meta-line">Comments: EMNLP 2025 System Demonstration Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.15339v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.15339v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern moderation systems increasingly support multiple languages, but often fail to address localisation and low-resource variants - creating safety gaps in real-world deployments. Small models offer a potential alternative to large LLMs, yet still demand considerable data and compute. We present LionGuard 2, a lightweight, multilingual moderation classifier tailored to the Singapore context, supporting English, Chinese, Malay, and partial Tamil. Built on pre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2 outperforms several commercial and open-source systems across 17 benchmarks, including both Singapore-specific and public English datasets. The system is actively deployed within the Singapore Government, demonstrating practical efficacy at scale. Our findings show that high-quality local data and robust multilingual embeddings can achieve strong moderation performance, without fine-tuning large models. We release our model weights and part of our training data to support future work on LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LionGuard 2：构建轻量级、数据高效和本地化的多语言内容审核系统</div>
<div class="mono" style="margin-top:8px">现代审核系统越来越支持多种语言，但往往未能解决本地化和低资源变体的问题，导致现实部署中的安全漏洞。小型模型为大型LLM提供了潜在替代方案，但仍需大量数据和计算。我们提出了LionGuard 2，一种针对新加坡环境量身定制的轻量级多语言审核分类器，支持英语、中文、马来语和部分泰米尔语。LionGuard 2基于预训练的OpenAI嵌入和多头序数分类器，在17个基准测试中超越了多个商业和开源系统，包括新加坡特定和公共英语数据集。该系统已在新加坡政府中积极部署，展示了大规模的实际有效性。我们的研究结果表明，高质量的本地数据和强大的多语言嵌入可以在不微调大型模型的情况下实现强大的审核性能。我们发布了模型权重和部分训练数据，以支持未来在LLM安全方面的工作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing multilingual moderation systems, which often overlook localization and low-resource language variants, leading to safety gaps in real-world applications. Previous methods typically rely on large language models that require substantial data and computational resources, which can be impractical. In contrast, the proposed LionGuard 2 offers a lightweight and data-efficient solution specifically designed for the Singapore context, supporting multiple languages including English, Chinese, Malay, and partial Tamil. This approach utilizes pre-trained OpenAI embeddings and a multi-head ordinal classifier, achieving superior performance across 17 benchmarks compared to various commercial and open-source systems. The methodology demonstrates that high-quality local data and robust multilingual embeddings can deliver effective moderation without the need for fine-tuning large models, thus contributing to the practical deployment of moderation systems within the Singapore Government and enhancing LLM safety. The performance results indicate that LionGuard 2 meets its objectives effectively.</div>
<div class="mono" style="margin-top:8px">本文解决了现代内容审核系统在支持多语言时面临的挑战，特别是在本地化和低资源变体方面，这可能导致现实应用中的安全漏洞。以往的方法通常依赖于大型语言模型（LLMs），这些模型需要大量的数据和计算资源，使其在本地化上下文中不够实用。提出的方法LionGuard 2是一种轻量级的多语言内容审核分类器，专为新加坡环境设计，利用预训练的OpenAI嵌入和多头序数分类器。该方法有效解决了现有系统的局限性，表明高质量的本地数据和强大的多语言嵌入可以在不微调大型模型的情况下实现强大的审核性能。本文的贡献在于展示了LionGuard 2在17个基准测试中的优越表现，包括新加坡特定和公共英语数据集，并成功在新加坡政府内部署，表明其在规模上的实际有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance</div>
<div class="meta-line">Authors: Wenbin Hu, Huihao Jing, Haochen Shi, Haoran Li, Yangqiu Song</div>
<div class="meta-line">First: 2025-09-26T12:11:29+00:00 · Latest: 2025-09-26T12:11:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22250v1">Abs</a> · <a href="https://arxiv.org/pdf/2509.22250v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The proliferation of Large Language Models (LLMs) has demonstrated remarkable capabilities, elevating the critical importance of LLM safety. However, existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic protection, failing to ensure safety for the nuanced and complex behaviors of modern LLM systems. To address this problem, we solve LLM safety from legal compliance perspectives, named safety compliance. In this work, we posit relevant established legal frameworks as safety standards for defining and measuring safety compliance, including the EU AI Act and GDPR, which serve as core legal frameworks for AI safety and data security in Europe. To bridge the gap between LLM safety and legal compliance, we first develop a new benchmark for safety compliance by generating realistic LLM safety scenarios seeded with legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization (GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively aligns LLMs with legal standards to mitigate safety risks. Our comprehensive experiments demonstrate that the Compliance Reasoner achieves superior performance on the new benchmark, with average improvements of +10.45% for the EU AI Act and +11.85% for GDPR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全合规：通过合规视角重新思考大型语言模型的安全推理</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的普及展示了显著的能力，提升了LLM安全的重要性。然而，现有的安全方法依赖于临时分类法，缺乏严格的系统保护，未能确保现代LLM系统复杂行为的安全。为了解决这个问题，我们从法律合规的角度解决LLM安全，称之为安全合规。在这项工作中，我们将相关的既定法律框架视为定义和衡量安全合规的安全标准，包括欧盟人工智能法案和通用数据保护条例，这些是欧洲人工智能安全和数据安全的核心法律框架。为了弥合LLM安全与法律合规之间的差距，我们首先通过生成带有法律条款的现实LLM安全场景来开发一个新的安全合规基准。随后，我们使用群体政策优化（GRPO）对Qwen3-8B进行对齐，以构建一个安全推理器——合规推理器，有效地将LLM与法律标准对齐，以降低安全风险。我们的综合实验表明，合规推理器在新的基准上表现优越，欧盟人工智能法案的平均提升为+10.45%，通用数据保护条例的平均提升为+11.85%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical issue of safety in Large Language Models (LLMs), highlighting that existing safety methods are often ad-hoc and lack a systematic approach, which fails to adequately manage the complex behaviors of these models. The proposed method, termed safety compliance, leverages established legal frameworks such as the EU AI Act and GDPR to define and measure safety standards, thus providing a more rigorous basis for LLM safety. This approach is well-motivated as it bridges the gap between LLM safety and legal compliance. The paper introduces a new benchmark for safety compliance by creating realistic scenarios based on legal statutes and employs Group Policy Optimization (GRPO) to develop a Compliance Reasoner that aligns LLMs with these legal standards. Experimental results indicate that the Compliance Reasoner significantly improves performance on the new benchmark, achieving average enhancements of +10.45% for the EU AI Act and +11.85% for GDPR, thereby supporting the goals of enhancing LLM safety.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）安全性的重要性，指出现有的安全方法往往是临时的，缺乏系统性，无法充分保护这些模型复杂行为的安全。提出的安全合规方法利用现有的法律框架，如欧盟人工智能法案和GDPR，来定义和衡量安全标准，从而提供比以往方法更为结构化的解决方案。本文的贡献在于通过法律法规启发的现实LLM安全场景，开发了一个新的安全合规基准，并引入了合规推理器，该推理器使用群体政策优化（GRPO）将LLMs与这些法律标准对齐。该方法在新基准上表现出显著的性能提升，欧盟人工智能法案平均提高了10.45%，GDPR平均提高了11.85%，有效支持了通过法律合规确保LLM安全的目标。</div>
</details>
</div>
<div class="card">
<div class="title">The Rogue Scalpel: Activation Steering Compromises LLM Safety</div>
<div class="meta-line">Authors: Anton Korznikov, Andrey Galichin, Alexey Dontsov, Oleg Y. Rogov, Ivan Oseledets, Elena Tutubalina</div>
<div class="meta-line">First: 2025-09-26T08:49:47+00:00 · Latest: 2025-09-26T08:49:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22067v1">Abs</a> · <a href="https://arxiv.org/pdf/2509.22067v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Activation steering is a promising technique for controlling LLM behavior by adding semantically meaningful vectors directly into a model&#x27;s hidden states during inference. It is often framed as a precise, interpretable, and potentially safer alternative to fine-tuning. We demonstrate the opposite: steering systematically breaks model alignment safeguards, making it comply with harmful requests. Through extensive experiments on different model families, we show that even steering in a random direction can increase the probability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign features from a sparse autoencoder (SAE), a common source of interpretable directions, increases these rates by a further 2-4%. Finally, we show that combining 20 randomly sampled vectors that jailbreak a single prompt creates a universal attack, significantly increasing harmful compliance on unseen requests. These results challenge the paradigm of safety through interpretability, showing that precise control over model internals does not guarantee precise control over model behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>流氓手术刀：激活引导妨碍大型语言模型安全性</div>
<div class="mono" style="margin-top:8px">激活引导是一种有前景的技术，通过在推理过程中将语义上有意义的向量直接添加到模型的隐藏状态中来控制大型语言模型的行为。它通常被视为一种精确、可解释且潜在更安全的替代微调的方法。我们证明了相反的观点：引导系统性地破坏了模型对齐的安全措施，使其遵从有害请求。通过对不同模型家族的广泛实验，我们显示即使在随机方向上引导也能将有害遵从的概率从0%提高到2-27%。令人担忧的是，从稀疏自编码器（SAE）引导良性特征，作为可解释方向的常见来源，进一步将这些比率提高了2-4%。最后，我们展示了结合20个随机采样的向量来破解单个提示会产生一种通用攻击，显著增加对未见请求的有害遵从。这些结果挑战了通过可解释性实现安全性的范式，表明对模型内部的精确控制并不保证对模型行为的精确控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the safety concerns associated with large language models (LLMs) by investigating the technique of activation steering, which aims to control LLM behavior through the addition of semantically meaningful vectors to hidden states during inference. Previous methods, including fine-tuning, have been criticized for their lack of interpretability and safety, while activation steering was proposed as a more precise and interpretable alternative. However, this study reveals that activation steering can compromise model alignment safeguards, leading to increased compliance with harmful requests. The authors conducted extensive experiments across various model families, demonstrating that even random steering can significantly raise harmful compliance rates, and that using benign features from a sparse autoencoder exacerbates this issue. The findings challenge the assumption that interpretability ensures safety, highlighting that precise control over model internals does not equate to safe model behavior, thus contributing critical insights into LLM safety mechanisms.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）的安全性问题，并评估激活引导技术的有效性，该技术旨在通过修改推理过程中的隐藏状态来控制LLM行为。以往的方法，包括微调，因缺乏可解释性和可能破坏模型对齐而受到批评。激活引导的提出是出于对更可解释和安全替代方案的需求，但研究结果表明，它实际上可能削弱安全性，导致有害合规率的增加。本文通过广泛的实验，展示了激活引导如何显著提高有害行为的发生率，即使使用无害特征，并且组合多个向量可以创建通用攻击。该方法论涉及对不同模型家族的系统测试，揭示了引导可以显著增加有害合规概率，从而挑战了可解释性确保LLM安全性的假设。</div>
</details>
</div>
<div class="card">
<div class="title">Preemptive Detection and Steering of LLM Misalignment via Latent Reachability</div>
<div class="meta-line">Authors: Sathwik Karnik, Somil Bansal</div>
<div class="meta-line">First: 2025-09-25T20:15:29+00:00 · Latest: 2025-09-25T20:15:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21528v1">Abs</a> · <a href="https://arxiv.org/pdf/2509.21528v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are now ubiquitous in everyday tools, raising urgent safety concerns about their tendency to generate harmful content. The dominant safety approach -- reinforcement learning from human feedback (RLHF) -- effectively shapes model behavior during training but offers no safeguards at inference time, where unsafe continuations may still arise. We propose BRT-Align, a reachability-based framework that brings control-theoretic safety tools to LLM inference. BRT-Align models autoregressive generation as a dynamical system in latent space and learn a safety value function via backward reachability, estimating the worst-case evolution of a trajectory. This enables two complementary mechanisms: (1) a runtime monitor that forecasts unsafe completions several tokens in advance, and (2) a least-restrictive steering filter that minimally perturbs latent states to redirect generation away from unsafe regions. Experiments across multiple LLMs and toxicity benchmarks demonstrate that BRT-Align provides more accurate and earlier detection of unsafe continuations than baselines. Moreover, for LLM safety alignment, BRT-Align substantially reduces unsafe generations while preserving sentence diversity and coherence. Qualitative results further highlight emergent alignment properties: BRT-Align consistently produces responses that are less violent, less profane, less offensive, and less politically biased. Together, these findings demonstrate that reachability analysis provides a principled and practical foundation for inference-time LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过潜在可达性进行大型语言模型不一致性的预防性检测与引导</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在日常工具中无处不在，带来了关于其生成有害内容的紧迫安全问题。主流的安全方法——基于人类反馈的强化学习（RLHF）——在训练期间有效地塑造模型行为，但在推理时没有提供任何保障，仍可能出现不安全的延续。我们提出了BRT-Align，这是一个基于可达性的框架，将控制理论安全工具引入LLM推理。BRT-Align将自回归生成建模为潜在空间中的动态系统，并通过反向可达性学习安全值函数，估计轨迹的最坏情况演变。这使得两种互补机制成为可能：（1）一个运行时监控器，提前几个标记预测不安全的完成，和（2）一个最少限制的引导过滤器，最小扰动潜在状态以将生成引导远离不安全区域。在多个LLM和毒性基准上的实验表明，BRT-Align比基线提供了更准确和更早的不安全延续检测。此外，对于LLM安全对齐，BRT-Align显著减少了不安全生成，同时保持句子的多样性和连贯性。定性结果进一步突出了新兴的对齐特性：BRT-Align始终生成更少暴力、更少粗俗、更少冒犯和更少政治偏见的响应。这些发现共同表明，可达性分析为推理时的LLM安全提供了一个原则性和实用的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the pressing safety concerns associated with large language models (LLMs), particularly their potential to generate harmful content during inference, a limitation of the existing reinforcement learning from human feedback (RLHF) approach which only shapes model behavior during training. The proposed method, BRT-Align, introduces a reachability-based framework that applies control-theoretic safety tools to LLM inference, modeling autoregressive generation as a dynamical system in latent space and learning a safety value function through backward reachability. This approach enables a runtime monitor for early detection of unsafe completions and a steering filter to minimally adjust latent states, effectively redirecting generation away from unsafe outputs. The paper demonstrates that BRT-Align significantly improves the accuracy and timeliness of unsafe continuation detection across multiple LLMs and toxicity benchmarks, achieving a substantial reduction in unsafe generations while maintaining sentence diversity and coherence, thus providing a principled foundation for LLM safety at inference time.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）在推理过程中可能生成有害内容的安全隐患进行了探讨，尽管现有的安全措施如基于人类反馈的强化学习（RLHF）仅在训练期间有效。所提出的方法BRT-Align通过采用基于可达性的框架，利用控制理论工具将自回归生成建模为潜在空间中的动态系统，从而能够估计不安全轨迹。这种方法引入了一个运行时监控器，用于提前检测不安全的生成结果，以及一个引导过滤器来调整潜在状态，有效减少不安全输出，同时保持生成句子的多样性和连贯性。多项LLM和毒性基准的实验结果表明，BRT-Align显著提高了不安全继续检测的准确性和及时性，并减少了有害输出，从而支持了增强LLM安全对齐的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework</div>
<div class="meta-line">Authors: Jiaqi Weng, Han Zheng, Hanyu Zhang, Qinqin He, Jialing Tao, Hui Xue, Zhixuan Chu, Xiting Wang</div>
<div class="meta-line">First: 2025-09-11T11:22:43+00:00 · Latest: 2025-09-24T03:58:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.18127v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.18127v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Increasing deployment of large language models (LLMs) in real-world applications raises significant safety concerns. Most existing safety research focuses on evaluating LLM outputs or specific safety tasks, limiting their ability to address broader, undefined risks. Sparse Autoencoders (SAEs) facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features decomposed from entangled signals. jHowever, prior applications on SAEs do not interpret features with fine-grained safety-related concepts, thus inadequately addressing safety-critical behaviors, such as generating toxic responses and violating safety regulations. For rigorous safety analysis, we must extract a rich and diverse set of safety-relevant features that effectively capture these high-risk behaviors, yet face two challenges: identifying SAEs with the greatest potential for generating safety concept-specific neurons, and the prohibitively high cost of detailed feature explanation. In this paper, we propose Safe-SAIL, a framework for interpreting SAE features within LLMs to advance mechanistic understanding in safety domains. Our approach systematically identifies SAE with best concept-specific interpretability, explains safety-related neurons, and introduces efficient strategies to scale up the interpretation process. We will release a comprehensive toolkit including SAE checkpoints and human-readable neuron explanations, which supports empirical analysis of safety risks to promote research on LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全SAIL：通过稀疏自编码器解释框架实现大型语言模型的细粒度安全景观</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在现实应用中的日益部署引发了重大安全问题。现有的大多数安全研究集中于评估LLM输出或特定安全任务，限制了其应对更广泛、未定义风险的能力。稀疏自编码器（SAEs）促进了解释性研究，通过解释从纠缠信号中分解出的单一意义原子特征来阐明模型行为。然而，之前对SAEs的应用并未用细粒度的安全相关概念解释特征，因此未能充分应对安全关键行为，如生成有毒响应和违反安全规定。为了进行严格的安全分析，我们必须提取丰富多样的安全相关特征，以有效捕捉这些高风险行为，但面临两个挑战：识别具有生成安全概念特定神经元最大潜力的SAEs，以及详细特征解释的高昂成本。本文提出了安全SAIL，一个在LLMs中解释SAE特征的框架，以推动安全领域的机制理解。我们的方法系统地识别具有最佳概念特定可解释性的SAE，解释安全相关神经元，并引入高效策略以扩大解释过程。我们将发布一个综合工具包，包括SAE检查点和人类可读的神经元解释，支持安全风险的实证分析，以促进LLM安全研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing use of large language models (LLMs) in practical applications has raised significant safety concerns, as existing safety research primarily evaluates LLM outputs or specific tasks, which limits the understanding of broader risks. Previous methods using Sparse Autoencoders (SAEs) have not effectively interpreted features related to fine-grained safety concepts, leading to inadequate assessments of safety-critical behaviors like generating toxic responses. The proposed Safe-SAIL framework addresses these issues by systematically identifying SAEs that yield concept-specific interpretability and providing efficient strategies for scaling the interpretation process. This paper contributes to the field by offering a comprehensive toolkit that includes SAE checkpoints and human-readable neuron explanations, facilitating empirical analysis of safety risks in LLMs. The methodology demonstrates improved interpretability of safety-related features, which supports the goal of advancing safety research in LLM applications.</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在实际应用中的广泛使用引发了重大安全隐患，因为现有的安全研究主要评估LLM的输出或特定任务，这限制了对更广泛风险的理解。以往使用稀疏自编码器（SAEs）进行可解释性的研究未能有效解决细粒度的安全相关概念，导致对生成有毒响应等安全关键行为的分析不足。提出的Safe-SAIL框架通过系统识别能够产生概念特定可解释性的SAE，并提供高效的扩展解释过程的策略，解决了这些局限性。本文的贡献在于提供了一个全面的工具包，包括SAE检查点和可读的神经元解释，促进了对LLM安全风险的实证分析。该方法论专注于提取多样的安全相关特征，以增强机制理解，最终推动对LLM安全的研究，有效应对高风险行为。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian scaling laws for in-context learning</div>
<div class="meta-line">Authors: Aryaman Arora, Dan Jurafsky, Christopher Potts, Noah D. Goodman</div>
<div class="meta-line">First: 2024-10-21T21:45:22+00:00 · Latest: 2025-09-22T16:30:22+00:00</div>
<div class="meta-line">Comments: COLM 2025 camera-ready version; 9 pages main text, 39 pages total</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.16531v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.16531v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In-context learning (ICL) is a powerful technique for getting language models to perform complex tasks with no training updates. Prior work has established strong correlations between the number of in-context examples provided and the accuracy of the model&#x27;s predictions. In this paper, we seek to explain this correlation by showing that ICL approximates a Bayesian learner. This perspective gives rise to a novel Bayesian scaling law for ICL. In experiments with \mbox{GPT-2} models of different sizes, our scaling law matches existing scaling laws in accuracy while also offering interpretable terms for task priors, learning efficiency, and per-example probabilities. To illustrate the analytic power that such interpretable scaling laws provide, we report on controlled synthetic dataset experiments designed to inform real-world studies of safety alignment. In our experimental protocol, we use SFT or DPO to suppress an unwanted existing model capability and then use ICL to try to bring that capability back (many-shot jailbreaking). We then study ICL on real-world instruction-tuned LLMs using capabilities benchmarks as well as a new many-shot jailbreaking dataset. In all cases, Bayesian scaling laws accurately predict the conditions under which ICL will cause suppressed behaviors to reemerge, which sheds light on the ineffectiveness of post-training at increasing LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文学习的贝叶斯缩放法则</div>
<div class="mono" style="margin-top:8px">上下文学习（ICL）是一种强大的技术，可以使语言模型在没有训练更新的情况下执行复杂任务。先前的研究已建立了提供的上下文示例数量与模型预测准确性之间的强相关性。本文旨在通过展示ICL近似贝叶斯学习者来解释这种相关性。这一视角产生了一个新的ICL贝叶斯缩放法则。在不同规模的\mbox{GPT-2}模型实验中，我们的缩放法则在准确性上与现有的缩放法则相匹配，同时也提供了可解释的任务先验、学习效率和每个示例概率的项。为了说明这种可解释的缩放法则所提供的分析能力，我们报告了旨在为现实世界安全对齐研究提供信息的受控合成数据集实验。在我们的实验协议中，我们使用SFT或DPO来抑制不需要的现有模型能力，然后使用ICL尝试恢复该能力（多次破解）。然后，我们使用能力基准以及一个新的多次破解数据集研究现实世界的指令调优LLM。在所有情况下，贝叶斯缩放法则准确预测了ICL将导致抑制行为重新出现的条件，这揭示了后训练在提高LLM安全性方面的无效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the growing interest in in-context learning (ICL) as a method for enabling language models to perform complex tasks without requiring training updates. Previous methods have established correlations between the number of in-context examples and model accuracy but lacked a theoretical framework to explain this relationship. The authors propose a novel Bayesian scaling law for ICL, which not only aligns with existing accuracy scaling laws but also provides interpretable components related to task priors and learning efficiency. The research methodology involves experiments with various sizes of GPT-2 models, where they demonstrate the applicability of their scaling law through controlled synthetic datasets and real-world instruction-tuned LLMs. The findings indicate that Bayesian scaling laws can predict the conditions under which ICL can reactivate suppressed model behaviors, thereby enhancing understanding of LLM safety alignment.</div>
<div class="mono" style="margin-top:8px">本文探讨了语言模型中的上下文学习（ICL）现象，该技术使模型能够在没有额外训练的情况下执行复杂任务。以往的方法建立了上下文示例数量与模型准确性之间的相关性，但缺乏理论框架来解释这些关系。所提出的方法引入了对ICL的贝叶斯视角，形成了一种新的贝叶斯缩放法则，该法则不仅与现有的准确性缩放法则一致，还提供了关于任务先验和学习效率的可解释见解。研究方法涉及对不同规模的GPT-2模型进行实验，通过受控的合成数据集和真实世界的指令调优LLM验证缩放法则。研究结果表明，贝叶斯缩放法则能够准确预测ICL重新激活被抑制的模型行为的条件，从而增强对模型安全对齐的理解。</div>
</details>
</div>
<div class="card">
<div class="title">Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs</div>
<div class="meta-line">Authors: Jean-Charles Noirot Ferrand, Yohan Beugin, Eric Pauley, Ryan Sheatsley, Patrick McDaniel</div>
<div class="meta-line">First: 2025-01-27T22:13:05+00:00 · Latest: 2025-09-19T21:29:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.16534v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.16534v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Alignment in large language models (LLMs) is used to enforce guidelines such as safety. Yet, alignment fails in the face of jailbreak attacks that modify inputs to induce unsafe outputs. In this paper, we introduce and evaluate a new technique for jailbreak attacks. We observe that alignment embeds a safety classifier in the LLM responsible for deciding between refusal and compliance, and seek to extract an approximation of this classifier: a surrogate classifier. To this end, we build candidate classifiers from subsets of the LLM. We first evaluate the degree to which candidate classifiers approximate the LLM&#x27;s safety classifier in benign and adversarial settings. Then, we attack the candidates and measure how well the resulting adversarial inputs transfer to the LLM. Our evaluation shows that the best candidates achieve accurate agreement (an F1 score above 80%) using as little as 20% of the model architecture. Further, we find that attacks mounted on the surrogate classifiers can be transferred to the LLM with high success. For example, a surrogate using only 50% of the Llama 2 model achieved an attack success rate (ASR) of 70% with half the memory footprint and runtime -- a substantial improvement over attacking the LLM directly, where we only observed a 22% ASR. These results show that extracting surrogate classifiers is an effective and efficient means for modeling (and therein addressing) the vulnerability of aligned models to jailbreaking attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>目标对齐：提取对齐大语言模型的安全分类器</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）中的对齐用于执行安全等指导方针。然而，在面对修改输入以诱导不安全输出的越狱攻击时，对齐会失败。本文介绍并评估了一种新的越狱攻击技术。我们观察到，对齐在LLM中嵌入了一个安全分类器，负责决定拒绝与遵从之间的选择，并试图提取该分类器的近似值：一个替代分类器。为此，我们从LLM的子集构建候选分类器。我们首先评估候选分类器在良性和对抗环境中近似LLM的安全分类器的程度。然后，我们攻击这些候选者，并测量结果对抗输入转移到LLM的效果。我们的评估显示，最佳候选者在使用仅20%的模型架构时，达到了准确一致（F1分数超过80%）。此外，我们发现对替代分类器发起的攻击可以高成功率地转移到LLM。例如，使用仅50%的Llama 2模型的替代分类器达到了70%的攻击成功率（ASR），且内存占用和运行时间减半——相比直接攻击LLM时仅观察到22%的ASR，这是一项显著的改进。这些结果表明，提取替代分类器是建模（并解决）对齐模型对越狱攻击脆弱性的一种有效且高效的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of large language models (LLMs) to jailbreak attacks that exploit alignment mechanisms intended to enforce safety guidelines. Previous methods have struggled to effectively counter these attacks, as they often fail to accurately model the safety classifiers embedded within LLMs. This paper proposes a novel approach to extract surrogate classifiers from subsets of the LLM, allowing for a more efficient evaluation of safety alignment. The methodology involves assessing the approximation of these candidate classifiers in both benign and adversarial contexts, followed by testing the transferability of attacks from the surrogate classifiers to the LLM. The findings indicate that the best surrogate classifiers can achieve an F1 score exceeding 80% with only 20% of the model architecture, and attacks on these classifiers yield a 70% attack success rate, significantly outperforming direct attacks on the LLM, which only achieved a 22% success rate. This demonstrates that the proposed method effectively models and mitigates the risks associated with aligned models under jailbreak conditions.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在监狱攻击下的脆弱性，这些攻击利用了旨在执行安全指南的对齐机制。以往的方法在有效应对这些攻击方面存在困难，因为它们往往无法准确建模嵌入在LLM中的安全分类器。本文提出了一种新方法，通过从LLM的子集提取代理分类器，来近似原始安全分类器。该方法包括评估这些候选分类器在良性和对抗环境中的表现，并测量从代理分类器到LLM的攻击可转移性。研究结果表明，表现最佳的代理分类器可以实现超过80%的F1分数，并且在资源需求显著降低的情况下，攻击成功率达到70%，这表明该方法在解决识别出的脆弱性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Digging Into the Internal: Causality-Based Analysis of LLM Function Calling</div>
<div class="meta-line">Authors: Zhenlan Ji, Daoyuan Wu, Wenxuan Wang, Pingchuan Ma, Shuai Wang, Lei Ma</div>
<div class="meta-line">First: 2025-09-18T08:30:26+00:00 · Latest: 2025-09-18T08:30:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16268v1">Abs</a> · <a href="https://arxiv.org/pdf/2509.16268v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Function calling (FC) has emerged as a powerful technique for facilitating large language models (LLMs) to interact with external systems and perform structured tasks. However, the mechanisms through which it influences model behavior remain largely under-explored. Besides, we discover that in addition to the regular usage of FC, this technique can substantially enhance the compliance of LLMs with user instructions. These observations motivate us to leverage causality, a canonical analysis method, to investigate how FC works within LLMs. In particular, we conduct layer-level and token-level causal interventions to dissect FC&#x27;s impact on the model&#x27;s internal computational logic when responding to user queries. Our analysis confirms the substantial influence of FC and reveals several in-depth insights into its mechanisms. To further validate our findings, we conduct extensive experiments comparing the effectiveness of FC-based instructions against conventional prompting methods. We focus on enhancing LLM safety robustness, a critical LLM application scenario, and evaluate four mainstream LLMs across two benchmark datasets. The results are striking: FC shows an average performance improvement of around 135% over conventional prompting methods in detecting malicious inputs, demonstrating its promising potential to enhance LLM reliability and capability in practical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深入内部：基于因果关系的LLM函数调用分析</div>
<div class="mono" style="margin-top:8px">函数调用（FC）已成为促进大型语言模型（LLMs）与外部系统交互和执行结构化任务的强大技术。然而，它影响模型行为的机制仍然在很大程度上未被探索。此外，我们发现除了常规使用FC外，这项技术可以显著增强LLMs对用户指令的遵从性。这些观察促使我们利用因果关系这一经典分析方法，研究FC在LLMs中的工作原理。特别地，我们进行层级和标记级的因果干预，以剖析FC在响应用户查询时对模型内部计算逻辑的影响。我们的分析确认了FC的显著影响，并揭示了其机制的若干深入见解。为了进一步验证我们的发现，我们进行了广泛的实验，比较基于FC的指令与传统提示方法的有效性。我们专注于增强LLM安全性鲁棒性，这是LLM的一个关键应用场景，并在两个基准数据集上评估了四个主流LLMs。结果令人瞩目：FC在检测恶意输入方面的平均性能提升约为135%，展示了其在实际应用中增强LLM可靠性和能力的良好潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the under-explored mechanisms of function calling (FC) in large language models (LLMs) and its influence on model behavior, particularly in enhancing compliance with user instructions. Previous methods primarily relied on conventional prompting techniques, which have limitations in effectively guiding LLMs. The proposed approach utilizes causality-based analysis to investigate FC&#x27;s impact at both layer and token levels, thereby providing a deeper understanding of its internal computational logic. This method is well-motivated by the need for improved LLM safety and robustness. The paper contributes by demonstrating that FC significantly outperforms traditional prompting methods, achieving an average performance improvement of around 135% in detecting malicious inputs across four mainstream LLMs evaluated on two benchmark datasets, thus supporting its goals of enhancing LLM reliability and capability in practical applications.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）中函数调用（FC）机制的不足之处，FC被认为是一种强大的技术，可以使LLMs执行结构化任务并与外部系统互动。以往的方法主要依赖于传统提示，未能有效增强对用户指令的遵从性。提出的方法利用基于因果关系的分析来研究FC对LLMs的影响，从而提供对其内部计算逻辑的更深入理解。本文的贡献在于通过层级和标记级的因果干预揭示FC的显著影响，并提供对其机制的见解。该方法论涉及广泛的实验，将基于FC的指令与传统提示方法进行比较，重点关注LLM的安全性和鲁棒性，结果表明在检测恶意输入方面平均性能提高约135%，从而支持了增强LLM在实际应用中可靠性和能力的目标。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Jailbreak Detection for (Almost) Free!</div>
<div class="meta-line">Authors: Guorui Chen, Yifan Xia, Xiaojun Jia, Zhijiang Li, Philip Torr, Jindong Gu</div>
<div class="meta-line">First: 2025-09-18T02:42:52+00:00 · Latest: 2025-09-18T02:42:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14558v1">Abs</a> · <a href="https://arxiv.org/pdf/2509.14558v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) enhance security through alignment when widely used, but remain susceptible to jailbreak attacks capable of producing inappropriate content. Jailbreak detection methods show promise in mitigating jailbreak attacks through the assistance of other models or multiple model inferences. However, existing methods entail significant computational costs. In this paper, we first present a finding that the difference in output distributions between jailbreak and benign prompts can be employed for detecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak Detection (FJD) which prepends an affirmative instruction to the input and scales the logits by temperature to further distinguish between jailbreak and benign prompts through the confidence of the first token. Furthermore, we enhance the detection performance of FJD through the integration of virtual instruction learning. Extensive experiments on aligned LLMs show that our FJD can effectively detect jailbreak prompts with almost no additional computational costs during LLM inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>几乎免费的LLM越狱检测！</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在广泛使用时通过对齐增强安全性，但仍然容易受到能够生成不当内容的越狱攻击。越狱检测方法在通过其他模型或多个模型推理来减轻越狱攻击方面显示出希望。然而，现有方法涉及显著的计算成本。本文首先提出一个发现，即越狱提示和良性提示之间的输出分布差异可以用于检测越狱提示。基于这一发现，我们提出了一种免费越狱检测（FJD），该方法在输入前添加肯定指令，并通过温度缩放logits，以进一步通过第一个token的置信度区分越狱和良性提示。此外，我们通过整合虚拟指令学习增强了FJD的检测性能。在对齐的LLMs上进行的大量实验表明，我们的FJD可以有效地检测越狱提示，几乎没有额外的计算成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of large language models (LLMs) to jailbreak attacks that can generate inappropriate content, highlighting the need for effective detection methods. Previous approaches to jailbreak detection often rely on multiple model inferences or additional models, leading to high computational costs. The proposed Free Jailbreak Detection (FJD) method differs by utilizing the output distribution differences between jailbreak and benign prompts, employing a simple affirmative instruction and temperature scaling to enhance detection without significant resource expenditure. This method is well-motivated as it aims to maintain security while minimizing computational overhead. The paper contributes a novel detection approach that integrates virtual instruction learning, demonstrating through extensive experiments that FJD can effectively identify jailbreak prompts with negligible additional costs during LLM inference, thereby supporting its intended goals of efficient security enhancement.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLM）在面临越狱攻击时的脆弱性，这种攻击可能导致生成不当内容。以往的越狱检测方法通常依赖于多个模型推理或额外模型，导致计算成本高昂。提出的免费越狱检测（FJD）方法通过利用越狱提示和良性提示之间的输出分布差异来进行检测，采用简单的肯定指令和温度缩放来增强检测效果，而不会显著增加计算开销。这种方法的动机明确，因为它旨在提高LLM的安全性，同时保持其效率。该论文贡献了一种新颖的检测方法，能够以最小的额外成本有效识别越狱提示，并在对齐的LLM上进行的广泛实验中表现出强大的性能，从而支持提高LLM应用安全性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Jailbreaking Large Language Models Through Content Concretization</div>
<div class="meta-line">Authors: Johan Wahréus, Ahmed Hussain, Panos Papadimitratos</div>
<div class="meta-line">First: 2025-09-16T10:34:26+00:00 · Latest: 2025-09-16T10:34:26+00:00</div>
<div class="meta-line">Comments: Accepted for presentation in the Conference on Game Theory and AI for Security (GameSec) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.12937v1">Abs</a> · <a href="https://arxiv.org/pdf/2509.12937v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly deployed for task automation and content generation, yet their safety mechanisms remain vulnerable to circumvention through different jailbreaking techniques. In this paper, we introduce \textit{Content Concretization} (CC), a novel jailbreaking technique that iteratively transforms abstract malicious requests into concrete, executable implementations. CC is a two-stage process: first, generating initial LLM responses using lower-tier, less constrained safety filters models, then refining them through higher-tier models that process both the preliminary output and original prompt. We evaluate our technique using 350 cybersecurity-specific prompts, demonstrating substantial improvements in jailbreak Success Rates (SRs), increasing from 7\% (no refinements) to 62\% after three refinement iterations, while maintaining a cost of 7.5\textcent~per prompt. Comparative A/B testing across nine different LLM evaluators confirms that outputs from additional refinement steps are consistently rated as more malicious and technically superior. Moreover, manual code analysis reveals that generated outputs execute with minimal modification, although optimal deployment typically requires target-specific fine-tuning. With eventual improved harmful code generation, these results highlight critical vulnerabilities in current LLM safety frameworks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过内容具体化破解大型语言模型</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地用于任务自动化和内容生成，但其安全机制仍然容易受到不同破解技术的规避。本文介绍了一种新颖的破解技术——内容具体化（CC），该技术通过迭代将抽象的恶意请求转化为具体的可执行实现。CC是一个两阶段的过程：首先，使用低级别、约束较少的安全过滤模型生成初始LLM响应，然后通过处理初步输出和原始提示的高级模型进行精炼。我们使用350个网络安全特定提示评估了我们的技术，显示出破解成功率（SRs）的显著提高，从7%（无精炼）增加到经过三次精炼迭代后的62%，同时每个提示的成本保持在7.5美分。对九个不同LLM评估器的比较A/B测试确认，额外精炼步骤的输出在恶意性和技术优越性上始终被评为更高。此外，手动代码分析显示，生成的输出在最小修改下即可执行，尽管最佳部署通常需要针对特定目标的微调。随着最终有害代码生成的改善，这些结果突显了当前LLM安全框架中的关键漏洞。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of Large Language Models (LLMs) to jailbreaking techniques that can bypass their safety mechanisms, which is a growing concern as these models are increasingly used for automation and content generation. Previous methods for jailbreaking often lacked effectiveness and could be easily thwarted, prompting the need for a more robust approach. The proposed method, Content Concretization (CC), differs by iteratively transforming abstract malicious requests into concrete implementations through a two-stage process that utilizes both lower-tier and higher-tier models for refinement. This approach is well-motivated as it significantly enhances the success rates of jailbreaking, achieving an increase from 7% to 62% after three iterations while maintaining a low cost per prompt. The methodology involves evaluating CC with 350 cybersecurity-specific prompts, and the results indicate that the refined outputs are rated as more malicious and technically superior, revealing critical weaknesses in existing LLM safety frameworks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在安全机制方面的脆弱性，尤其是其易受监狱破解技术影响的问题。以往的方法在将抽象的恶意请求转化为可执行输出方面效果不佳，导致成功率低。提出的内容具体化（CC）方法通过一个两阶段的过程，首先使用低层次模型生成初始LLM响应，然后通过高层次模型进行精炼，有效解决了现有方法的局限性。本文的贡献在于证明CC在经过三次迭代后，监狱破解成功率从7%提高到62%，同时保持低运营成本。该方法通过评估350个特定于网络安全的提示，结果表明，精炼后的输出被评为更具恶意和技术优势，突显了当前LLM安全框架中的关键脆弱性。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Pretraining: Toward the Next Generation of Safe AI</div>
<div class="meta-line">Authors: Pratyush Maini, Sachin Goyal, Dylan Sam, Alex Robey, Yash Savani, Yiding Jiang, Andy Zou, Matt Fredrikson, Zacharcy C. Lipton, J. Zico Kolter</div>
<div class="meta-line">First: 2025-04-23T17:58:08+00:00 · Latest: 2025-09-15T17:51:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.16980v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.16980v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) are increasingly deployed in high-stakes settings, the risk of generating harmful or toxic content remains a central challenge. Post-hoc alignment methods are brittle: once unsafe patterns are learned during pretraining, they are hard to remove. In this work, we present a data-centric pretraining framework that builds safety into the model from the start. Our framework consists of four key steps: (i) Safety Filtering: building a safety classifier to classify webdata into safe and unsafe categories; (ii) Safety Rephrasing: we recontextualize unsafe webdata into safer narratives; (iii) Native Refusal: we develop RefuseWeb and Moral Education pretraining datasets that actively teach model to refuse on unsafe content and the moral reasoning behind it, and (iv) Harmfulness-Tag annotated pretraining: we flag unsafe content during pretraining using a special token, and use it to steer model away from unsafe generations at inference. Our safety-pretrained models reduce attack success rates from 38.8\% to 8.4\% on standard LLM safety benchmarks with no performance degradation on general tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全预训练：迈向下一代安全人工智能</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在高风险环境中的日益部署，生成有害或有毒内容的风险仍然是一个核心挑战。事后对齐方法脆弱：一旦在预训练期间学习到不安全的模式，就很难去除。在这项工作中，我们提出了一种以数据为中心的预训练框架，从一开始就将安全性融入模型。我们的框架包括四个关键步骤：（i）安全过滤：构建安全分类器，将网络数据分类为安全和不安全类别；（ii）安全重述：我们将不安全的网络数据重新语境化为更安全的叙述；（iii）本地拒绝：我们开发了RefuseWeb和道德教育预训练数据集，积极教导模型拒绝不安全内容及其背后的道德推理；（iv）有害标签注释的预训练：我们在预训练期间使用特殊标记标记不安全内容，并利用它在推理时引导模型远离不安全生成。我们的安全预训练模型在标准LLM安全基准上将攻击成功率从38.8%降低到8.4%，且在一般任务上没有性能下降。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical issue of harmful content generation by large language models (LLMs) in high-stakes environments, where traditional post-hoc alignment methods often fail to effectively eliminate learned unsafe patterns. The proposed approach differs by integrating safety measures directly into the pretraining phase through a data-centric framework that includes safety filtering, rephrasing, native refusal, and harmfulness-tag annotation. This method is well-motivated as it aims to proactively instill safety in LLMs rather than attempting to rectify issues after they arise. The contribution of the paper lies in its innovative pretraining framework, which significantly reduces attack success rates from 38.8% to 8.4% on safety benchmarks while maintaining performance on general tasks, thus supporting the goal of creating safer AI systems.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）在高风险应用中生成有害内容的挑战，这在实际应用中带来了重大风险。以往的方法，特别是事后对齐技术，已被证明无效，因为它们难以消除在预训练过程中学习到的不安全模式。所提出的方法引入了一种以数据为中心的预训练框架，从一开始就整合了安全措施，包括安全过滤、重新表述不安全内容、开发道德推理数据集以及在预训练过程中标注有害内容。这种方法显著增强了模型的安全性，在标准安全基准上将攻击成功率从38.8%降低到8.4%，同时保持了通用任务的性能，从而支持了创建更安全的人工智能系统的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs</div>
<div class="meta-line">Authors: Yu Yan, Sheng Sun, Zhe Wang, Yijun Lin, Zenghao Duan, zhifei zheng, Min Liu, Zhiyi yin, Jianping Zhang</div>
<div class="meta-line">First: 2025-08-22T12:41:26+00:00 · Latest: 2025-09-15T03:59:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.16347v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.16347v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the development of Large Language Models (LLMs), numerous efforts have revealed their vulnerabilities to jailbreak attacks. Although these studies have driven the progress in LLMs&#x27; safety alignment, it remains unclear whether LLMs have internalized authentic knowledge to deal with real-world crimes, or are merely forced to simulate toxic language patterns. This ambiguity raises concerns that jailbreak success is often attributable to a hallucination loop between jailbroken LLM and judger LLM. By decoupling the use of jailbreak techniques, we construct knowledge-intensive Q\&amp;A to investigate the misuse threats of LLMs in terms of dangerous knowledge possession, harmful task planning utility, and harmfulness judgment robustness. Experiments reveal a mismatch between jailbreak success rates and harmful knowledge possession in LLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness judgments on toxic language patterns. Our study reveals a gap between existing LLM safety assessments and real-world threat potential.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混淆是最终障碍：重新思考越狱评估与调查大型语言模型的真实滥用威胁</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）的发展，许多努力揭示了它们对越狱攻击的脆弱性。尽管这些研究推动了LLMs安全对齐的进展，但尚不清楚LLMs是否内化了应对现实犯罪的真实知识，或仅仅被迫模拟有毒语言模式。这种模糊性引发了担忧，即越狱成功往往归因于越狱LLM与评判LLM之间的幻觉循环。通过解耦越狱技术的使用，我们构建了知识密集型问答，以调查LLMs在危险知识拥有、有害任务规划效用和有害性判断鲁棒性方面的滥用威胁。实验揭示了越狱成功率与LLMs中有害知识拥有之间的不匹配，现有的LLM作为评判者框架往往将有害性判断锚定在有毒语言模式上。我们的研究揭示了现有LLM安全评估与现实威胁潜力之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the vulnerabilities of Large Language Models (LLMs) to jailbreak attacks, highlighting the uncertainty regarding their ability to genuinely understand and respond to real-world misuse threats. Previous methods primarily focused on safety alignment but often failed to distinguish between authentic knowledge and simulated toxic language patterns, leading to misleading assessments of LLMs&#x27; capabilities. The proposed approach involves constructing knowledge-intensive question-and-answer frameworks that evaluate LLMs based on their possession of dangerous knowledge, harmful task planning, and robustness in harmfulness judgments. This methodology reveals a significant disconnect between the success rates of jailbreak attempts and the actual harmful knowledge held by LLMs, indicating that current safety assessments may not accurately reflect real-world threats. The findings contribute to a deeper understanding of LLM vulnerabilities and suggest the need for more nuanced evaluation methods in assessing their safety.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在越狱攻击中的脆弱性，强调了它们是否真正理解和应对现实世界滥用威胁的能力与仅仅模仿有害语言之间的不确定性。以往的方法主要集中在通过有毒语言模式评估LLM的安全性，这可能无法准确反映其实际知识或威胁潜力。本文提出了一种新方法，解耦越狱技术，采用知识密集型问答评估LLM在处理危险知识和有害任务规划方面的能力。研究结果表明，越狱成功率与有害知识的拥有之间存在显著脱节，表明当前的安全评估可能无法有效捕捉LLM所带来的实际风险。该方法论表明，LLM的有害性判断过于依赖有毒语言模式，从而为理解LLM在实际环境中的安全性提供了更细致的视角。</div>
</details>
</div>
<div class="card">
<div class="title">Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M</div>
<div class="meta-line">Authors: Piyush Pant</div>
<div class="meta-line">First: 2025-09-10T23:22:59+00:00 · Latest: 2025-09-10T23:22:59+00:00</div>
<div class="meta-line">Comments: 17 pages, 3 figures. Code and dataset available at https://github.com/PiyushWithPant/Improving-LLM-Safety-and-Helpfulness-using-SFT-and-DPO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.09055v1">Abs</a> · <a href="https://arxiv.org/pdf/2509.09055v1">PDF</a> · <a href="https://github.com/PiyushWithPant/Improving-LLM-Safety-and-Helpfulness-using-SFT-and-DPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This research investigates the effectiveness of alignment techniques, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a combined SFT+DPO approach on improving the safety and helpfulness of the OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset, we train and evaluate four models: the base OPT350M, an SFT model, a DPO model, and a model trained with both SFT and DPO. We introduce three key evaluation metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined Alignment Score (CAS), all derived from reward model outputs. The results show that while SFT outperforms DPO, The combined SFT+DPO model outperforms all others across all metrics, demonstrating the complementary nature of these techniques. Our findings also highlight challenges posed by noisy data, limited GPU resources, and training constraints. This study offers a comprehensive view of how fine-tuning strategies affect model alignment and provides a foundation for more robust alignment pipelines in future work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用SFT和DPO提高LLM安全性和有用性：关于OPT-350M的研究</div>
<div class="mono" style="margin-top:8px">本研究探讨了对齐技术的有效性，包括监督微调（SFT）、直接偏好优化（DPO）以及结合SFT+DPO的方法，以提高OPT-350M语言模型的安全性和有用性。利用Anthropic Helpful-Harmless RLHF数据集，我们训练和评估了四个模型：基础OPT350M、SFT模型、DPO模型以及一个同时使用SFT和DPO训练的模型。我们引入了三个关键评估指标：无害率（HmR）、有用率（HpR）和综合对齐得分（CAS），均源自奖励模型输出。结果表明，虽然SFT优于DPO，但结合SFT+DPO的模型在所有指标上均优于其他模型，展示了这些技术的互补性。我们的研究还强调了噪声数据、有限的GPU资源和训练限制所带来的挑战。本研究提供了微调策略如何影响模型对齐的全面视角，并为未来更强大的对齐管道奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the growing need for improving the safety and helpfulness of language models, specifically focusing on the OPT-350M model. Previous methods, such as traditional fine-tuning, often faced challenges related to data noise and resource limitations, leading to suboptimal alignment outcomes. The proposed approach combines Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to enhance model performance, effectively addressing the shortcomings of existing techniques. The study contributes by introducing new evaluation metrics—Harmlessness Rate, Helpfulness Rate, and Combined Alignment Score—and demonstrates that the SFT+DPO model significantly outperforms both SFT and DPO alone across all metrics. The methodology involves training and evaluating four models using the Anthropic Helpful-Harmless RLHF dataset, achieving improved alignment and setting a foundation for future research in model safety and helpfulness.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型的安全性和有用性，特别是OPT-350M模型。以往的方法，如传统的微调方法，面临着噪声数据和有限计算资源等问题，这些问题妨碍了有效的模型对齐。提出的方法结合了监督微调（SFT）和直接偏好优化（DPO），利用它们的互补优势，从而改善对齐结果。该研究通过引入新的评估指标——无害率、有用率和综合对齐分数，做出了贡献，同时证明SFT+DPO模型在所有指标上显著优于单独的SFT和DPO。这种方法有效提升了模型在安全性和有用性方面的表现，支持了创建更可靠语言模型的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment</div>
<div class="meta-line">Authors: Gang Cheng, Haibo Jin, Wenbin Zhang, Haohan Wang, Jun Zhuang</div>
<div class="meta-line">First: 2025-09-07T22:35:15+00:00 · Latest: 2025-09-07T22:35:15+00:00</div>
<div class="meta-line">Comments: Preprint, under review. TL;DR: We propose a multi-turn red-teaming framework, RCA, that reveals critical regulatory vulnerabilities in financial LLMs, achieving over 93% attack success on a proposed new benchmark, FIN-Bench</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.10546v1">Abs</a> · <a href="https://arxiv.org/pdf/2509.10546v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly integrated into financial applications, yet existing red-teaming research primarily targets harmful content, largely neglecting regulatory risks. In this work, we aim to investigate the vulnerability of financial LLMs through red-teaming approaches. We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that iteratively conceals regulatory risks to provoke seemingly compliant yet regulatory-violating responses from LLMs. To enable systematic evaluation, we construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA effectively bypasses nine mainstream LLMs, achieving an average attack success rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1. These findings reveal a critical gap in current alignment techniques and underscore the urgent need for stronger moderation mechanisms in financial domains. We hope this work offers practical insights for advancing robust and domain-aware LLM alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过风险隐匿揭示金融领域大型语言模型的脆弱性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地应用于金融领域，但现有的红队研究主要针对有害内容，基本忽视了监管风险。本研究旨在通过红队方法调查金融LLMs的脆弱性。我们引入了风险隐匿攻击（RCA），这是一种新颖的多轮框架，迭代地隐匿监管风险，以引发看似合规但违反监管的LLM响应。为了实现系统评估，我们构建了FIN-Bench，这是一个特定领域的基准，用于评估金融环境中LLM的安全性。在FIN-Bench上的大量实验表明，RCA有效绕过了九种主流LLM，平均攻击成功率（ASR）达到93.18%，其中GPT-4.1的成功率为98.28%，OpenAI o1为97.56%。这些发现揭示了当前对齐技术的关键缺口，并强调了金融领域迫切需要更强的监管机制。我们希望这项工作为推进稳健且领域感知的LLM对齐提供实用见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the increasing integration of Large Language Models (LLMs) in financial applications, highlighting a gap in existing red-teaming efforts that primarily focus on harmful content while overlooking regulatory risks. Previous methods have not adequately assessed these vulnerabilities, leading to a need for a more targeted approach. The proposed Risk-Concealment Attacks (RCA) framework introduces a multi-turn strategy to iteratively conceal regulatory risks, prompting LLMs to generate responses that appear compliant but violate regulations. This work contributes by establishing FIN-Bench, a benchmark for evaluating LLM safety in financial contexts, and demonstrates that RCA can effectively bypass nine mainstream LLMs with an average attack success rate of 93.18%, including 98.28% on GPT-4.1, thereby revealing significant shortcomings in current alignment techniques and emphasizing the necessity for improved moderation in financial applications.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在金融应用中的日益整合，强调现有红队研究主要集中在有害内容上，而忽视了监管风险的缺口。以往的方法未能充分评估这些脆弱性，因此提出了风险隐蔽攻击（RCA），这是一种新颖的多轮框架，旨在迭代隐蔽监管风险，从而引发合规但实际上违反监管的LLMs响应。这种方法具有良好的动机，因为它揭示了当前对齐技术的关键弱点，并强调了在金融领域改善监管的必要性。该方法论包括构建FIN-Bench，这是一个用于评估LLM安全性的特定领域基准，广泛实验表明RCA成功绕过了九种主流LLMs，平均攻击成功率达到93.18%，其中GPT-4.1的成功率为98.28%，这表明所提出的方法有效支持其在金融领域增强LLM对齐的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models</div>
<div class="meta-line">Authors: Youjia Zheng, Mohammad Zandsalimy, Shanu Sushmita</div>
<div class="meta-line">First: 2025-09-05T19:57:38+00:00 · Latest: 2025-09-05T19:57:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.05471v1">Abs</a> · <a href="https://arxiv.org/pdf/2509.05471v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly vulnerable to a sophisticated form of adversarial prompting known as camouflaged jailbreaking. This method embeds malicious intent within seemingly benign language to evade existing safety mechanisms. Unlike overt attacks, these subtle prompts exploit contextual ambiguity and the flexible nature of language, posing significant challenges to current defense systems. This paper investigates the construction and impact of camouflaged jailbreak prompts, emphasizing their deceptive characteristics and the limitations of traditional keyword-based detection methods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts, containing 500 curated examples (400 harmful and 100 benign prompts) designed to rigorously stress-test LLM safety protocols. In addition, we propose a multi-faceted evaluation framework that measures harmfulness across seven dimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards, Harmful Potential, Educational Value, Content Quality, and Compliance Score. Our findings reveal a stark contrast in LLM behavior: while models demonstrate high safety and content quality with benign inputs, they exhibit a significant decline in performance and safety when confronted with camouflaged jailbreak attempts. This disparity underscores a pervasive vulnerability, highlighting the urgent need for more nuanced and adaptive security strategies to ensure the responsible and robust deployment of LLMs in real-world applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面具背后：大型语言模型中伪装越狱的基准测试</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越容易受到一种复杂的对抗性提示形式的攻击，称为伪装越狱。这种方法在看似无害的语言中嵌入恶意意图，以规避现有的安全机制。与明显的攻击不同，这些微妙的提示利用了上下文模糊性和语言的灵活性，对当前的防御系统构成了重大挑战。本文研究了伪装越狱提示的构建及其影响，强调了它们的欺骗特性和传统基于关键词的检测方法的局限性。我们引入了一个新的基准数据集——伪装越狱提示，包含500个经过精心挑选的示例（400个有害和100个无害提示），旨在严格测试LLM的安全协议。此外，我们提出了一个多维评估框架，衡量七个维度的有害性：安全意识、技术可行性、实施保障、有害潜力、教育价值、内容质量和合规评分。我们的研究结果揭示了LLM行为的明显对比：当模型面对无害输入时，表现出高安全性和内容质量，而在面对伪装越狱尝试时，性能和安全性显著下降。这种差异突显了普遍的脆弱性，强调了迫切需要更细致和适应性的安全策略，以确保LLM在现实应用中的负责任和稳健部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the increasing vulnerability of Large Language Models (LLMs) to camouflaged jailbreaking, a sophisticated form of adversarial prompting that embeds malicious intent within seemingly benign language, which traditional keyword-based detection methods fail to adequately address. The proposed approach introduces a novel benchmark dataset, Camouflaged Jailbreak Prompts, consisting of 500 curated examples designed to rigorously test LLM safety protocols, along with a multi-faceted evaluation framework assessing harmfulness across seven dimensions. The findings reveal that while LLMs perform well with benign inputs, they significantly underperform and exhibit reduced safety when faced with camouflaged jailbreak attempts, emphasizing the need for more adaptive security strategies in LLM deployment.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）对伪装越狱攻击的日益脆弱，这是一种将恶意意图嵌入看似无害语言中的复杂对抗性提示技术，从而绕过现有的安全机制。传统的检测方法主要依赖关键词识别，无法有效应对这些微妙的攻击，因为它们具有上下文模糊性。本文的贡献在于引入了一个新的基准数据集——伪装越狱提示，包含500个示例，旨在严格评估LLM的安全协议，并提出一个全面的评估框架，从七个维度评估危害性。研究方法表明，尽管LLMs在处理无害输入时保持高安全性和内容质量，但在面对伪装越狱攻击时，其性能和安全性显著下降，突显出关键脆弱性，并强调了在LLM部署中需要改进的安全策略。</div>
</details>
</div>
<div class="card">
<div class="title">Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning</div>
<div class="meta-line">Authors: Tiansheng Huang, Gautam Bhattacharya, Pratik Joshi, Josh Kimball, Ling Liu</div>
<div class="meta-line">First: 2024-08-18T21:45:03+00:00 · Latest: 2025-09-05T04:54:29+00:00</div>
<div class="meta-line">Comments: Rejected by AAAI25-AIA. Accepted by ICML25. Authors are thankful to the anonymous reviewers from both AAAI25-AIA and ICML25</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2408.09600v3">Abs</a> · <a href="https://arxiv.org/pdf/2408.09600v3">PDF</a> · <a href="https://github.com/git-disl/Antidote">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safety aligned Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- a few harmful data mixed in the fine-tuning dataset can break the LLMs&#x27;s safety alignment. While several defenses have been proposed, our evaluation shows that existing defenses fail \textit{when some specific training hyper-parameters are chosen} -- a large learning rate or a large number of training epochs in the fine-tuning stage can easily invalidate the defense. To this end, we propose Antidote, a post-fine-tuning stage solution, which remains \textbf{\textit{agnostic to the training hyper-parameters in the fine-tuning stage}}. Antidote relies on the philosophy that by removing the harmful parameters, the harmful model can be recovered from the harmful behaviors, regardless of how those harmful parameters are formed in the fine-tuning stage. With this philosophy, we introduce a one-shot pruning stage after harmful fine-tuning to remove the harmful weights that are responsible for the generation of harmful content. Despite its embarrassing simplicity, empirical results show that Antidote can reduce harmful score while maintaining accuracy on downstream tasks. Code is available at https://github.com/git-disl/Antidote.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解药：针对有害微调的大型语言模型的后微调安全对齐</div>
<div class="mono" style="margin-top:8px">安全对齐的大型语言模型（LLMs）易受到有害微调攻击——在微调数据集中混入少量有害数据会破坏LLMs的安全对齐。尽管提出了几种防御措施，但我们的评估表明，现有防御在选择某些特定训练超参数时失效——在微调阶段，较大的学习率或较多的训练轮次会轻易使防御失效。为此，我们提出了解药，一种后微调阶段的解决方案，它对微调阶段的训练超参数保持无关性。解药依赖于这样一种理念：通过去除有害参数，可以从有害行为中恢复有害模型，而不管这些有害参数在微调阶段是如何形成的。基于这一理念，我们在有害微调后引入了一次性剪枝阶段，以去除负责生成有害内容的有害权重。尽管其简单性令人尴尬，但实证结果表明，解药可以在保持下游任务准确性的同时降低有害评分。代码可在 https://github.com/git-disl/Antidote 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of safety-aligned Large Language Models (LLMs) to harmful fine-tuning attacks, where the introduction of a small amount of harmful data can compromise their safety alignment. Previous methods have been ineffective under certain training hyper-parameter conditions, such as high learning rates or extended training epochs, which can invalidate defenses. The proposed approach, Antidote, offers a post-fine-tuning solution that is agnostic to these hyper-parameters, focusing on removing harmful weights to recover the model from harmful behaviors. This methodology introduces a one-shot pruning stage to eliminate harmful parameters while preserving model accuracy on downstream tasks. Empirical results demonstrate that Antidote effectively reduces harmful outputs while maintaining performance, supporting its intended goals.</div>
<div class="mono" style="margin-top:8px">本研究解决了安全对齐的大型语言模型（LLMs）在受到有害微调攻击时的脆弱性，即有害数据的引入可能会破坏其安全对齐。以往的方法在某些训练超参数条件下无效，例如高学习率或延长训练周期，这可能会使防御失效。所提出的方法Antidote提供了一种与这些超参数无关的微调后解决方案，专注于去除导致生成有害内容的有害权重。该方法引入了一种一次性剪枝阶段，以从有害行为中恢复模型。实证结果表明，Antidote能够有效降低有害评分，同时保持下游任务的准确性，支持其预期目标。</div>
</details>
</div>
<div class="card">
<div class="title">Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed and Inappropriate Contexts</div>
<div class="meta-line">Authors: Rushi Wang, Jiateng Liu, Cheng Qian, Yifan Shen, Yanzhou Pan, Zhaozhuo Xu, Ahmed Abbasi, Heng Ji, Denghui Zhang</div>
<div class="meta-line">First: 2025-09-02T00:40:34+00:00 · Latest: 2025-09-02T00:40:34+00:00</div>
<div class="meta-line">Comments: 36 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.04500v1">Abs</a> · <a href="https://arxiv.org/pdf/2509.04500v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Incorporating external context can significantly enhance the response quality of Large Language Models (LLMs). However, real-world contexts often mix relevant information with disproportionate inappropriate content, posing reliability risks. How do LLMs process and prioritize mixed context? To study this, we introduce the Poisoned Context Testbed, pairing queries with real-world contexts containing relevant and inappropriate content. Inspired by associative learning in animals, we adapt the Rescorla-Wagner (RW) model from neuroscience to quantify how competing contextual signals influence LLM outputs. Our adapted model reveals a consistent behavioral pattern: LLMs exhibit a strong tendency to incorporate information that is less prevalent in the context. This susceptibility is harmful in real-world settings, where small amounts of inappropriate content can substantially degrade response quality. Empirical evaluations on our testbed further confirm this vulnerability. To tackle this, we introduce RW-Steering, a two-stage finetuning-based approach that enables the model to internally identify and ignore inappropriate signals. Unlike prior methods that rely on extensive supervision across diverse context mixtures, RW-Steering generalizes robustly across varying proportions of inappropriate content. Experiments show that our best fine-tuned model improves response quality by 39.8% and reverses the undesirable behavior curve, establishing RW-Steering as a robust, generalizable context engineering solution for improving LLM safety in real-world use.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可信度的上下文工程：在混合和不当上下文下的Rescorla-Wagner引导</div>
<div class="mono" style="margin-top:8px">引入外部上下文可以显著提高大型语言模型（LLMs）的响应质量。然而，现实世界的上下文往往将相关信息与不当内容混合，带来可靠性风险。LLMs如何处理和优先考虑混合上下文？为此，我们引入了毒化上下文测试平台，将查询与包含相关和不当内容的现实世界上下文配对。受到动物联想学习的启发，我们将神经科学中的Rescorla-Wagner（RW）模型进行了调整，以量化竞争上下文信号如何影响LLM输出。我们调整后的模型揭示了一种一致的行为模式：LLMs表现出强烈倾向于纳入在上下文中较少出现的信息。这种易感性在现实世界中是有害的，因为少量不当内容会显著降低响应质量。我们在测试平台上的实证评估进一步确认了这一脆弱性。为了解决这个问题，我们引入了RW引导，这是一种基于两阶段微调的方法，使模型能够内部识别和忽略不当信号。与依赖于广泛监督的先前方法不同，RW引导在不同不当内容比例下具有良好的泛化能力。实验表明，我们最佳的微调模型提高了响应质量39.8%，并逆转了不良行为曲线，确立了RW引导作为改善LLM在现实世界使用中安全性的强大、可泛化的上下文工程解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enhancing the response quality of Large Language Models (LLMs) when exposed to mixed contexts that contain both relevant and inappropriate information, which can compromise reliability. Previous methods have struggled with extensive supervision requirements and have not effectively managed the influence of inappropriate content. The proposed approach, RW-Steering, adapts the Rescorla-Wagner model from neuroscience to allow LLMs to internally identify and disregard inappropriate signals, thus improving their robustness across varying context mixtures. The paper contributes a novel two-stage finetuning methodology that demonstrates a 39.8% improvement in response quality, effectively reversing harmful behavioral tendencies and establishing RW-Steering as a viable solution for enhancing LLM safety in practical applications.</div>
<div class="mono" style="margin-top:8px">本研究解决了在处理包含相关和不当内容的混合上下文时，提高大型语言模型（LLMs）可靠性的挑战。以往的方法通常需要大量监督，并且在不同上下文混合中表现不佳，导致不当信号可能降低响应质量。所提出的方法RW-Steering借鉴神经科学中的Rescorla-Wagner模型，使LLMs能够内部识别并忽略不当上下文信号，从而提高响应质量。该方法论涉及在毒性上下文测试平台上进行的两阶段微调过程，结果表明，最佳微调模型在响应质量上提高了39.8%，有效解决了识别出的脆弱性，支持了在实际应用中增强LLM安全性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Unraveling LLM Jailbreaks Through Safety Knowledge Neurons</div>
<div class="meta-line">Authors: Chongwen Zhao, Kaizhu Huang</div>
<div class="meta-line">First: 2025-09-01T17:17:06+00:00 · Latest: 2025-09-01T17:17:06+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.01631v1">Abs</a> · <a href="https://arxiv.org/pdf/2509.01631v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly attracting attention in various applications. Nonetheless, there is a growing concern as some users attempt to exploit these models for malicious purposes, including the synthesis of controlled substances and the propagation of disinformation, a technique known as &quot;Jailbreak.&quot; While some studies have achieved defenses against jailbreak attacks by modifying output distributions or detecting harmful content, the exact rationale still remains elusive. In this work, we present a novel neuron-level interpretability method that focuses on the role of safety-related knowledge neurons. Unlike existing approaches, our method projects the model&#x27;s internal representation into a more consistent and interpretable vocabulary space. We then show that adjusting the activation of safety-related neurons can effectively control the model&#x27;s behavior with a mean ASR higher than 97%. Building on this insight, we propose SafeTuning, a fine-tuning strategy that reinforces safety-critical neurons to improve model robustness against jailbreaks. SafeTuning consistently reduces attack success rates across multiple LLMs and outperforms all four baseline defenses. These findings offer a new perspective on understanding and defending against jailbreak attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过安全知识神经元揭示大型语言模型的越狱</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在各种应用中越来越受到关注。然而，随着一些用户试图利用这些模型进行恶意活动，包括合成受控物质和传播虚假信息，越狱技术的担忧日益加剧。尽管一些研究通过修改输出分布或检测有害内容来防御越狱攻击，但其确切原理仍然难以捉摸。在本研究中，我们提出了一种新颖的神经元级可解释性方法，专注于安全相关知识神经元的作用。与现有方法不同，我们的方法将模型的内部表示投影到一个更一致和可解释的词汇空间。然后，我们展示了调整安全相关神经元的激活可以有效控制模型的行为，平均攻击成功率（ASR）超过97%。基于这一见解，我们提出了SafeTuning，一种强化安全关键神经元的微调策略，以提高模型对越狱的鲁棒性。SafeTuning在多个LLM上持续降低攻击成功率，并超越所有四个基线防御。这些发现为理解和防御越狱攻击提供了新的视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the increasing exploitation of Large Language Models (LLMs) for malicious purposes, particularly through techniques known as &#x27;Jailbreak&#x27; that enable harmful outputs. Previous methods aimed at defending against these attacks often involved modifying output distributions or detecting harmful content, but they lacked clarity on the underlying mechanisms. This paper introduces a novel neuron-level interpretability method that emphasizes safety-related knowledge neurons, projecting the model&#x27;s internal representations into a more interpretable vocabulary space. The proposed approach, SafeTuning, fine-tunes these safety-critical neurons to enhance model robustness, achieving a mean attack success rate (ASR) of over 97% and consistently reducing attack success rates across multiple LLMs, thus outperforming existing defenses and contributing significantly to the understanding and mitigation of jailbreak attacks.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）被恶意利用的日益严重的问题，特别是通过被称为“越狱”的技术，这对安全构成了重大威胁。以往的方法试图通过修改输出分布或检测有害内容来防御这些攻击，但往往缺乏清晰的理由和有效性。本文提出了一种新的神经元级可解释性方法，强调与安全相关的知识神经元，为模型的内部表示提供了更一致和可解释的词汇空间。所提出的方法SafeTuning对这些安全关键神经元进行微调，达到超过97%的平均攻击成功率（ASR），并显著降低多个LLM的攻击成功率，超越现有防御。这一贡献增强了对越狱漏洞的理解，并提供了一种改善模型安全性的强大策略。</div>
</details>
</div>
<div class="card">
<div class="title">Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection</div>
<div class="meta-line">Authors: Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, George Turkiyyah, Bernard Ghanem</div>
<div class="meta-line">First: 2025-08-28T13:22:33+00:00 · Latest: 2025-08-28T13:22:33+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.20766v1">Abs</a> · <a href="https://arxiv.org/pdf/2508.20766v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box method that amplifies a model&#x27;s safety alignment by permanently steering its activations toward the refusal-mediating subspace. ROSI operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that ROSI consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align &#x27;uncensored&#x27; models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扭转咒语：通过秩一安全注入实现轻量级对齐放大</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）中的安全对齐通常涉及调解内部表示以拒绝有害请求。最近的研究表明，这些安全机制可以通过消融或移除模型内特定的表示方向来绕过。在本文中，我们提出了相反的方法：秩一安全注入（ROSI），这是一种白盒方法，通过永久性地将模型的激活引导到拒绝调解子空间来放大模型的安全对齐。ROSI作为一种简单的、无需微调的秩一权重修改，应用于所有残差流写矩阵。所需的安全方向可以从一小组有害和无害的指令对中计算得出。我们展示了ROSI在评估Llama Guard 3时始终提高安全拒绝率，同时在MMLU、HellaSwag和Arc等标准基准上保持模型的效用。此外，我们还展示了ROSI可以通过放大自身潜在安全方向来重新对齐“未审查”模型，证明其作为有效的最后一公里安全程序的实用性。我们的结果表明，针对性的、可解释的权重引导是一种廉价而有效的机制，可以改善LLM的安全性，补充更资源密集的微调范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of safety alignment in Large Language Models (LLMs), where existing methods often fail due to the ability of harmful requests to bypass safety mechanisms by altering internal representations. The proposed approach, Rank-One Safety Injection (ROSI), differs from past methods by amplifying safety alignment instead of attempting to mediate harmful requests, thus providing a more robust solution. ROSI is a lightweight, fine-tuning-free method that modifies rank-one weights in residual stream write matrices, utilizing a small set of harmful and harmless instruction pairs to compute necessary safety directions. The paper demonstrates that ROSI significantly increases safety refusal rates as evaluated by Llama Guard 3, while maintaining model performance on standard benchmarks like MMLU, HellaSwag, and Arc, indicating its effectiveness as a last-mile safety enhancement for LLMs.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）中安全对齐的挑战，现有方法常因有害请求能够通过改变内部表示而绕过安全机制而失效。所提出的方法，即秩一安全注入（ROSI），与过去的方法不同，它通过增强安全对齐而非试图调解有害请求，从而解决了以往技术的局限性。ROSI是一种轻量级的、无需微调的方法，通过修改残差流写矩阵中的秩一权重，利用一小组有害和无害指令对来计算所需的安全方向。论文表明，ROSI显著提高了Llama Guard 3评估的安全拒绝率，同时在MMLU、HellaSwag和Arc等标准基准上保持了模型性能，表明它有效增强了LLM的安全性，而不需要传统微调方法的资源需求。</div>
</details>
</div>
<div class="card">
<div class="title">LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models</div>
<div class="meta-line">Authors: Zhiyuan Ning, Tianle Gu, Jiaxin Song, Shixin Hong, Lingyu Li, Huacan Liu, Jie Li, Yixu Wang, Meng Lingyu, Yan Teng, Yingchun Wang</div>
<div class="meta-line">First: 2025-08-18T08:59:01+00:00 · Latest: 2025-08-27T12:21:20+00:00</div>
<div class="meta-line">Comments: 7pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.12733v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.12733v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread adoption and increasing prominence of large language models (LLMs) in global technologies necessitate a rigorous focus on ensuring their safety across a diverse range of linguistic and cultural contexts. The lack of a comprehensive evaluation and diverse data in existing multilingual safety evaluations for LLMs limits their effectiveness, hindering the development of robust multilingual safety alignment. To address this critical gap, we introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted with meticulous attention to linguistic authenticity. The LinguaSafe dataset comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated using a combination of translated, transcreated, and natively-sourced data, our dataset addresses the critical need for multilingual safety evaluations of LLMs, filling the void in the safety evaluation of LLMs across diverse under-represented languages from Hungarian to Malay. LinguaSafe presents a multidimensional and fine-grained evaluation framework, with direct and indirect safety assessments, including further evaluations for oversensitivity. The results of safety and helpfulness evaluations vary significantly across different domains and different languages, even in languages with similar resource levels. Our benchmark provides a comprehensive suite of metrics for in-depth safety evaluation, underscoring the critical importance of thoroughly assessing multilingual safety in LLMs to achieve more balanced safety alignment. Our dataset and code are released to the public to facilitate further research in the field of multilingual LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LinguaSafe：大型语言模型的综合多语言安全基准</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在全球技术中的广泛应用和日益重要性，要求我们在多样的语言和文化背景下严格关注其安全性。现有的多语言安全评估缺乏全面的评估和多样的数据，限制了其有效性，阻碍了强大多语言安全对齐的发展。为了解决这一关键问题，我们推出了LinguaSafe，这是一个经过精心设计的综合多语言安全基准，注重语言的真实性。LinguaSafe数据集包含12种语言的45,000条条目，从匈牙利语到马来语。我们的数据集结合了翻译、再创作和本土数据，满足了对LLMs多语言安全评估的迫切需求，填补了在匈牙利语到马来语等多种代表性不足语言的安全评估空白。LinguaSafe提供了一个多维度和细致的评估框架，包括直接和间接的安全评估，以及对过度敏感性的进一步评估。安全性和有用性评估的结果在不同领域和不同语言之间差异显著，即使在资源水平相似的语言中也是如此。我们的基准提供了一套全面的指标，用于深入的安全评估，强调了在LLMs中彻底评估多语言安全以实现更平衡的安全对齐的重要性。我们的数据集和代码已向公众发布，以促进多语言LLM安全领域的进一步研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the urgent need for effective safety evaluations of large language models (LLMs) across various linguistic and cultural contexts, as existing multilingual safety assessments are limited by a lack of comprehensive data and evaluation frameworks. Previous methods have failed to provide robust multilingual safety alignment due to insufficient diversity in data and evaluation metrics. The proposed approach, LinguaSafe, introduces a comprehensive multilingual safety benchmark that includes 45,000 entries in 12 languages, utilizing a mix of translated, transcreated, and natively-sourced data to ensure linguistic authenticity. This benchmark offers a multidimensional evaluation framework that assesses both direct and indirect safety, revealing significant variations in safety and helpfulness across different languages and domains. The methodology not only fills a critical gap in multilingual safety evaluations but also supports the goal of achieving balanced safety alignment in LLMs, as evidenced by its detailed metrics and public availability for further research.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）在不同语言和文化背景下的安全评估需求，指出现有多语言安全评估因缺乏全面和多样化的数据而存在的局限性。以往的方法未能充分捕捉不同语言的细微差别，导致LLMs的安全对齐效果不佳。提出的LinguaSafe方法引入了一个强大的多语言安全基准，包含12种语言的45,000条数据，采用翻译、再创作和本土数据相结合的方式，确保语言的真实性。该基准提供了一个多维度的评估框架，包括直接和间接的安全评估，显示出不同领域和语言之间的安全性和有效性存在显著差异。研究结果强调了对多语言安全评估的全面性需求，以实现LLMs的平衡安全对齐，并将数据集和代码公开，以支持该领域的进一步研究。</div>
</details>
</div>
<div class="card">
<div class="title">Prefill-level Jailbreak: A Black-Box Risk Analysis of Large Language Models</div>
<div class="meta-line">Authors: Yakai Li, Jiekang Hu, Weiduan Sang, Luping Ma, Dongsheng Nie, Weijuan Zhang, Aimin Yu, Yi Su, Qingjia Huang, Qihang Zhou</div>
<div class="meta-line">First: 2025-04-28T07:38:43+00:00 · Latest: 2025-08-25T20:17:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.21038v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.21038v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models face security threats from jailbreak attacks. Existing research has predominantly focused on prompt-level attacks while largely ignoring the underexplored attack surface of user-controlled response prefilling. This functionality allows an attacker to dictate the beginning of a model&#x27;s output, thereby shifting the attack paradigm from persuasion to direct state manipulation.In this paper, we present a systematic black-box security analysis of prefill-level jailbreak attacks. We categorize these new attacks and evaluate their effectiveness across fourteen language models. Our experiments show that prefill-level attacks achieve high success rates, with adaptive methods exceeding 99% on several models. Token-level probability analysis reveals that these attacks work through initial-state manipulation by changing the first-token probability from refusal to compliance.Furthermore, we show that prefill-level jailbreak can act as effective enhancers, increasing the success of existing prompt-level attacks by 10 to 15 percentage points. Our evaluation of several defense strategies indicates that conventional content filters offer limited protection. We find that a detection method focusing on the manipulative relationship between the prompt and the prefill is more effective. Our findings reveal a gap in current LLM safety alignment and highlight the need to address the prefill attack surface in future safety training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>预填级越狱：大型语言模型的黑箱风险分析</div>
<div class="mono" style="margin-top:8px">大型语言模型面临来自越狱攻击的安全威胁。现有研究主要集中在提示级攻击上，而在用户控制的响应预填充这一尚未充分探索的攻击面上则大多被忽视。该功能允许攻击者决定模型输出的开头，从而将攻击范式从说服转变为直接状态操控。本文对预填级越狱攻击进行了系统的黑箱安全分析。我们对这些新攻击进行了分类，并评估了它们在十四种语言模型上的有效性。实验表明，预填级攻击的成功率很高，适应性方法在多个模型上超过99%。标记级概率分析显示，这些攻击通过改变首标记的拒绝概率为合规来实现初始状态操控。此外，我们还表明，预填级越狱可以作为有效的增强器，将现有提示级攻击的成功率提高10到15个百分点。我们对几种防御策略的评估表明，传统内容过滤器提供的保护有限。我们发现，关注提示与预填之间操控关系的检测方法更为有效。我们的研究揭示了当前大型语言模型安全对齐的缺口，并强调了在未来安全训练中需要解决预填攻击面的问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the security vulnerabilities of Large Language Models (LLMs) to jailbreak attacks, highlighting a significant gap in existing research that primarily focuses on prompt-level attacks while neglecting the prefill-level attack surface. Previous methods have not adequately explored how user-controlled response prefilling can manipulate model outputs, leading to a need for a new approach that shifts the attack paradigm from persuasion to direct state manipulation. The paper contributes by systematically analyzing prefill-level jailbreak attacks across fourteen language models, demonstrating that these attacks can achieve success rates exceeding 99% and enhance existing prompt-level attacks by 10 to 15 percentage points. The proposed methodology includes a black-box security analysis and token-level probability assessments, revealing that initial-state manipulation is key to the effectiveness of these attacks. The findings underscore the limitations of conventional defense strategies and advocate for a detection method that focuses on the relationship between prompts and prefills, indicating a critical need for improved safety measures in LLMs.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在监狱攻击方面的安全漏洞，主要研究集中在提示级攻击上，而忽视了用户控制的响应预填充所带来的重大威胁。以往的方法主要关注提示级攻击，效果和范围有限。本文提出了一种新方法，系统分析预填充级监狱攻击，证明这些攻击可以更直接有效地操控模型输出。研究对这些攻击进行了分类，并评估了它们在十四种语言模型上的成功率，结果显示，适应性方法的预填充级攻击成功率超过99%，并且可以将现有提示级攻击的成功率提高10到15个百分点。研究结果强调了传统防御策略的不足，并提倡一种针对提示与预填充响应之间关系的检测方法，突显了LLM安全对齐中的关键缺口，以及未来研究需要解决这一脆弱性的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation</div>
<div class="meta-line">Authors: Jun Zhuang, Haibo Jin, Ye Zhang, Zhengjian Kang, Wenbin Zhang, Gaby G. Dagher, Haohan Wang</div>
<div class="meta-line">Venue: EMNLP</div>
<div class="meta-line">First: 2025-05-24T06:47:32+00:00 · Latest: 2025-08-25T00:27:19+00:00</div>
<div class="meta-line">Comments: Accepted for EMNLP&#x27;25 Findings. TL;DR: We propose a new two-stage intent-based prompt-refinement framework, IntentPrompt, that aims to explore the vulnerability of LLMs&#x27; content moderation guardrails by refining prompts into benign-looking declarative forms via intent manipulation for red-teaming purposes</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18556v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.18556v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Intent detection, a core component of natural language understanding, has considerably evolved as a crucial mechanism in safeguarding large language models (LLMs). While prior work has applied intent detection to enhance LLMs&#x27; moderation guardrails, showing a significant success against content-level jailbreaks, the robustness of these intent-aware guardrails under malicious manipulations remains under-explored. In this work, we investigate the vulnerability of intent-aware guardrails and demonstrate that LLMs exhibit implicit intent detection capabilities. We propose a two-stage intent-based prompt-refinement framework, IntentPrompt, that first transforms harmful inquiries into structured outlines and further reframes them into declarative-style narratives by iteratively optimizing prompts via feedback loops to enhance jailbreak success for red-teaming purposes. Extensive experiments across four public benchmarks and various black-box LLMs indicate that our framework consistently outperforms several cutting-edge jailbreak methods and evades even advanced Intent Analysis (IA) and Chain-of-Thought (CoT)-based defenses. Specifically, our &quot;FSTR+SPIN&quot; variant achieves attack success rates ranging from 88.25% to 96.54% against CoT-based defenses on the o1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based defenses. These findings highlight a critical weakness in LLMs&#x27; safety mechanisms and suggest that intent manipulation poses a growing challenge to content moderation guardrails.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过意图操控探索大型语言模型内容审核护栏的脆弱性</div>
<div class="mono" style="margin-top:8px">意图检测是自然语言理解的核心组成部分，已显著发展为保护大型语言模型（LLMs）的关键机制。尽管之前的工作已将意图检测应用于增强LLMs的审核护栏，并在内容级别的越狱攻击中取得了显著成功，但这些意图感知护栏在恶意操控下的鲁棒性仍然未被充分探索。在本研究中，我们调查了意图感知护栏的脆弱性，并证明LLMs表现出隐式意图检测能力。我们提出了一种两阶段的基于意图的提示优化框架IntentPrompt，首先将有害查询转化为结构化大纲，并通过反馈循环迭代优化提示，将其进一步重构为声明式叙述，以增强越狱成功率，服务于红队目的。针对四个公共基准和多种黑箱LLMs的广泛实验表明，我们的框架始终优于几种最先进的越狱方法，并成功规避了先进的意图分析（IA）和思维链（CoT）防御。具体而言，我们的“FSTR+SPIN”变体在o1模型上对CoT防御的攻击成功率范围为88.25%至96.54%，在GPT-4o模型上对IA防御的攻击成功率范围为86.75%至97.12%。这些发现突显了LLMs安全机制的关键弱点，并表明意图操控对内容审核护栏构成日益严峻的挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the vulnerability of content moderation guardrails in large language models (LLMs), particularly focusing on intent detection, which is essential for natural language understanding. Previous methods have successfully applied intent detection to improve moderation guardrails but have not thoroughly examined their robustness against malicious intent manipulations. The proposed approach, IntentPrompt, differs by introducing a two-stage prompt-refinement framework that transforms harmful inquiries into structured outlines and reframes them into declarative narratives, thereby enhancing the effectiveness of jailbreak attempts. The contribution of this research lies in demonstrating the implicit intent detection capabilities of LLMs and revealing critical weaknesses in their safety mechanisms. Through extensive experiments on four public benchmarks, the proposed method outperformed existing jailbreak techniques, achieving attack success rates between 88.25% and 97.12% against various defenses, indicating a significant challenge for content moderation systems.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）内容审查防护机制的脆弱性，特别关注意图检测，这是自然语言理解的核心组成部分。以往的方法成功地应用意图检测来改善审查，但尚未充分研究这些系统在恶意意图操控下的稳健性。所提出的方法IntentPrompt通过引入一个两阶段框架来优化提示，从而增强越狱尝试的有效性，解决了现有方法的局限性。本文的贡献在于揭示了LLMs安全机制的重大弱点，并展示了意图操控如何有效挑战这些防护机制。通过在四个公共基准上进行广泛实验，该方法实现了高达88.25%至97.12%的攻击成功率，表明其性能支持揭示LLMs内容审查能力中的脆弱性这一目标。</div>
</details>
</div>
<div class="card">
<div class="title">SafeLLM: Unlearning Harmful Outputs from Large Language Models against Jailbreak Attacks</div>
<div class="meta-line">Authors: Xiangman Li, Xiaodong Wu, Qi Li, Jianbing Ni, Rongxing Lu</div>
<div class="meta-line">First: 2025-08-21T02:39:14+00:00 · Latest: 2025-08-21T02:39:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.15182v1">Abs</a> · <a href="https://arxiv.org/pdf/2508.15182v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Jailbreak attacks pose a serious threat to the safety of Large Language Models (LLMs) by crafting adversarial prompts that bypass alignment mechanisms, causing the models to produce harmful, restricted, or biased content. In this paper, we propose SafeLLM, a novel unlearning-based defense framework that unlearn the harmful knowledge from LLMs while preserving linguistic fluency and general capabilities. SafeLLM employs a three-stage pipeline: (1) dynamic unsafe output detection using a hybrid approach that integrates external classifiers with model-internal evaluations; (2) token-level harmful content tracing through feedforward network (FFN) activations to localize harmful knowledge; and (3) constrained optimization to suppress unsafe behavior without degrading overall model quality. SafeLLM achieves targeted and irreversible forgetting by identifying and neutralizing FFN substructures responsible for harmful generation pathways. Extensive experiments on prominent LLMs (Vicuna, LLaMA, and GPT-J) across multiple jailbreak benchmarks show that SafeLLM substantially reduces attack success rates while maintaining high general-purpose performance. Compared to standard defense methods such as supervised fine-tuning and direct preference optimization, SafeLLM offers stronger safety guarantees, more precise control over harmful behavior, and greater robustness to unseen attacks. Moreover, SafeLLM maintains the general performance after the harmful knowledge unlearned. These results highlight unlearning as a promising direction for scalable and effective LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeLLM：针对越狱攻击从大型语言模型中去除有害输出</div>
<div class="mono" style="margin-top:8px">越狱攻击通过构造对抗性提示绕过对齐机制，对大型语言模型（LLMs）的安全构成严重威胁，导致模型生成有害、受限或偏见的内容。本文提出了SafeLLM，一种新颖的基于去学习的防御框架，旨在去除LLMs中的有害知识，同时保持语言流畅性和一般能力。SafeLLM采用三阶段流程：（1）使用混合方法进行动态不安全输出检测，将外部分类器与模型内部评估相结合；（2）通过前馈网络（FFN）激活进行标记级有害内容追踪，以定位有害知识；（3）约束优化以抑制不安全行为而不降低整体模型质量。SafeLLM通过识别和中和负责有害生成路径的FFN子结构，实现了有针对性和不可逆转的遗忘。在多个越狱基准测试中，对知名LLMs（Vicuna、LLaMA和GPT-J）进行的广泛实验表明，SafeLLM显著降低了攻击成功率，同时保持高通用性能。与标准防御方法（如监督微调和直接偏好优化）相比，SafeLLM提供了更强的安全保障、更精确的有害行为控制以及对未见攻击的更大鲁棒性。此外，SafeLLM在去除有害知识后保持了整体性能。这些结果突显了去学习作为可扩展和有效的LLM安全方向的前景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the significant threat posed by jailbreak attacks on Large Language Models (LLMs), which exploit adversarial prompts to bypass safety mechanisms and generate harmful content. Previous methods, such as supervised fine-tuning and direct preference optimization, have limitations in effectively mitigating these attacks while maintaining model performance. The proposed SafeLLM framework introduces a novel unlearning-based approach that detects unsafe outputs, traces harmful content at the token level, and employs constrained optimization to suppress unsafe behavior without degrading overall model quality. This methodology demonstrates that SafeLLM can achieve targeted and irreversible forgetting of harmful knowledge while preserving linguistic fluency and general capabilities. Experimental results on various LLMs indicate that SafeLLM significantly reduces attack success rates and maintains high general-purpose performance, thus supporting its goals of enhancing LLM safety and robustness against unseen attacks.</div>
<div class="mono" style="margin-top:8px">本研究解决了监狱攻击对大型语言模型（LLMs）构成的重大威胁，这些攻击利用对抗性提示引发有害输出。以往的方法，如监督微调和直接偏好优化，在有效减轻这些攻击的同时保持模型性能方面存在局限性。提出的SafeLLM框架引入了一种基于遗忘的方法，动态检测不安全输出，在标记级别追踪有害内容，并采用约束优化来抑制不安全行为，而不降低整体模型质量。这种方法在Vicuna、LLaMA和GPT-J等知名LLMs上显著降低了攻击成功率，同时保持了高通用性能，从而提供了比现有方法更强的安全保证和对未知攻击的更大鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">CCFC: Core &amp; Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection</div>
<div class="meta-line">Authors: Jiaming Hu, Haoyu Wang, Debarghya Mukherjee, Ioannis Ch. Paschalidis</div>
<div class="meta-line">First: 2025-08-19T04:17:21+00:00 · Latest: 2025-08-19T04:17:21+00:00</div>
<div class="meta-line">Comments: 11 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.14128v1">Abs</a> · <a href="https://arxiv.org/pdf/2508.14128v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Jailbreak attacks pose a serious challenge to the safe deployment of large language models (LLMs). We introduce CCFC (Core &amp; Core-Full-Core), a dual-track, prompt-level defense framework designed to mitigate LLMs&#x27; vulnerabilities from prompt injection and structure-aware jailbreak attacks. CCFC operates by first isolating the semantic core of a user query via few-shot prompting, and then evaluating the query using two complementary tracks: a core-only track to ignore adversarial distractions (e.g., toxic suffixes or prefix injections), and a core-full-core (CFC) track to disrupt the structural patterns exploited by gradient-based or edit-based attacks. The final response is selected based on a safety consistency check across both tracks, ensuring robustness without compromising on response quality. We demonstrate that CCFC cuts attack success rates by 50-75% versus state-of-the-art defenses against strong adversaries (e.g., DeepInception, GCG), without sacrificing fidelity on benign queries. Our method consistently outperforms state-of-the-art prompt-level defenses, offering a practical and effective solution for safer LLM deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CCFC：用于LLM越狱保护的核心与全核心双轨防御</div>
<div class="mono" style="margin-top:8px">越狱攻击对大型语言模型（LLMs）的安全部署构成了严重挑战。我们介绍了CCFC（核心与全核心），这是一种双轨、提示级防御框架，旨在减轻LLMs在提示注入和结构感知越狱攻击中的脆弱性。CCFC首先通过少量提示隔离用户查询的语义核心，然后使用两个互补轨道评估查询：一个仅核心轨道以忽略对抗性干扰（例如，有毒后缀或前缀注入），以及一个核心-全核心（CFC）轨道以破坏被基于梯度或编辑的攻击利用的结构模式。最终响应基于两个轨道的安全一致性检查进行选择，确保在不妥协响应质量的情况下保持鲁棒性。我们证明CCFC将针对强大对手（例如，DeepInception，GCG）的攻击成功率降低了50-75%，而不牺牲对良性查询的保真度。我们的方法始终优于最先进的提示级防御，提供了一种更安全的LLM部署的实用有效解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the significant challenge posed by jailbreak attacks on large language models (LLMs), which threaten their safe deployment. Previous methods have struggled with effectively mitigating vulnerabilities to prompt injection and structure-aware attacks, often failing to maintain response quality while ensuring safety. The proposed CCFC (Core &amp; Core-Full-Core) framework introduces a dual-track defense mechanism that isolates the semantic core of user queries and evaluates them through two complementary tracks to enhance robustness against adversarial distractions and structural exploitation. This approach is well-motivated as it significantly reduces attack success rates by 50-75% compared to existing state-of-the-art defenses while preserving the fidelity of benign queries. The methodology demonstrates superior performance in safeguarding LLMs, making it a practical solution for enhancing their safety during deployment.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）面临的越狱攻击这一重大挑战，旨在确保其安全部署。以往的方法在有效减轻对提示注入和结构感知攻击的脆弱性方面存在困难，往往牺牲响应质量或未能提供稳健的防御。提出的CCFC（核心与全核心）框架引入了一种双轨提示级防御，通过隔离用户查询的语义核心并通过两个互补轨道进行评估，从而增强对对抗性干扰和结构利用的抵御能力。这种方法具有良好的动机，因为它将攻击成功率降低了50-75%，相比于现有的最先进防御，同时保持了对良性查询的高保真度。该方法展示了改善LLM部署安全性的实用解决方案，性能优于以往的防御措施。</div>
</details>
</div>
<div class="card">
<div class="title">Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation</div>
<div class="meta-line">Authors: Yutong He, Alexander Robey, Naoki Murata, Yiding Jiang, Joshua Nathaniel Williams, George J. Pappas, Hamed Hassani, Yuki Mitsufuji, Ruslan Salakhutdinov, J. Zico Kolter</div>
<div class="meta-line">Venue: Transactions on Machine Learning Research (TMLR), 2025. ISSN 2835-8856</div>
<div class="meta-line">First: 2024-03-28T02:35:53+00:00 · Latest: 2025-08-16T03:22:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.19103v4">Abs</a> · <a href="https://arxiv.org/pdf/2403.19103v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prompt engineering is an effective but labor-intensive way to control text-to-image (T2I) generative models. Its time-intensive nature and complexity have spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, or produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically produces human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompt distribution built upon the reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, styles, and images across multiple T2I models, including Stable Diffusion, DALL-E, and Midjourney.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于个性化文本到图像生成的自动化黑箱提示工程</div>
<div class="mono" style="margin-top:8px">提示工程是一种有效但劳动密集的控制文本到图像（T2I）生成模型的方法。其时间密集性和复杂性促使了自动提示生成算法的发展。然而，这些方法通常在T2I模型之间的可转移性方面存在困难，需要对底层模型的白箱访问，或生成非直观的提示。在本研究中，我们介绍了PRISM，这是一种自动生成可人类解释和可转移提示的算法，能够在仅有黑箱访问T2I模型的情况下有效生成所需概念。受到大型语言模型（LLM）越狱的启发，PRISM利用LLM的上下文学习能力，迭代地优化基于参考图像构建的候选提示分布。我们的实验展示了PRISM在多个T2I模型（包括Stable Diffusion、DALL-E和Midjourney）中生成准确提示的多样性和有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of prompt engineering in text-to-image (T2I) generative models, which is traditionally labor-intensive and complex, leading to the development of automated algorithms that often lack transferability, require white-box access, or yield non-intuitive prompts. The proposed method, PRISM, distinguishes itself by generating human-interpretable and transferable prompts using only black-box access to T2I models, thus overcoming the limitations of existing approaches. This paper contributes a novel algorithm that utilizes the in-context learning capabilities of large language models (LLMs) to iteratively refine prompts based on reference images. The methodology involves generating prompts that effectively convey desired concepts across various T2I models, including Stable Diffusion, DALL-E, and Midjourney, with experimental results demonstrating PRISM&#x27;s versatility and effectiveness in producing accurate prompts for diverse objects and styles, thereby supporting its intended goals.</div>
<div class="mono" style="margin-top:8px">本研究解决了文本到图像（T2I）生成模型中提示工程的挑战，这一过程传统上劳动密集且缺乏可转移性，通常需要对模型的白盒访问。以往的方法在这些限制上表现不佳，导致生成的提示不够直观，且无法在不同模型间良好泛化。所提出的方法PRISM通过仅使用黑盒访问生成可理解且可转移的提示，提供了解决方案，灵感来源于大型语言模型越狱技术。本文贡献了一种新颖的算法，利用上下文学习基于参考图像细化提示分布。实验结果表明，PRISM能够有效生成适用于多种对象、风格和图像的准确提示，展示了其多功能性并支持其预期目标。</div>
</details>
</div>
<div class="card">
<div class="title">POEX: Towards Policy Executable Jailbreak Attacks Against the LLM-based Robots</div>
<div class="meta-line">Authors: Xuancun Lu, Zhengxian Huang, Xinfeng Li, Chi Zhang, Xiaoyu ji, Wenyuan Xu</div>
<div class="meta-line">First: 2024-12-21T13:58:27+00:00 · Latest: 2025-08-11T08:29:19+00:00</div>
<div class="meta-line">Comments: Homepage: https://poex-jailbreak.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.16633v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.16633v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://poex-jailbreak.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The integration of LLMs into robots has witnessed significant growth, where LLMs can convert instructions into executable robot policies. However, the inherent vulnerability of LLMs to jailbreak attacks brings critical security risks from the digital domain to the physical world. An attacked LLM-based robot could execute harmful policies and cause physical harm. In this paper, we investigate the feasibility and rationale of jailbreak attacks against LLM-based robots and answer three research questions: (1) How applicable are existing LLM jailbreak attacks against LLM-based robots? (2) What unique challenges arise if they are not directly applicable? (3) How to defend against such jailbreak attacks? To this end, we first construct a &quot;human-object-environment&quot; robot risks-oriented Harmful-RLbench and then conduct a measurement study on LLM-based robot systems. Our findings conclude that traditional LLM jailbreak attacks are inapplicable in robot scenarios, and we identify two unique challenges: determining policy-executable optimization directions and accurately evaluating robot-jailbroken policies. To enable a more thorough security analysis, we introduce POEX (POlicy EXecutable) jailbreak, a red-teaming framework that induces harmful yet executable policy to jailbreak LLM-based robots. POEX incorporates hidden layer gradient optimization to guarantee jailbreak success and policy execution as well as a multi-agent evaluator to accurately assess the practical executability of policies. Experiments conducted on the real-world robotic systems and in simulation demonstrate the efficacy of POEX, highlighting critical security vulnerabilities and its transferability across LLMs. Finally, we propose prompt-based and model-based defenses to mitigate attacks. Our findings underscore the urgent need for security measures to ensure the safe deployment of LLM-based robots in critical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POEX：针对基于LLM的机器人政策可执行越狱攻击的研究</div>
<div class="mono" style="margin-top:8px">将LLM集成到机器人中已显著增长，LLM可以将指令转换为可执行的机器人政策。然而，LLM固有的越狱攻击脆弱性将数字领域的关键安全风险带入物理世界。被攻击的基于LLM的机器人可能执行有害政策并造成物理伤害。本文研究了针对基于LLM的机器人的越狱攻击的可行性和理由，并回答了三个研究问题：（1）现有的LLM越狱攻击对基于LLM的机器人适用性如何？（2）如果不直接适用，会出现什么独特挑战？（3）如何防御此类越狱攻击？为此，我们首先构建了一个以“人-物-环境”为导向的有害RLbench，然后对基于LLM的机器人系统进行测量研究。我们的研究结果表明，传统的LLM越狱攻击在机器人场景中不适用，我们识别出两个独特挑战：确定政策可执行的优化方向和准确评估机器人越狱政策。为了进行更全面的安全分析，我们引入了POEX（政策可执行）越狱，这是一个红队框架，诱导有害但可执行的政策以越狱基于LLM的机器人。POEX结合了隐藏层梯度优化，以确保越狱成功和政策执行，以及一个多智能体评估器，以准确评估政策的实际可执行性。在真实机器人系统和模拟中的实验表明了POEX的有效性，突显了关键的安全漏洞及其在LLM之间的可转移性。最后，我们提出了基于提示和基于模型的防御措施以减轻攻击。我们的研究结果强调了确保基于LLM的机器人在关键应用中安全部署的紧迫性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing integration of large language models (LLMs) into robotic systems, which poses significant security risks due to their vulnerability to jailbreak attacks that could lead to harmful actions in the physical world. Previous methods for executing jailbreak attacks on LLMs are found to be ineffective in robotic contexts, primarily due to unique challenges such as the need for policy-executable optimization and accurate evaluation of robot-jailbroken policies. This paper introduces POEX (Policy Executable jailbreak), a novel framework that utilizes hidden layer gradient optimization and a multi-agent evaluator to successfully induce harmful yet executable policies for LLM-based robots. The methodology includes constructing a risk-oriented Harmful-RLbench and conducting experiments on both real-world robotic systems and simulations, demonstrating that POEX effectively highlights critical security vulnerabilities and is transferable across different LLMs. The findings emphasize the necessity for robust security measures to ensure the safe deployment of LLM-based robots in sensitive applications.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLM）在机器人系统中的日益整合，这带来了显著的安全风险，因为它们容易受到越狱攻击，可能导致物理世界中的有害行为。以往的LLM越狱攻击方法在机器人环境中被发现无效，主要是由于需要可执行策略优化和准确评估机器人越狱策略等独特挑战。本文提出了一种新框架POEX（可执行策略），通过利用隐藏层梯度优化和多智能体评估器来克服这些挑战，确保越狱成功和实际策略执行。该方法论包括构建一个以风险为导向的有害RLbench，并在真实机器人系统和模拟环境中进行实验，证明POEX在揭示关键安全漏洞和在各种LLM中适用性的有效性。研究结果强调了在敏感应用中保护基于LLM的机器人安全措施的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Humans overrely on overconfident language models, across languages</div>
<div class="meta-line">Authors: Neil Rathi, Dan Jurafsky, Kaitlyn Zhou</div>
<div class="meta-line">First: 2025-07-08T18:01:01+00:00 · Latest: 2025-08-08T00:50:04+00:00</div>
<div class="meta-line">Comments: camera ready</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.06306v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.06306v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) are deployed globally, it is crucial that their responses are calibrated across languages to accurately convey uncertainty and limitations. Prior work shows that LLMs are linguistically overconfident in English, leading users to overrely on confident generations. However, the usage and interpretation of epistemic markers (e.g., &#x27;I think it&#x27;s&#x27;) differs sharply across languages. Here, we study the risks of multilingual linguistic (mis)calibration, overconfidence, and overreliance across five languages to evaluate LLM safety in a global context. Our work finds that overreliance risks are high across languages. We first analyze the distribution of LLM-generated epistemic markers and observe that LLMs are overconfident across languages, frequently generating strengtheners even as part of incorrect responses. Model generations are, however, sensitive to documented cross-linguistic variation in usage: for example, models generate the most markers of uncertainty in Japanese and the most markers of certainty in German and Mandarin. Next, we measure human reliance rates across languages, finding that reliance behaviors differ cross-linguistically: for example, participants are significantly more likely to discount expressions of uncertainty in Japanese than in English (i.e., ignore their &#x27;hedging&#x27; function and rely on generations that contain them). Taken together, these results indicate a high risk of reliance on overconfident model generations across languages. Our findings highlight the challenges of multilingual linguistic calibration and stress the importance of culturally and linguistically contextualized model safety evaluations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人类在多语言环境中过度依赖自信的语言模型</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在全球范围内的部署，确保其响应在不同语言中准确传达不确定性和局限性至关重要。先前的研究表明，LLMs在英语中表现出语言上的过度自信，导致用户过度依赖自信的生成。然而，认知标记（例如，“我认为”）的使用和解释在不同语言中差异显著。在此，我们研究了五种语言中多语言语言（误）校准、过度自信和过度依赖的风险，以评估LLM在全球背景下的安全性。我们的研究发现，各种语言中的过度依赖风险很高。我们首先分析了LLM生成的认知标记的分布，观察到LLMs在不同语言中表现出过度自信，甚至在错误响应中也经常生成加强语。然而，模型生成对已记录的跨语言使用差异敏感：例如，模型在日语中生成最多的不确定性标记，而在德语和普通话中生成最多的确定性标记。接下来，我们测量了不同语言中的人类依赖率，发现依赖行为在跨语言中存在差异：例如，参与者在日语中显著更可能忽视不确定性的表达，而不是在英语中（即，忽视其“保留”功能，依赖包含这些表达的生成）。综合来看，这些结果表明，在不同语言中对过度自信的模型生成的依赖风险很高。我们的发现突显了多语言语言校准的挑战，并强调了文化和语言背景下模型安全评估的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical issue of how large language models (LLMs) convey uncertainty across different languages, as prior studies have shown that these models tend to be overconfident in English, leading to user overreliance on their outputs. Existing methods have not adequately considered the linguistic variations in the interpretation of epistemic markers, which can result in miscalibration and increased risks of overconfidence. This paper proposes a comprehensive analysis of LLM-generated epistemic markers across five languages, revealing that while LLMs are generally overconfident, their outputs vary significantly based on linguistic context. The methodology involves analyzing the distribution of epistemic markers and measuring human reliance rates, demonstrating that users&#x27; reliance behaviors differ across languages. The findings indicate a high risk of overreliance on LLM outputs globally, emphasizing the need for culturally and linguistically informed safety evaluations of these models.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在不同语言中如何传达不确定性的问题，因为先前的研究表明，这些模型在英语中往往过于自信，导致用户过度依赖其输出。现有方法未能充分考虑对认知标记的语言差异解释，这可能导致误校准和过度自信的风险增加。本文提出对五种语言中LLM性能的全面分析，重点研究认知标记的分布和人类依赖率，揭示用户在解释不确定性时存在显著的跨语言差异。该方法包括分析模型生成的响应和测量人类依赖行为，最终表明对过于自信输出的依赖是一个普遍问题。研究结果强调了对LLM进行多语言校准的必要性，以确保与全球用户的互动更加安全和准确。</div>
</details>
</div>
<div class="card">
<div class="title">CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations</div>
<div class="meta-line">Authors: Xiaohu Li, Yunfeng Ning, Zepeng Bao, Mayi Xu, Jianhao Chen, Tieyun Qian</div>
<div class="meta-line">Venue: ACL 2025</div>
<div class="meta-line">First: 2025-07-08T14:45:21+00:00 · Latest: 2025-08-06T05:32:54+00:00</div>
<div class="meta-line">Comments: Accepted to ACL 2025 (Findings), camera-ready version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.06043v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.06043v2">PDF</a> · <a href="https://github.com/NLPGM/CAVGAN">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Security alignment enables the Large Language Model (LLM) to gain the protection against malicious queries, but various jailbreak attack methods reveal the vulnerability of this security mechanism. Previous studies have isolated LLM jailbreak attacks and defenses. We analyze the security protection mechanism of the LLM, and propose a framework that combines attack and defense. Our method is based on the linearly separable property of LLM intermediate layer embedding, as well as the essence of jailbreak attack, which aims to embed harmful problems and transfer them to the safe area. We utilize generative adversarial network (GAN) to learn the security judgment boundary inside the LLM to achieve efficient jailbreak attack and defense. The experimental results indicate that our method achieves an average jailbreak success rate of 88.85\% across three popular LLMs, while the defense success rate on the state-of-the-art jailbreak dataset reaches an average of 84.17\%. This not only validates the effectiveness of our approach but also sheds light on the internal security mechanisms of LLMs, offering new insights for enhancing model security The code and data are available at https://github.com/NLPGM/CAVGAN.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CAVGAN：通过对大型语言模型内部表示的生成对抗攻击统一越狱与防御</div>
<div class="mono" style="margin-top:8px">安全对齐使大型语言模型（LLM）能够抵御恶意查询的攻击，但各种越狱攻击方法揭示了该安全机制的脆弱性。以往的研究将LLM越狱攻击与防御分开。我们分析了LLM的安全保护机制，并提出了一个结合攻击与防御的框架。我们的方法基于LLM中间层嵌入的线性可分性以及越狱攻击的本质，旨在嵌入有害问题并将其转移到安全区域。我们利用生成对抗网络（GAN）学习LLM内部的安全判断边界，以实现高效的越狱攻击与防御。实验结果表明，我们的方法在三种流行的LLM上实现了平均88.85%的越狱成功率，而在最先进的越狱数据集上的防御成功率达到了84.17%。这不仅验证了我们方法的有效性，还揭示了LLM的内部安全机制，为增强模型安全性提供了新思路。代码和数据可在https://github.com/NLPGM/CAVGAN获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of Large Language Models (LLMs) to jailbreak attacks, which exploit weaknesses in their security alignment mechanisms. Previous methods have treated attack and defense as separate entities, failing to provide a unified approach to enhance LLM security. The proposed framework, CAVGAN, integrates both aspects by leveraging the linearly separable property of LLM intermediate layer embeddings and employing generative adversarial networks (GANs) to learn the security judgment boundary. This method effectively combines attack and defense strategies, achieving an average jailbreak success rate of 88.85% across three popular LLMs and a defense success rate of 84.17% on a state-of-the-art jailbreak dataset, thereby validating its effectiveness and contributing to a deeper understanding of LLM security mechanisms.</div>
<div class="mono" style="margin-top:8px">本文研究了大型语言模型（LLM）在监狱突破攻击下的脆弱性，这些攻击利用了其安全对齐机制的弱点。以往的方法将攻击和防御视为孤立的过程，未能提供统一的安全增强方案。所提出的方法CAVGAN通过利用LLM中间层嵌入的线性可分性特征，结合生成对抗网络（GAN）学习LLM内部的安全判断边界，从而整合了攻击和防御策略。其贡献在于构建了一个框架，不仅在三种流行LLM上实现了平均监狱突破成功率88.85%，而且在最先进的监狱突破数据集上实现了84.17%的防御成功率，证明了该方法的有效性，并为提升模型安全性提供了新的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial-Guided Diffusion for Multimodal LLM Attacks</div>
<div class="meta-line">Authors: Chengwei Xia, Fan Ma, Ruijie Quan, Kun Zhan, Yi Yang</div>
<div class="meta-line">First: 2025-07-31T02:57:20+00:00 · Latest: 2025-07-31T02:57:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.23202v1">Abs</a> · <a href="https://arxiv.org/pdf/2507.23202v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper addresses the challenge of generating adversarial image using a diffusion model to deceive multimodal large language models (MLLMs) into generating the targeted responses, while avoiding significant distortion of the clean image. To address the above challenges, we propose an adversarial-guided diffusion (AGD) approach for adversarial attack MLLMs. We introduce adversarial-guided noise to ensure attack efficacy. A key observation in our design is that, unlike most traditional adversarial attacks which embed high-frequency perturbations directly into the clean image, AGD injects target semantics into the noise component of the reverse diffusion. Since the added noise in a diffusion model spans the entire frequency spectrum, the adversarial signal embedded within it also inherits this full-spectrum property. Importantly, during reverse diffusion, the adversarial image is formed as a linear combination of the clean image and the noise. Thus, when applying defenses such as a simple low-pass filtering, which act independently on each component, the adversarial image within the noise component is less likely to be suppressed, as it is not confined to the high-frequency band. This makes AGD inherently robust to variety defenses. Extensive experiments demonstrate that our AGD outperforms state-of-the-art methods in attack performance as well as in model robustness to some defenses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对抗引导扩散用于多模态大语言模型攻击</div>
<div class="mono" style="margin-top:8px">本文解决了使用扩散模型生成对抗图像以欺骗多模态大语言模型（MLLMs）生成目标响应的挑战，同时避免对干净图像的显著失真。为了解决上述挑战，我们提出了一种对抗引导扩散（AGD）方法用于对抗攻击MLLMs。我们引入对抗引导噪声以确保攻击效果。我们设计中的一个关键观察是，与大多数传统对抗攻击直接将高频扰动嵌入干净图像不同，AGD将目标语义注入反向扩散的噪声成分中。由于扩散模型中添加的噪声跨越整个频谱，因此嵌入其中的对抗信号也继承了这种全频谱特性。重要的是，在反向扩散过程中，对抗图像作为干净图像和噪声的线性组合形成。因此，当应用诸如简单低通滤波等防御措施时，这些措施独立作用于每个成分，噪声成分中的对抗图像不太可能被抑制，因为它并不局限于高频带。这使得AGD在各种防御中本质上具有鲁棒性。大量实验表明，我们的AGD在攻击性能和模型对某些防御的鲁棒性方面优于最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of generating adversarial images that can deceive multimodal large language models (MLLMs) while maintaining the integrity of the original image. Previous methods typically embed high-frequency perturbations directly into the clean image, which can lead to significant distortions and make the adversarial examples more detectable. The proposed adversarial-guided diffusion (AGD) approach differs by injecting target semantics into the noise component of the reverse diffusion process, allowing for a full-spectrum adversarial signal that is less susceptible to common defense mechanisms. The contribution of this paper lies in demonstrating that AGD not only enhances attack efficacy but also improves robustness against various defenses. The methodology involves introducing adversarial-guided noise during the diffusion process, resulting in adversarial images that are linear combinations of clean images and noise. Experimental results show that AGD outperforms state-of-the-art methods in both attack performance and resilience to defenses, supporting its effectiveness in achieving the research goals.</div>
<div class="mono" style="margin-top:8px">本研究解决了生成对抗性图像以欺骗多模态大型语言模型（MLLMs）同时保持原始图像完整性的问题。以往的方法通常直接将高频扰动嵌入图像中，导致显著失真和对防御的脆弱性。所提出的对抗引导扩散（AGD）方法通过将目标语义注入反向扩散过程中的噪声成分，提供了一种更有效且不易被检测的攻击方式。该方法的动机明确，因为它利用了扩散模型中噪声的全频谱特性，从而增强了对各种防御的鲁棒性。本文贡献了一种新颖的方法论，实验结果表明其在攻击性能和对防御的抗性方面优于现有技术。</div>
</details>
</div>
<div class="card">
<div class="title">LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning</div>
<div class="meta-line">Authors: Gabriel J. Perin, Runjin Chen, Xuxi Chen, Nina S. T. Hirata, Zhangyang Wang, Junyuan Hong</div>
<div class="meta-line">First: 2025-06-18T16:30:02+00:00 · Latest: 2025-07-25T18:57:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.15606v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.15606v3">PDF</a> · <a href="http://github.com/VITA-Group/LoX">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have become indispensable in real-world applications. However, their widespread adoption raises significant safety concerns, particularly in responding to socially harmful questions. Despite substantial efforts to improve model safety through alignment, aligned models can still have their safety protections undermined by subsequent fine-tuning - even when the additional training data appears benign. In this paper, we empirically demonstrate that this vulnerability stems from the sensitivity of safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building on this insight, we propose a novel training-free method, termed Low-Rank Extrapolation (LoX), to enhance safety robustness by extrapolating the safety subspace of an aligned LLM. Our experimental results confirm the effectiveness of LoX, demonstrating significant improvements in robustness against both benign and malicious fine-tuning attacks while preserving the model&#x27;s adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute reductions in attack success rates (ASR) facing benign or malicious fine-tuning attacks. By investigating the ASR landscape of parameters, we attribute the success of LoX to that the extrapolation moves LLM parameters to a flatter zone, thereby less sensitive to perturbations. The code is available at github.com/VITA-Group/LoX.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LoX：低秩外推增强LLM在微调下的安全性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）在现实应用中变得不可或缺。然而，它们的广泛采用引发了重大安全隐患，特别是在回应社会有害问题时。尽管在对齐方面进行了大量努力以提高模型安全性，但对齐模型仍可能因后续微调而削弱其安全保护——即使额外的训练数据看似无害。本文通过实证研究表明，这一脆弱性源于LLM参数中安全关键低秩子空间对微调的敏感性。基于这一见解，我们提出了一种新颖的无训练方法，称为低秩外推（LoX），通过外推对齐LLM的安全子空间来增强安全性鲁棒性。我们的实验结果确认了LoX的有效性，显示在面对良性和恶意微调攻击时，鲁棒性显著提高，同时保持模型对新任务的适应性。例如，LoX在面对良性或恶意微调攻击时，攻击成功率（ASR）绝对降低了11%至54%。通过研究参数的ASR景观，我们将LoX的成功归因于外推将LLM参数移动到一个更平坦的区域，从而对扰动的敏感性降低。代码可在github.com/VITA-Group/LoX获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the safety concerns associated with the fine-tuning of Large Language Models (LLMs), which can undermine their safety protections even when additional training data seems harmless. Previous methods aimed at improving model safety have not effectively mitigated this vulnerability, particularly due to the sensitivity of low-rank subspaces in LLM parameters. The proposed approach, Low-Rank Extrapolation (LoX), is a training-free method that enhances safety robustness by extrapolating the safety subspace of aligned LLMs, effectively addressing the limitations of existing methods. This paper contributes by empirically demonstrating that LoX significantly improves robustness against both benign and malicious fine-tuning attacks, achieving absolute reductions in attack success rates ranging from 11% to 54%, while maintaining the model&#x27;s adaptability to new tasks. The success of LoX is attributed to its ability to move LLM parameters to a flatter zone, making them less sensitive to perturbations.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在微调过程中可能出现的安全隐患，即即使使用看似无害的训练数据，也可能削弱其安全保护。以往的方法主要集中在模型对齐，但未能有效保护模型免受微调引入的脆弱性。提出的低秩外推（LoX）方法通过外推对齐LLM的安全子空间来增强安全鲁棒性，且无需额外训练，这一方法的提出是基于对LLM参数中低秩子空间敏感性的深入理解。该论文的贡献在于提出了一种新颖的无训练方法，显著提高了对无害和恶意微调攻击的鲁棒性，攻击成功率降低幅度在11%到54%之间，同时保持了模型对新任务的适应能力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260101_0334.html">20260101_0334</a>
<a href="archive/20251231_0340.html">20251231_0340</a>
<a href="archive/20251230_0340.html">20251230_0340</a>
<a href="archive/20251229_0339.html">20251229_0339</a>
<a href="archive/20251228_0337.html">20251228_0337</a>
<a href="archive/20251226_0334.html">20251226_0334</a>
<a href="archive/20251225_0335.html">20251225_0335</a>
<a href="archive/20251224_0340.html">20251224_0340</a>
<a href="archive/20251223_0337.html">20251223_0337</a>
<a href="archive/20251222_0336.html">20251222_0336</a>
<a href="archive/20251221_0337.html">20251221_0337</a>
<a href="archive/20251220_0338.html">20251220_0338</a>
<a href="archive/20251219_0346.html">20251219_0346</a>
<a href="archive/20251218_0343.html">20251218_0343</a>
<a href="archive/20251217_0352.html">20251217_0352</a>
<a href="archive/20251216_0346.html">20251216_0346</a>
<a href="archive/20251215_0337.html">20251215_0337</a>
<a href="archive/20251214_0334.html">20251214_0334</a>
<a href="archive/20251213_0347.html">20251213_0347</a>
<a href="archive/20251212_0350.html">20251212_0350</a>
<a href="archive/20251211_0354.html">20251211_0354</a>
<a href="archive/20251210_0344.html">20251210_0344</a>
<a href="archive/20251209_0348.html">20251209_0348</a>
<a href="archive/20251208_0336.html">20251208_0336</a>
<a href="archive/20251207_0335.html">20251207_0335</a>
<a href="archive/20251206_0340.html">20251206_0340</a>
<a href="archive/20251205_0346.html">20251205_0346</a>
<a href="archive/20251204_0344.html">20251204_0344</a>
<a href="archive/20251203_0343.html">20251203_0343</a>
<a href="archive/20251202_0352.html">20251202_0352</a>
<a href="archive/20251201_0334.html">20251201_0334</a>
<a href="archive/20251130_0333.html">20251130_0333</a>
<a href="archive/20251129_0338.html">20251129_0338</a>
<a href="archive/20251128_0337.html">20251128_0337</a>
<a href="archive/20251127_0339.html">20251127_0339</a>
<a href="archive/20251126_0343.html">20251126_0343</a>
<a href="archive/20251125_0334.html">20251125_0334</a>
<a href="archive/20251124_0335.html">20251124_0335</a>
<a href="archive/20251123_0334.html">20251123_0334</a>
<a href="archive/20251122_0348.html">20251122_0348</a>
<a href="archive/20251121_0347.html">20251121_0347</a>
<a href="archive/20251120_0401.html">20251120_0401</a>
<a href="archive/20251119_0350.html">20251119_0350</a>
<a href="archive/20251118_0347.html">20251118_0347</a>
<a href="archive/20251117_0340.html">20251117_0340</a>
<a href="archive/20251116_0340.html">20251116_0340</a>
<a href="archive/20251115_0313.html">20251115_0313</a>
<a href="archive/20251114_0315.html">20251114_0315</a>
<a href="archive/20251113_0315.html">20251113_0315</a>
<a href="archive/20251112_0314.html">20251112_0314</a>
<a href="archive/20251111_0349.html">20251111_0349</a>
<a href="archive/20251110_0329.html">20251110_0329</a>
<a href="archive/20251109_0314.html">20251109_0314</a>
<a href="archive/20251108_0315.html">20251108_0315</a>
<a href="archive/20251107_0317.html">20251107_0317</a>
<a href="archive/20251106_0349.html">20251106_0349</a>
<a href="archive/20251105_0321.html">20251105_0321</a>
<a href="archive/20251104_1539.html">20251104_1539</a>
<a href="archive/20251104_0324.html">20251104_0324</a>
<a href="archive/20251103_0315.html">20251103_0315</a>
<a href="archive/20251102_1356.html">20251102_1356</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
