<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-02 13:24</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251102_1324</div>
    <div class="row"><div class="card">
<div class="title">Chasing Moving Targets with Online Self-Play Reinforcement Learning for   Safer Language Models</div>
<div class="meta-line">Authors: Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, Natasha Jaques</div>
<div class="meta-line">First: 2025-06-09T06:35:12+00:00 · Latest: 2025-10-06T03:42:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.07468v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.07468v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过在线自我对弈强化学习追踪移动目标以提高语言模型的安全性</div>
<div class="mono" style="margin-top:8px">传统的语言模型（LM）安全对齐依赖于反应性、分离的程序：攻击者利用静态模型，随后进行防御性微调以修补暴露的漏洞。这种顺序方法造成了不匹配——攻击者过度拟合过时的防御，而防御者则始终滞后于新出现的威胁。为了解决这个问题，我们提出了Self-RedTeam，一种在线自我对弈强化学习算法，其中攻击者和防御者代理通过持续互动共同进化。我们将安全对齐视为一个双人零和游戏，其中单个模型在攻击者和防御者角色之间交替——生成对抗性提示并对其进行保护——同时奖励语言模型裁定结果。这使得动态共同适应成为可能。基于零和游戏的博弈论框架，我们建立了理论安全保证，这激励了我们方法的设计：如果自我对弈收敛到纳什均衡，防御者将可靠地产生对任何对抗性输入的安全响应。在实证上，Self-RedTeam发现了比针对静态防御者训练的攻击者更具多样性的攻击（+21.8% SBERT），并在安全基准上实现了更高的鲁棒性（例如，在WildJailBreak上+65.5%），相比于针对静态攻击者训练的防御者。我们进一步提出了隐藏的思维链，允许代理私下规划，从而提高对抗性多样性并减少过度拒绝。我们的结果激励了从反应性修补转向主动共同进化的LM安全训练，使得通过多代理强化学习（MARL）实现可扩展、自主和鲁棒的LM自我改进成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to improve the safety alignment of language models (LMs) by addressing the limitations of conventional reactive approaches that leave models vulnerable to emerging threats. The authors propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction, framing safety alignment as a two-player zero-sum game. Experimental results show that Self-RedTeam uncovers a greater diversity of attacks, achieving a 21.8% increase in SBERT scores and a 65.5% improvement on safety benchmarks like WildJailBreak, demonstrating the effectiveness of proactive co-evolution in enhancing LM safety training.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于通过解决传统反应方法的局限性来提高语言模型（LM）的安全对齐，这些方法使模型容易受到不断演变的威胁。作者提出了Self-RedTeam，这是一种在线自我对抗强化学习算法，允许攻击者和防御者代理通过持续互动共同进化，将安全对齐框定为一个双人零和博弈。实验结果表明，与传统方法相比，Self-RedTeam生成了21.8%更多的多样化攻击，并在安全基准测试中实现了65.5%的更高鲁棒性，突显了主动共同进化在增强LM安全训练中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">MetaSC: Test-Time Safety Specification Optimization for Language Models</div>
<div class="meta-line">Authors: Víctor Gallego</div>
<div class="meta-line">Venue: ICLR 2025</div>
<div class="meta-line">First: 2025-02-11T22:06:25+00:00 · Latest: 2025-04-07T09:15:30+00:00</div>
<div class="meta-line">Comments: Published at ICLR 2025 Workshop on Foundation Models in the Wild</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.07985v2">Abs</a> · <a href="http://arxiv.org/pdf/2502.07985v2">PDF</a> · <a href="https://github.com/vicgalle/meta-self-critique.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a novel dynamic safety framework that optimizes language model
(LM) safety reasoning at inference time without modifying model weights.
Building on recent advances in self-critique methods, our approach leverages a
meta-critique mechanism that iteratively updates safety prompts-termed
specifications-to drive the critique and revision process adaptively. This
test-time optimization not only improves performance against adversarial
jailbreak requests but also in diverse general safety-related tasks, such as
avoiding moral harm or pursuing honest responses. Our empirical evaluations
across several language models demonstrate that dynamically optimized safety
prompts yield significantly higher safety scores compared to fixed system
prompts and static self-critique defenses. Code released at
https://github.com/vicgalle/meta-self-critique.git .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MetaSC：语言模型的测试时安全规范优化</div>
<div class="mono" style="margin-top:8px">我们提出了一种新颖的动态安全框架，在推理时优化语言模型（LM）的安全推理，而无需修改模型权重。基于自我批评方法的最新进展，我们的方法利用了一种元批评机制，迭代更新安全提示——称为规范——以自适应地推动批评和修订过程。这种测试时优化不仅提高了对抗性越狱请求的性能，还在避免道德伤害或追求诚实回应等多样化的安全相关任务中表现出色。我们在多个语言模型上的实证评估表明，动态优化的安全提示相比固定系统提示和静态自我批评防御，显著提高了安全评分。代码发布于 https://github.com/vicgalle/meta-self-critique.git。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to enhance the safety of language models during inference without altering their weights. The authors introduce a dynamic safety framework that employs a meta-critique mechanism to iteratively optimize safety prompts, referred to as specifications, which guide the model&#x27;s critique and revision processes. Experimental results indicate that this test-time optimization leads to significantly improved safety scores against adversarial attacks and in various safety-related tasks compared to traditional fixed prompts and static defenses.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于在推理过程中增强语言模型的安全性，而不改变其权重。作者提出了一种动态安全框架，采用元批评机制迭代优化安全提示（称为规范），以改善模型对对抗性请求和一般安全任务的响应。实验结果表明，动态优化的安全提示在多个语言模型中显著优于固定提示和静态自我批评方法的安全评分。</div>
</details>
</div>
<div class="card">
<div class="title">Dissecting Adversarial Robustness of Multimodal LM Agents</div>
<div class="meta-line">Authors: Chen Henry Wu, Rishi Shah, Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, Aditi Raghunathan</div>
<div class="meta-line">Venue: ICLR 2025 oral</div>
<div class="meta-line">First: 2024-06-18T17:32:48+00:00 · Latest: 2025-02-04T20:02:17+00:00</div>
<div class="meta-line">Comments: ICLR 2025. Also oral at NeurIPS 2024 Open-World Agents Workshop</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2406.12814v3">Abs</a> · <a href="http://arxiv.org/pdf/2406.12814v3">PDF</a> · <a href="https://github.com/ChenWu98/agent-attack">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As language models (LMs) are used to build autonomous agents in real
environments, ensuring their adversarial robustness becomes a critical
challenge. Unlike chatbots, agents are compound systems with multiple
components taking actions, which existing LMs safety evaluations do not
adequately address. To bridge this gap, we manually create 200 targeted
adversarial tasks and evaluation scripts in a realistic threat model on top of
VisualWebArena, a real environment for web agents. To systematically examine
the robustness of agents, we propose the Agent Robustness Evaluation (ARE)
framework. ARE views the agent as a graph showing the flow of intermediate
outputs between components and decomposes robustness as the flow of adversarial
information on the graph. We find that we can successfully break latest agents
that use black-box frontier LMs, including those that perform reflection and
tree search. With imperceptible perturbations to a single image (less than 5%
of total web page pixels), an attacker can hijack these agents to execute
targeted adversarial goals with success rates up to 67%. We also use ARE to
rigorously evaluate how the robustness changes as new components are added. We
find that inference-time compute that typically improves benign performance can
open up new vulnerabilities and harm robustness. An attacker can compromise the
evaluator used by the reflexion agent and the value function of the tree search
agent, which increases the attack success relatively by 15% and 20%. Our data
and code for attacks, defenses, and evaluation are at
https://github.com/ChenWu98/agent-attack</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态语言模型代理的对抗鲁棒性剖析</div>
<div class="mono" style="margin-top:8px">随着语言模型（LM）被用于构建真实环境中的自主代理，确保其对抗鲁棒性成为一项关键挑战。与聊天机器人不同，代理是由多个组件组成的复合系统，这些组件采取行动，而现有的LM安全评估并未充分解决这一问题。为填补这一空白，我们手动创建了200个针对性的对抗任务和评估脚本，基于VisualWebArena这一真实的网络代理环境，构建了一个现实的威胁模型。为了系统地检查代理的鲁棒性，我们提出了代理鲁棒性评估（ARE）框架。ARE将代理视为一个图，展示组件之间中间输出的流动，并将鲁棒性分解为图上的对抗信息流动。我们发现可以成功攻破最新的使用黑箱前沿LM的代理，包括那些执行反射和树搜索的代理。通过对单个图像施加不可察觉的扰动（少于5%的网页总像素），攻击者可以劫持这些代理以成功执行针对性的对抗目标，成功率高达67%。我们还使用ARE严格评估随着新组件的添加鲁棒性如何变化。我们发现，通常改善良性性能的推理时计算可能会打开新的漏洞并损害鲁棒性。攻击者可以破坏反射代理使用的评估器和树搜索代理的价值函数，分别使攻击成功率提高15%和20%。我们的攻击、防御和评估的数据和代码可在https://github.com/ChenWu98/agent-attack获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the critical challenge of adversarial robustness in multimodal language model agents, which are increasingly used in real-world applications. The authors developed a framework called Agent Robustness Evaluation (ARE) and created 200 targeted adversarial tasks within a realistic threat model using VisualWebArena to systematically assess the robustness of these agents. The findings reveal that even minor perturbations to a single image can enable attackers to manipulate agents to achieve adversarial goals with success rates reaching 67%, and that adding new components can inadvertently introduce vulnerabilities, increasing attack success rates by 15% to 20%.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于解决多模态语言模型（LM）代理在现实应用中确保对抗鲁棒性的关键挑战。作者开发了一个名为代理鲁棒性评估（ARE）的框架，通过将代理建模为表示组件之间信息流的图来系统地评估这些代理的鲁棒性。实验结果表明，针对性的对抗任务可以成功破坏最先进的代理，攻击成功率高达67%，且对输入图像的扰动极小，同时还强调了添加新组件可能无意中引入的脆弱性，从而降低整体鲁棒性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251102_1243.html">20251102_1243</a>
<a href="archive/20251102_1232.html">20251102_1232</a>
<a href="archive/20251102_1220.html">20251102_1220</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
