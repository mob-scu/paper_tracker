<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-07 03:17</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251107_0317</div>
    <div class="row"><div class="card">
<div class="title">An Automated Framework for Strategy Discovery, Retrieval, and Evolution   in LLM Jailbreak Attacks</div>
<div class="meta-line">Authors: Xu Liu, Yan Chen, Kan Ling, Yichi Zhu, Hengrun Zhang, Guisheng Fan, Huiqun Yu</div>
<div class="meta-line">First: 2025-11-04T08:24:22+00:00 · Latest: 2025-11-04T08:24:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.02356v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.02356v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread deployment of Large Language Models (LLMs) as public-facing
web services and APIs has made their security a core concern for the web
ecosystem. Jailbreak attacks, as one of the significant threats to LLMs, have
recently attracted extensive research. In this paper, we reveal a jailbreak
strategy which can effectively evade current defense strategies. It can extract
valuable information from failed or partially successful attack attempts and
contains self-evolution from attack interactions, resulting in sufficient
strategy diversity and adaptability. Inspired by continuous learning and
modular design principles, we propose ASTRA, a jailbreak framework that
autonomously discovers, retrieves, and evolves attack strategies to achieve
more efficient and adaptive attacks. To enable this autonomous evolution, we
design a closed-loop &quot;attack-evaluate-distill-reuse&quot; core mechanism that not
only generates attack prompts but also automatically distills and generalizes
reusable attack strategies from every interaction. To systematically accumulate
and apply this attack knowledge, we introduce a three-tier strategy library
that categorizes strategies into Effective, Promising, and Ineffective based on
their performance scores. The strategy library not only provides precise
guidance for attack generation but also possesses exceptional extensibility and
transferability. We conduct extensive experiments under a black-box setting,
and the results show that ASTRA achieves an average Attack Success Rate (ASR)
of 82.7%, significantly outperforming baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于LLM越狱攻击的策略发现、检索和演化的自动化框架</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）作为面向公众的网络服务和API的广泛部署，使其安全性成为网络生态系统的核心关注点。越狱攻击作为对LLMs的重要威胁之一，最近引起了广泛的研究。本文揭示了一种能够有效规避当前防御策略的越狱策略。它可以从失败或部分成功的攻击尝试中提取有价值的信息，并包含来自攻击交互的自我演化，导致足够的策略多样性和适应性。受到持续学习和模块化设计原则的启发，我们提出了ASTRA，一个能够自主发现、检索和演化攻击策略的越狱框架，以实现更高效和适应性的攻击。为了实现这种自主演化，我们设计了一个闭环的“攻击-评估-提炼-重用”核心机制，不仅生成攻击提示，还自动提炼和概括每次交互中的可重用攻击策略。为了系统地积累和应用这些攻击知识，我们引入了一个三层策略库，根据其性能评分将策略分类为有效、前景良好和无效。策略库不仅为攻击生成提供精确指导，还具有卓越的可扩展性和可转移性。我们在黑箱设置下进行了广泛的实验，结果表明ASTRA的平均攻击成功率（ASR）为82.7%，显著优于基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the security vulnerabilities of Large Language Models (LLMs) in the context of jailbreak attacks, which have become a significant threat as these models are increasingly deployed in public-facing applications. Previous methods for defending against such attacks have been limited in their adaptability and effectiveness, often failing to evolve in response to new attack strategies. The proposed approach, ASTRA, differs by autonomously discovering, retrieving, and evolving attack strategies through a closed-loop mechanism that enhances strategy diversity and adaptability. This framework is well-motivated by principles of continuous learning and modular design, contributing a systematic method for accumulating and applying attack knowledge through a categorized strategy library. In extensive black-box experiments, ASTRA demonstrated an average Attack Success Rate (ASR) of 82.7%, significantly outperforming existing baselines, thus supporting its goal of more efficient and adaptive jailbreak attacks.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）的安全漏洞，特别是针对其作为网络服务部署时所面临的越狱攻击。以往的方法在应对不断演变的攻击策略时适应性和有效性有限，因此需要一种更具动态性的解决方案。所提出的框架ASTRA通过自主发现、检索和演变攻击策略，采用持续学习机制，从而解决了现有方法的不足。本文的贡献在于开发了一种闭环机制，该机制不仅生成和优化攻击提示，还根据性能将策略分类为三级库。实验结果表明，ASTRA的平均攻击成功率（ASR）达到82.7%，显示出其在增强越狱攻击的适应性和效率方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Prompt Injection as an Emerging Threat: Evaluating the Resilience of   Large Language Models</div>
<div class="meta-line">Authors: Daniyal Ganiuly, Assel Smaiyl</div>
<div class="meta-line">First: 2025-11-03T14:43:56+00:00 · Latest: 2025-11-03T14:43:56+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.01634v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.01634v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly used in intelligent systems
that perform reasoning, summarization, and code generation. Their ability to
follow natural-language instructions, while powerful, also makes them
vulnerable to a new class of attacks known as prompt injection. In these
attacks, hidden or malicious instructions are inserted into user inputs or
external content, causing the model to ignore its intended task or produce
unsafe responses. This study proposes a unified framework for evaluating how
resistant Large Language Models (LLMs) are to prompt injection attacks. The
framework defines three complementary metrics such as the Resilience
Degradation Index (RDI), Safety Compliance Coefficient (SCC), and Instructional
Integrity Metric (IIM) to jointly measure robustness, safety, and semantic
stability. We evaluated four instruction-tuned models (GPT-4, GPT-4o, LLaMA-3
8B Instruct, and Flan-T5-Large) on five common language tasks: question
answering, summarization, translation, reasoning, and code generation. Results
show that GPT-4 performs best overall, while open-weight models remain more
vulnerable. The findings highlight that strong alignment and safety tuning are
more important for resilience than model size alone. Results show that all
models remain partially vulnerable, especially to indirect and direct-override
attacks. GPT-4 achieved the best overall resilience (RDR = 9.8 %, SCR = 96.4
%), while open-source models exhibited higher performance degradation and lower
safety scores. The findings demonstrate that alignment strength and safety
tuning play a greater role in resilience than model size alone. The proposed
framework offers a structured, reproducible approach for assessing model
robustness and provides practical insights for improving LLM safety and
reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提示注入作为新兴威胁：评估大型语言模型的韧性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在执行推理、摘要和代码生成的智能系统中越来越多地被使用。它们遵循自然语言指令的能力虽然强大，但也使其容易受到一种新型攻击的威胁，即提示注入。在这些攻击中，隐藏或恶意的指令被插入到用户输入或外部内容中，导致模型忽略其预期任务或产生不安全的响应。本研究提出了一个统一框架，用于评估大型语言模型（LLMs）对提示注入攻击的抵抗力。该框架定义了三个互补指标，如韧性降级指数（RDI）、安全合规系数（SCC）和指令完整性指标（IIM），以共同衡量鲁棒性、安全性和语义稳定性。我们对四个经过指令调优的模型（GPT-4、GPT-4o、LLaMA-3 8B Instruct 和 Flan-T5-Large）在五个常见语言任务上进行了评估：问答、摘要、翻译、推理和代码生成。结果显示，GPT-4的整体表现最佳，而开放权重模型则更容易受到攻击。研究结果强调，强对齐和安全调优对韧性的重要性超过了模型大小。结果表明，所有模型仍然部分脆弱，尤其是对间接和直接覆盖攻击。GPT-4实现了最佳的整体韧性（RDR = 9.8%，SCR = 96.4%），而开源模型表现出更高的性能降级和较低的安全评分。研究结果表明，对齐强度和安全调优在韧性中发挥的作用大于模型大小。所提出的框架提供了一种结构化、可重复的方法来评估模型的鲁棒性，并为提高LLM的安全性和可靠性提供了实用见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the increasing vulnerability of Large Language Models (LLMs) to prompt injection attacks, which can lead to unsafe outputs by manipulating user inputs. Previous methods lacked a comprehensive evaluation framework to assess model resilience against such attacks, often focusing on individual aspects of performance without considering robustness, safety, and semantic stability together. This paper proposes a unified framework that includes three metrics: the Resilience Degradation Index (RDI), Safety Compliance Coefficient (SCC), and Instructional Integrity Metric (IIM), allowing for a holistic assessment of LLMs. The methodology involves evaluating four instruction-tuned models across five language tasks, revealing that while GPT-4 shows the highest resilience, all models exhibit vulnerabilities, particularly to direct and indirect attacks. The findings underscore the importance of alignment and safety tuning over model size, contributing valuable insights for enhancing the safety and reliability of LLMs.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）面临的提示注入攻击这一新兴威胁，这些攻击可能会影响模型在推理和摘要等任务中的表现。以往的方法缺乏全面评估模型对这些攻击的抵御能力的框架，导致对其脆弱性的理解不足。本文提出了一个统一框架，引入了三种指标——抵御降级指数（RDI）、安全合规系数（SCC）和指令完整性指标（IIM），用于评估模型的鲁棒性、安全性和语义稳定性。该方法通过在五个语言任务上评估四个指令调优模型，结果显示，尽管GPT-4在整体抵御能力上表现最佳，但所有模型仍然在一定程度上脆弱，尤其是对直接和间接攻击。研究结果强调了对齐和安全调优在增强抵御能力方面的重要性，超越了模型规模，为改善LLMs的安全性和可靠性提供了有价值的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM   Judges</div>
<div class="meta-line">Authors: Hamin Koo, Minseon Kim, Jaehyung Kim</div>
<div class="meta-line">First: 2025-11-03T09:18:27+00:00 · Latest: 2025-11-03T09:18:27+00:00</div>
<div class="meta-line">Comments: under review, 28 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.01375v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.01375v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对齐与错位：使用元优化的LLM评估者进行自动LLM越狱</div>
<div class="mono" style="margin-top:8px">识别大型语言模型（LLMs）的脆弱性对于通过解决固有弱点来提高其安全性至关重要。越狱是指对手通过精心设计的输入提示绕过保护措施，在红队测试中发挥核心作用，探测LLMs以引发意外或不安全的行为。最近的基于优化的越狱方法通过利用LLMs迭代地优化攻击提示。然而，它们通常严重依赖于稀疏的二元攻击成功率（ASR）信号或手动制作的评分模板，这引入了人为偏见和评分结果的不确定性。为了解决这些局限性，我们引入了AMIS（对齐与错位），一个通过双层结构共同演化越狱提示和评分模板的元优化框架。在内循环中，使用固定评分模板通过细粒度和密集反馈来优化提示。在外循环中，使用ASR对齐分数优化模板，逐渐演变以更好地反映查询的真实攻击结果。这个共同优化过程产生了逐渐更强的越狱提示和更校准的评分信号。在AdvBench和JBB-Behaviors上的评估表明，AMIS实现了最先进的性能，包括在Claude-3.5-Haiku上达到88.0%的ASR和在Claude-4-Sonnet上达到100.0%的ASR，显著超越现有基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical need to identify vulnerabilities in large language models (LLMs) to enhance their safety against adversarial attacks, particularly through jailbreaks that exploit crafted input prompts. Previous methods for optimizing jailbreak prompts often depended on sparse binary attack success rate (ASR) signals or biased, manually created scoring templates, leading to inefficiencies and inaccuracies. The proposed approach, AMIS (Align to MISalign), introduces a meta-optimization framework that simultaneously evolves both jailbreak prompts and scoring templates using a bi-level structure, thereby mitigating the limitations of earlier methods. This framework allows for fine-grained feedback in the inner loop and optimizes the scoring template in the outer loop, resulting in more effective prompts and calibrated scoring signals. The methodology was evaluated on AdvBench and JBB-Behaviors, achieving state-of-the-art performance with 88.0% ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, significantly surpassing existing baselines and supporting the research goals.</div>
<div class="mono" style="margin-top:8px">本研究解决了识别大型语言模型（LLMs）漏洞的关键需求，以增强其安全性，特别关注越狱问题，即对手利用精心设计的提示绕过安全保护。以往的方法主要依赖于二元攻击成功率（ASR）信号或手动创建的评分模板，这带来了稀疏性和人为偏见等挑战。所提出的方法AMIS（Align to MISalign）引入了一种元优化框架，通过双层结构同时演变越狱提示和评分模板，有效解决了现有方法的局限性。该方法论涉及使用详细反馈来优化提示，同时基于ASR对齐分数优化评分模板，从而提高提示的有效性和评分的准确性。在AdvBench和JBB-Behaviors上的实验结果表明，AMIS实现了最先进的性能，在Claude-3.5-Haiku上达到88.0%的ASR，在Claude-4-Sonnet上达到100.0%的ASR，显著超越了之前的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Exploiting Latent Space Discontinuities for Building Universal LLM   Jailbreaks and Data Extraction Attacks</div>
<div class="meta-line">Authors: Kayua Oleques Paim, Rodrigo Brandao Mansilha, Diego Kreutz, Muriel Figueredo Franco, Weverton Cordeiro</div>
<div class="meta-line">First: 2025-11-01T01:19:12+00:00 · Latest: 2025-11-01T01:19:12+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures, 4 tables, Published at the Brazilian Symposium
  on Cybersecurity (SBSeg 2025)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.00346v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.00346v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid proliferation of Large Language Models (LLMs) has raised
significant concerns about their security against adversarial attacks. In this
work, we propose a novel approach to crafting universal jailbreaks and data
extraction attacks by exploiting latent space discontinuities, an architectural
vulnerability related to the sparsity of training data. Unlike previous
methods, our technique generalizes across various models and interfaces,
proving highly effective in seven state-of-the-art LLMs and one image
generation model. Initial results indicate that when these discontinuities are
exploited, they can consistently and profoundly compromise model behavior, even
in the presence of layered defenses. The findings suggest that this strategy
has substantial potential as a systemic attack vector.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用潜在空间不连续性构建通用LLM越狱和数据提取攻击</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的快速普及引发了对其安全性在对抗攻击下的重大担忧。在本研究中，我们提出了一种新颖的方法，通过利用潜在空间不连续性（与训练数据稀疏性相关的架构漏洞）来制作通用越狱和数据提取攻击。与之前的方法不同，我们的技术在各种模型和接口中具有广泛的适用性，在七个最先进的LLM和一个图像生成模型中证明了其高效性。初步结果表明，当利用这些不连续性时，它们可以持续而深刻地影响模型行为，即使在分层防御的情况下也是如此。研究结果表明，这一策略作为系统性攻击向量具有巨大的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing security concerns surrounding Large Language Models (LLMs) due to their vulnerability to adversarial attacks. Previous methods for crafting jailbreaks and data extraction attacks have been limited in their effectiveness and adaptability across different models. The proposed approach leverages latent space discontinuities, a specific architectural vulnerability linked to training data sparsity, allowing for a more generalized and effective attack strategy. This method is well-motivated as it exploits inherent weaknesses in LLM architectures, demonstrating significant potential for systemic attacks. The methodology was tested across seven state-of-the-art LLMs and one image generation model, achieving consistent and profound compromises in model behavior, even against layered defenses, thereby supporting the goals of enhancing understanding of LLM vulnerabilities.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）日益严重的安全问题，因其易受到对抗性攻击的影响。以往的攻击方法在不同模型间的有效性和适应性上存在局限性。本文提出了一种新方法，利用潜在空间的不连续性，这是一种源于训练数据稀疏性的特定架构漏洞，从而创建通用的越狱和数据提取攻击，能够在各种模型和接口中通用。该方法在七个最先进的LLM和一个图像生成模型中表现出显著的有效性，能够在分层防御下依然有效地破坏模型行为，表明这种方法作为系统性攻击向量具有巨大的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM   Agents</div>
<div class="meta-line">Authors: Sheng Yin, Xianghe Pang, Yuanzhuo Ding, Menglan Chen, Yutong Bi, Yichen Xiong, Wenhao Huang, Zhen Xiang, Jing Shao, Siheng Chen</div>
<div class="meta-line">First: 2024-12-17T18:55:58+00:00 · Latest: 2025-10-31T08:18:50+00:00</div>
<div class="meta-line">Comments: 28 pages, 19 tables, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.13178v5">Abs</a> · <a href="http://arxiv.org/pdf/2412.13178v5">PDF</a> · <a href="https://github.com/shengyin1224/SafeAgentBench">Code1</a> · <a href="https://huggingface.co/datasets/safeagentbench/SafeAgentBench">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the integration of large language models (LLMs), embodied agents have
strong capabilities to understand and plan complicated natural language
instructions. However, a foreseeable issue is that those embodied agents can
also flawlessly execute some hazardous tasks, potentially causing damages in
the real world. Existing benchmarks predominantly overlook critical safety
risks, focusing solely on planning performance, while a few evaluate LLMs&#x27;
safety awareness only on non-interactive image-text data. To address this gap,
we present SafeAgentBench -- the first comprehensive benchmark for safety-aware
task planning of embodied LLM agents in interactive simulation environments,
covering both explicit and implicit hazards. SafeAgentBench includes: (1) an
executable, diverse, and high-quality dataset of 750 tasks, rigorously curated
to cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal
embodied environment with a low-level controller, supporting multi-agent
execution with 17 high-level actions for 9 state-of-the-art baselines; and (3)
reliable evaluation methods from both execution and semantic perspectives.
Experimental results show that, although agents based on different design
frameworks exhibit substantial differences in task success rates, their overall
safety awareness remains weak. The most safety-conscious baseline achieves only
a 10% rejection rate for detailed hazardous tasks. Moreover, simply replacing
the LLM driving the agent does not lead to notable improvements in safety
awareness. Dataset and codes are available in
https://github.com/shengyin1224/SafeAgentBench and
https://huggingface.co/datasets/safeagentbench/SafeAgentBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeAgentBench：具身LLM代理的安全任务规划基准</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）的集成，具身代理在理解和规划复杂自然语言指令方面具有强大的能力。然而，一个可预见的问题是，这些具身代理也可以无缝执行一些危险任务，可能在现实世界中造成损害。现有基准主要忽视关键的安全风险，仅关注规划性能，而少数评估LLMs的安全意识仅基于非交互式的图像-文本数据。为了解决这一空白，我们提出了SafeAgentBench——第一个全面的安全意识任务规划基准，针对具身LLM代理在交互式仿真环境中的应用，涵盖显性和隐性危害。SafeAgentBench包括：（1）一个可执行的、多样化的高质量数据集，包含750个任务，严格策划以覆盖10种潜在危害和3种任务类型；（2）SafeAgentEnv，一个具有低级控制器的通用具身环境，支持多代理执行，提供9个最先进基线的17个高级动作；（3）从执行和语义角度出发的可靠评估方法。实验结果表明，尽管基于不同设计框架的代理在任务成功率上存在显著差异，但它们的整体安全意识仍然较弱。最具安全意识的基线在详细危险任务中的拒绝率仅为10%。此外，仅仅更换驱动代理的LLM并未显著提高安全意识。数据集和代码可在https://github.com/shengyin1224/SafeAgentBench和https://huggingface.co/datasets/safeagentbench/SafeAgentBench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical issue of safety in task planning for embodied agents utilizing large language models (LLMs), as existing benchmarks primarily focus on planning performance while neglecting safety risks. Previous methods have inadequately assessed safety awareness, often relying on non-interactive data, leading to a gap in evaluating real-world implications. The proposed SafeAgentBench offers a comprehensive benchmark that includes a diverse dataset of 750 tasks addressing various hazards, a universal embodied environment for multi-agent execution, and robust evaluation methods. This methodology enables a thorough assessment of safety-aware task planning, revealing that current agents, despite differing frameworks, exhibit weak safety awareness, with the best baseline achieving only a 10% rejection rate for hazardous tasks. The findings underscore the need for improved safety measures in embodied agents, supporting the goal of enhancing their operational safety in real-world scenarios.</div>
<div class="mono" style="margin-top:8px">本研究关注利用大型语言模型（LLM）的具身代理在任务规划中的安全性问题，因为现有基准主要关注规划性能，而忽视了安全风险。以往的方法在评估安全意识时往往依赖非交互数据，未能捕捉现实世界交互的复杂性。提出的SafeAgentBench引入了一个全面的基准，评估交互仿真环境中安全意识的任务规划，包含750个涵盖各种危险的多样化任务数据集和一个支持多代理执行的通用环境。该方法包括从执行和语义角度进行严格评估，结果显示当前代理的安全意识较弱，表现最佳的基线在危险任务中的拒绝率仅为10%。该基准旨在增强具身代理的安全能力，表明在任务执行中需要改进安全措施。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251106_0349.html">20251106_0349</a>
<a href="archive/20251105_0321.html">20251105_0321</a>
<a href="archive/20251104_1539.html">20251104_1539</a>
<a href="archive/20251104_0324.html">20251104_0324</a>
<a href="archive/20251103_0315.html">20251103_0315</a>
<a href="archive/20251102_1356.html">20251102_1356</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
