<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-05 03:21</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251105_0321</div>
    <div class="row"><div class="card">
<div class="title">SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM   Agents</div>
<div class="meta-line">Authors: Sheng Yin, Xianghe Pang, Yuanzhuo Ding, Menglan Chen, Yutong Bi, Yichen Xiong, Wenhao Huang, Zhen Xiang, Jing Shao, Siheng Chen</div>
<div class="meta-line">First: 2024-12-17T18:55:58+00:00 · Latest: 2025-10-31T08:18:50+00:00</div>
<div class="meta-line">Comments: 28 pages, 19 tables, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.13178v5">Abs</a> · <a href="http://arxiv.org/pdf/2412.13178v5">PDF</a> · <a href="https://github.com/shengyin1224/SafeAgentBench">Code1</a> · <a href="https://huggingface.co/datasets/safeagentbench/SafeAgentBench">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the integration of large language models (LLMs), embodied agents have
strong capabilities to understand and plan complicated natural language
instructions. However, a foreseeable issue is that those embodied agents can
also flawlessly execute some hazardous tasks, potentially causing damages in
the real world. Existing benchmarks predominantly overlook critical safety
risks, focusing solely on planning performance, while a few evaluate LLMs&#x27;
safety awareness only on non-interactive image-text data. To address this gap,
we present SafeAgentBench -- the first comprehensive benchmark for safety-aware
task planning of embodied LLM agents in interactive simulation environments,
covering both explicit and implicit hazards. SafeAgentBench includes: (1) an
executable, diverse, and high-quality dataset of 750 tasks, rigorously curated
to cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal
embodied environment with a low-level controller, supporting multi-agent
execution with 17 high-level actions for 9 state-of-the-art baselines; and (3)
reliable evaluation methods from both execution and semantic perspectives.
Experimental results show that, although agents based on different design
frameworks exhibit substantial differences in task success rates, their overall
safety awareness remains weak. The most safety-conscious baseline achieves only
a 10% rejection rate for detailed hazardous tasks. Moreover, simply replacing
the LLM driving the agent does not lead to notable improvements in safety
awareness. Dataset and codes are available in
https://github.com/shengyin1224/SafeAgentBench and
https://huggingface.co/datasets/safeagentbench/SafeAgentBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeAgentBench：具身LLM代理的安全任务规划基准</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）的集成，具身代理具备理解和规划复杂自然语言指令的强大能力。然而，一个可预见的问题是，这些具身代理也可能无误地执行一些危险任务，从而在现实世界中造成损害。现有基准主要忽视关键的安全风险，仅关注规划性能，而少数评估LLMs的安全意识仅基于非交互式的图像-文本数据。为了解决这一空白，我们提出了SafeAgentBench——第一个全面的安全意识任务规划基准，针对具身LLM代理在交互式仿真环境中的安全性，涵盖显性和隐性危害。SafeAgentBench包括：（1）一个可执行、多样化且高质量的750个任务数据集，严格策划以涵盖10种潜在危害和3种任务类型；（2）SafeAgentEnv，一个具有低级控制器的通用具身环境，支持9个最先进基线的17个高级动作的多代理执行；（3）从执行和语义角度出发的可靠评估方法。实验结果表明，尽管基于不同设计框架的代理在任务成功率上表现出显著差异，但它们的整体安全意识仍然较弱。最具安全意识的基线在详细危险任务中的拒绝率仅为10%。此外，仅仅更换驱动代理的LLM并未显著改善安全意识。数据集和代码可在https://github.com/shengyin1224/SafeAgentBench和https://huggingface.co/datasets/safeagentbench/SafeAgentBench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical issue of safety in task planning for embodied agents powered by large language models (LLMs), as existing benchmarks primarily focus on planning performance while neglecting safety risks. Previous methods have inadequately evaluated safety awareness, often relying on non-interactive data, which fails to capture real-world hazards. The proposed SafeAgentBench offers a comprehensive benchmark that includes a diverse dataset of 750 tasks covering various hazards and a universal environment for multi-agent execution, thus filling the gap in safety-aware evaluations. The methodology involves rigorous task curation and reliable evaluation methods from both execution and semantic perspectives. Experimental findings reveal that despite differences in design frameworks, agents exhibit weak safety awareness, with the best baseline achieving only a 10% rejection rate for hazardous tasks, indicating a need for improved safety measures in embodied agents.</div>
<div class="mono" style="margin-top:8px">本文关注大型语言模型（LLMs）驱动的具身代理在任务规划中的安全性问题，因为现有基准主要关注规划性能，而忽视了安全风险。以往的方法未能充分评估交互环境中的安全性，导致对这些代理如何执行危险任务的理解存在空白。提出的SafeAgentBench作为一个综合基准，包含750个涵盖各种危险的多样化任务数据集、一个支持多代理执行的通用具身环境以及可靠的评估方法。该方法论涉及评估代理在安全意识任务规划中的表现，结果显示尽管设计框架存在差异，但整体安全意识仍然不足，表现最佳的基线在危险任务中的拒绝率仅为10%。这些发现突显了在具身代理中改善安全措施的必要性，支持了SafeAgentBench作为未来该领域研究的重要工具的需求。</div>
</details>
</div>
<div class="card">
<div class="title">ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for   Audio-Language Models</div>
<div class="meta-line">Authors: Weifei Jin, Yuxin Cao, Junjie Su, Minhui Xue, Jie Hao, Ke Xu, Jin Song Dong, Derui Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-30T03:19:59+00:00 · Latest: 2025-10-30T03:19:59+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.26096v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.26096v1">PDF</a> · <a href="https://github.com/WeifeiJin/ALMGuard">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Audio-Language Models (ALMs) have significantly improved
multimodal understanding capabilities. However, the introduction of the audio
modality also brings new and unique vulnerability vectors. Previous studies
have proposed jailbreak attacks that specifically target ALMs, revealing that
defenses directly transferred from traditional audio adversarial attacks or
text-based Large Language Model (LLM) jailbreaks are largely ineffective
against these ALM-specific threats. To address this issue, we propose ALMGuard,
the first defense framework tailored to ALMs. Based on the assumption that
safety-aligned shortcuts naturally exist in ALMs, we design a method to
identify universal Shortcut Activation Perturbations (SAPs) that serve as
triggers that activate the safety shortcuts to safeguard ALMs at inference
time. To better sift out effective triggers while preserving the model&#x27;s
utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM),
which restricts perturbations to Mel-frequency bins that are sensitive to
jailbreaks but insensitive to speech understanding. Both theoretical analyses
and empirical results demonstrate the robustness of our method against both
seen and unseen attacks. Overall, \MethodName reduces the average success rate
of advanced ALM-specific jailbreak attacks to 4.6% across four models, while
maintaining comparable utility on benign benchmarks, establishing it as the new
state of the art. Our code and data are available at
https://github.com/WeifeiJin/ALMGuard.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ALMGuard：音频语言模型的安全捷径及其发现方式</div>
<div class="mono" style="margin-top:8px">音频语言模型（ALMs）的最新进展显著提高了多模态理解能力。然而，音频模态的引入也带来了新的独特脆弱性向量。之前的研究提出了专门针对ALMs的越狱攻击，揭示了直接从传统音频对抗攻击或基于文本的大型语言模型（LLM）越狱转移的防御在这些ALM特定威胁面前大多无效。为了解决这个问题，我们提出了ALMGuard，这是第一个针对ALMs量身定制的防御框架。基于安全对齐捷径自然存在于ALMs的假设，我们设计了一种方法来识别通用捷径激活扰动（SAPs），作为触发器以激活安全捷径，在推理时保护ALMs。为了更好地筛选有效触发器，同时保持模型在良性任务上的效用，我们进一步提出了梅尔梯度稀疏掩码（M-GSM），它将扰动限制在对越狱敏感但对语音理解不敏感的梅尔频率区间。理论分析和实证结果均表明我们的方法对已知和未知攻击的鲁棒性。总体而言，\MethodName将四个模型中先进的ALM特定越狱攻击的平均成功率降低到4.6%，同时在良性基准上保持了可比的效用，确立了其作为新的最先进技术。我们的代码和数据可在https://github.com/WeifeiJin/ALMGuard获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities introduced by audio modalities in Audio-Language Models (ALMs), which have been shown to be susceptible to jailbreak attacks that traditional defenses fail to mitigate. Previous methods, primarily derived from audio adversarial attacks or text-based Large Language Model defenses, have proven ineffective against these specific threats. The proposed approach, ALMGuard, is motivated by the existence of safety-aligned shortcuts within ALMs and introduces a novel defense framework that identifies universal Shortcut Activation Perturbations (SAPs) to activate these safety mechanisms during inference. The methodology includes the Mel-Gradient Sparse Mask (M-GSM) to ensure that perturbations target Mel-frequency bins sensitive to attacks while preserving model performance on benign tasks. Experimental results indicate that ALMGuard reduces the average success rate of advanced jailbreak attacks to 4.6% across four models while maintaining comparable utility on benign benchmarks, establishing a new state of the art in this domain.</div>
<div class="mono" style="margin-top:8px">本研究针对音频语言模型（ALMs）引入的脆弱性进行探讨，这些模型在多模态理解方面取得了进展，但也容易受到独特的越狱攻击。以往从传统音频对抗攻击或基于文本的大型语言模型（LLM）越狱中衍生出的防御措施在应对这些ALM特定威胁时效果不佳。所提出的方法ALMGuard基于安全对齐捷径在ALMs中存在的假设，设计了一种识别通用捷径激活扰动（SAPs）的方法，以在推理时激活这些安全机制。该方法还引入了梅尔梯度稀疏掩码（M-GSM），确保扰动仅限于对越狱敏感但对语音理解不敏感的梅尔频率区间。实验结果表明，ALMGuard将四个模型中先进的ALM特定越狱攻击的平均成功率降低至4.6%，同时在良性任务上保持了相当的性能，确立了该领域的新技术标准。</div>
</details>
</div>
<div class="card">
<div class="title">Improving LLM Safety Alignment with Dual-Objective Optimization</div>
<div class="meta-line">Authors: Xuandong Zhao, Will Cai, Tianneng Shi, David Huang, Licong Lin, Song Mei, Dawn Song</div>
<div class="meta-line">Venue: ICML 2025</div>
<div class="meta-line">First: 2025-03-05T18:01:05+00:00 · Latest: 2025-10-30T01:16:06+00:00</div>
<div class="meta-line">Comments: ICML 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.03710v3">Abs</a> · <a href="http://arxiv.org/pdf/2503.03710v3">PDF</a> · <a href="https://github.com/wicai24/DOOR-Alignment">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing training-time safety alignment techniques for large language models
(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization
(DPO), a widely deployed alignment method, exhibits limitations in both
experimental and theoretical contexts as its loss function proves suboptimal
for refusal learning. Through gradient-based analysis, we identify these
shortcomings and propose an improved safety alignment that disentangles DPO
objectives into two components: (1) robust refusal training, which encourages
refusal even when partial unsafe generations are produced, and (2) targeted
unlearning of harmful knowledge. This approach significantly increases LLM
robustness against a wide range of jailbreak attacks, including prefilling,
suffix, and multi-turn attacks across both in-distribution and
out-of-distribution scenarios. Furthermore, we introduce a method to emphasize
critical refusal tokens by incorporating a reward-based token-level weighting
mechanism for refusal learning, which further improves the robustness against
adversarial exploits. Our research also suggests that robustness to jailbreak
attacks is correlated with token distribution shifts in the training process
and internal representations of refusal and harmful tokens, offering valuable
directions for future research in LLM safety alignment. The code is available
at https://github.com/wicai24/DOOR-Alignment</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过双目标优化提高大型语言模型的安全对齐</div>
<div class="mono" style="margin-top:8px">现有的大型语言模型（LLMs）训练时安全对齐技术仍然容易受到越狱攻击。直接偏好优化（DPO）作为一种广泛应用的对齐方法，在实验和理论背景下均表现出局限性，因为其损失函数在拒绝学习中被证明是次优的。通过基于梯度的分析，我们识别了这些缺陷，并提出了一种改进的安全对齐方法，将DPO目标分解为两个组成部分：（1）强健的拒绝训练，鼓励在产生部分不安全生成时仍然拒绝，以及（2）针对有害知识的有针对性的去学习。这种方法显著提高了LLM对各种越狱攻击的鲁棒性，包括预填充、后缀和多轮攻击，涵盖了分布内和分布外场景。此外，我们引入了一种通过结合基于奖励的令牌级加权机制来强调关键拒绝令牌的方法，以进一步提高对抗性利用的鲁棒性。我们的研究还表明，对越狱攻击的鲁棒性与训练过程中的令牌分布变化以及拒绝和有害令牌的内部表示相关，为未来LLM安全对齐的研究提供了宝贵的方向。代码可在 https://github.com/wicai24/DOOR-Alignment 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of large language models (LLMs) to jailbreak attacks, highlighting the limitations of existing training-time safety alignment techniques, particularly direct preference optimization (DPO), which is found to be suboptimal for refusal learning. The proposed method improves upon DPO by disentangling its objectives into robust refusal training and targeted unlearning of harmful knowledge, effectively addressing the identified shortcomings. This dual-objective optimization approach enhances LLM robustness against various jailbreak attacks, including prefilling, suffix, and multi-turn attacks, in both in-distribution and out-of-distribution contexts. The methodology includes a reward-based token-level weighting mechanism to emphasize critical refusal tokens, which further strengthens the model&#x27;s defenses against adversarial exploits. The findings indicate a correlation between robustness to jailbreak attacks and shifts in token distribution during training, providing insights for future research in LLM safety alignment.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在面对越狱攻击时的脆弱性，强调了现有训练时安全对齐技术的局限性，特别是直接偏好优化（DPO），其在拒绝学习方面表现不佳。所提出的方法通过将DPO的目标分解为稳健的拒绝训练和有针对性的有害知识去除，有效解决了以往方法的不足。该研究的贡献在于通过一种新颖的基于奖励的令牌级加权机制显著增强了LLM对各种越狱攻击的鲁棒性。该方法论包括基于梯度的分析和双目标优化框架的实施，展示了在抵御分布内和分布外攻击方面的改进性能，从而支持了更安全的LLM部署目标。</div>
</details>
</div>
<div class="card">
<div class="title">MixAT: Combining Continuous and Discrete Adversarial Training for LLMs</div>
<div class="meta-line">Authors: Csaba Dékány, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, Martin Vechev</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-22T17:32:50+00:00 · Latest: 2025-10-28T09:41:22+00:00</div>
<div class="meta-line">Comments: Published at 39th Conference on Neural Information Processing Systems
  (NeurIPS 2025)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.16947v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.16947v2">PDF</a> · <a href="https://github.com/insait-institute/MixAT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent efforts in Large Language Model (LLM) safety and alignment,
current adversarial attacks on frontier LLMs can still consistently force
harmful generations. Although adversarial training has been widely studied and
shown to significantly improve the robustness of traditional machine learning
models, its strengths and weaknesses in the context of LLMs are less
understood. Specifically, while existing discrete adversarial attacks are
effective at producing harmful content, training LLMs with concrete adversarial
prompts is often computationally expensive, leading to reliance on continuous
relaxations. At the same time, despite their effectiveness and generalization
capabilities, training with continuous perturbations does not always capture
the full spectrum of vulnerabilities exploited by discrete attacks. In this
work, we aim to bridge this gap by introducing MixAT, a novel method that
combines stronger discrete and faster continuous attacks during training. We
rigorously evaluate MixAT across a wide spectrum of state-of-the-art attacks,
proposing the At Least One Attack Success Rate (ALO-ASR) metric to capture the
worst-case vulnerability of models. We show MixAT achieves substantially better
robustness (ALO-ASR &lt; 20%) compared to prior defenses (ALO-ASR &gt; 50%), while
maintaining a runtime comparable to methods based on continuous relaxations. We
further analyze MixAT in realistic deployment settings, exploring how chat
templates, quantization, low-rank adapters, and temperature affect both
adversarial training and evaluation, revealing additional blind spots in
current methodologies. Our results demonstrate that MixAT&#x27;s discrete-continuous
defense offers a principled and superior robustness-accuracy tradeoff with
minimal computational overhead, highlighting its promise for building safer
LLMs. We provide our code and models at
https://github.com/insait-institute/MixAT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MixAT：结合连续和离散对抗训练的LLM</div>
<div class="mono" style="margin-top:8px">尽管近期在大型语言模型（LLM）安全性和对齐方面进行了努力，当前对前沿LLM的对抗攻击仍能持续强制生成有害内容。虽然对抗训练已被广泛研究并显示出显著提高传统机器学习模型的鲁棒性，但在LLM背景下其优缺点尚不清楚。具体而言，现有的离散对抗攻击在生成有害内容方面有效，但使用具体对抗提示训练LLM通常计算成本高，导致依赖于连续松弛。同时，尽管连续扰动的训练有效且具有泛化能力，但并不总能捕捉到离散攻击所利用的全部脆弱性。在本研究中，我们旨在通过引入MixAT这一新方法来填补这一空白，该方法在训练过程中结合了更强的离散攻击和更快的连续攻击。我们在广泛的最先进攻击中严格评估MixAT，提出了至少一个攻击成功率（ALO-ASR）指标，以捕捉模型的最坏情况脆弱性。我们展示了MixAT在鲁棒性方面显著优于先前的防御（ALO-ASR &lt; 20%），而运行时间与基于连续松弛的方法相当。我们进一步分析了MixAT在现实部署环境中的表现，探讨了聊天模板、量化、低秩适配器和温度如何影响对抗训练和评估，揭示了当前方法中的额外盲点。我们的结果表明，MixAT的离散-连续防御提供了原则性和优越的鲁棒性-准确性权衡，计算开销最小，突显了其构建更安全LLM的潜力。我们在https://github.com/insait-institute/MixAT提供了我们的代码和模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the ongoing challenges of safety and alignment in Large Language Models (LLMs), particularly in the context of adversarial attacks that can lead to harmful outputs. Previous methods primarily focused on discrete adversarial training, which, while effective, is computationally intensive and often relies on continuous relaxations that fail to capture all vulnerabilities. The proposed MixAT method innovatively combines discrete and continuous adversarial training, effectively bridging the gap between the two approaches. This paper contributes by introducing a new evaluation metric, At Least One Attack Success Rate (ALO-ASR), and demonstrating that MixAT achieves significantly improved robustness (ALO-ASR &lt; 20%) compared to existing defenses (ALO-ASR &gt; 50%), while maintaining comparable computational efficiency. The methodology is rigorously tested across various state-of-the-art attacks, and the findings indicate that MixAT provides a better robustness-accuracy tradeoff, making it a promising solution for enhancing the safety of LLMs.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）在安全性和对齐方面的持续挑战，特别是在面对可能导致有害输出的对抗性攻击时。以往的方法主要依赖于离散对抗训练，虽然有效，但计算成本高昂，而连续扰动则未能充分捕捉离散攻击所利用的脆弱性。所提出的MixAT方法结合了离散和连续对抗训练，有效地弥合了这两种方法之间的差距。本文贡献了一种新的评估指标，即至少一次攻击成功率（ALO-ASR），用于评估模型脆弱性，并证明MixAT在提高鲁棒性方面显著优于现有防御（ALO-ASR &lt; 20% vs ALO-ASR &gt; 50%），同时保持了可比的计算效率。研究结果表明，MixAT提供了良好的鲁棒性与准确性之间的权衡，使其成为增强LLMs在实际应用中安全性的有希望的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">SAGE: A Generic Framework for LLM Safety Evaluation</div>
<div class="meta-line">Authors: Madhur Jindal, Hari Shrawgi, Parag Agrawal, Sandipan Dandapat</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-04-28T11:01:08+00:00 · Latest: 2025-10-27T10:19:55+00:00</div>
<div class="meta-line">Comments: Accepted to EMNLP 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.19674v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.19674v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Language Models are rapidly deployed across diverse applications
from healthcare to financial advice, safety evaluation struggles to keep pace.
Current benchmarks focus on single-turn interactions with generic policies,
failing to capture the conversational dynamics of real-world usage and the
application-specific harms that emerge in context. Such potential oversights
can lead to harms that go unnoticed in standard safety benchmarks and other
current evaluation methodologies. To address these needs for robust AI safety
evaluation, we introduce SAGE (Safety AI Generic Evaluation), an automated
modular framework designed for customized and dynamic harm evaluations. SAGE
employs prompted adversarial agents with diverse personalities based on the Big
Five model, enabling system-aware multi-turn conversations that adapt to target
applications and harm policies. We evaluate seven state-of-the-art LLMs across
three applications and harm policies. Multi-turn experiments show that harm
increases with conversation length, model behavior varies significantly when
exposed to different user personalities and scenarios, and some models minimize
harm via high refusal rates that reduce usefulness. We also demonstrate policy
sensitivity within a harm category where tightening a child-focused sexual
policy substantially increases measured defects across applications. These
results motivate adaptive, policy-aware, and context-specific testing for safer
real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAGE：大型语言模型安全评估的通用框架</div>
<div class="mono" style="margin-top:8px">随着大型语言模型在医疗保健到金融建议等多种应用中迅速部署，安全评估难以跟上步伐。目前的基准测试侧重于与通用策略的单轮交互，未能捕捉到真实使用中的对话动态和上下文中出现的特定应用危害。这些潜在的疏漏可能导致在标准安全基准和其他当前评估方法中未被注意的危害。为满足对强大AI安全评估的需求，我们引入了SAGE（安全AI通用评估），这是一个旨在定制和动态危害评估的自动化模块化框架。SAGE采用基于五大人格模型的多样化个性化对抗代理，能够进行系统感知的多轮对话，适应目标应用和危害政策。我们在三个应用和危害政策中评估了七个最先进的LLM。多轮实验表明，随着对话长度的增加，危害也在增加；模型行为在不同用户个性和场景下显著变化；一些模型通过高拒绝率来最小化危害，从而降低了实用性。我们还展示了在一个危害类别内的政策敏感性，其中收紧以儿童为中心的性政策显著增加了跨应用的缺陷测量。这些结果促使我们进行适应性、政策感知和上下文特定的测试，以实现更安全的现实世界部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the urgent need for effective safety evaluation of Large Language Models (LLMs) as they are increasingly used in various applications, highlighting the limitations of existing benchmarks that primarily assess single-turn interactions without considering the complexities of real-world conversations. Previous methods have failed to account for the conversational dynamics and specific harms that can arise in context, leading to potential oversights in safety assessments. The proposed SAGE framework offers a solution by utilizing prompted adversarial agents with diverse personalities based on the Big Five model, facilitating system-aware multi-turn conversations that adapt to specific applications and harm policies. This paper contributes by demonstrating the effectiveness of SAGE in evaluating seven state-of-the-art LLMs across three applications, revealing that harm increases with conversation length and that model behavior is significantly influenced by user personality and scenario. The findings support the need for adaptive, policy-aware, and context-specific testing to enhance the safety of LLM deployments in real-world settings.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）在各类应用中快速部署所带来的安全评估需求，指出现有基准测试主要集中在单轮交互和通用策略上，未能考虑真实场景中的对话动态和特定危害，导致安全评估可能存在疏漏。以往方法未能有效捕捉这些问题，而提出的SAGE框架通过利用基于五大人格模型的多样化个性化对抗代理，提供了一种新颖的方法，支持针对特定应用和危害政策的系统感知多轮对话评估。这种方法使得对LLMs的评估更加细致，研究发现，随着对话长度的增加，危害往往上升，模型行为受到用户个性和场景的显著影响。研究结果表明，需要进行适应性、政策意识的测试，以提高LLMs在实际部署中的安全性，评估七种最先进模型在三种应用中的表现，展示了该框架在识别特定上下文风险方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense   Against Adversarial LLM Jailbreaks</div>
<div class="meta-line">Authors: Md. Mehedi Hasan, Ziaur Rahman, Rafid Mostafiz, Md. Abir Hossain</div>
<div class="meta-line">First: 2025-10-26T11:19:47+00:00 · Latest: 2025-10-26T11:19:47+00:00</div>
<div class="meta-line">Comments: 11 pages, 5 figures. Preprint version under review in the area of
  Artificial Intelligence (cs.AI)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22628v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.22628v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a real-time modular defense system named Sentra-Guard.
The system detects and mitigates jailbreak and prompt injection attacks
targeting large language models (LLMs). The framework uses a hybrid
architecture with FAISS-indexed SBERT embedding representations that capture
the semantic meaning of prompts, combined with fine-tuned transformer
classifiers, which are machine learning models specialized for distinguishing
between benign and adversarial language inputs. It identifies adversarial
prompts in both direct and obfuscated attack vectors. A core innovation is the
classifier-retriever fusion module, which dynamically computes context-aware
risk scores that estimate how likely a prompt is to be adversarial based on its
content and context. The framework ensures multilingual resilience with a
language-agnostic preprocessing layer. This component automatically translates
non-English prompts into English for semantic evaluation, enabling consistent
detection across over 100 languages. The system includes a HITL feedback loop,
where decisions made by the automated system are reviewed by human experts for
continual learning and rapid adaptation under adversarial pressure.
Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and
malicious prompts, enhancing detection reliability and reducing false
positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 =
1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading
baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike
black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible
with diverse LLM backends. Its modular design supports scalable deployment in
both commercial and open-source environments. The system establishes a new
state-of-the-art in adversarial LLM defense.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Sentra-Guard：一种多语言人机框架，用于实时防御对抗性LLM越狱</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为Sentra-Guard的实时模块化防御系统。该系统检测并缓解针对大型语言模型（LLMs）的越狱和提示注入攻击。该框架采用混合架构，使用FAISS索引的SBERT嵌入表示来捕捉提示的语义意义，结合微调的变换器分类器，这些机器学习模型专门用于区分良性和对抗性语言输入。它识别直接和模糊攻击向量中的对抗性提示。核心创新是分类器-检索器融合模块，它动态计算上下文感知风险评分，估计提示基于其内容和上下文的对抗性可能性。该框架确保多语言的韧性，具有语言无关的预处理层。该组件自动将非英语提示翻译成英语进行语义评估，从而在100多种语言中实现一致检测。该系统包括一个HITL反馈循环，自动系统做出的决策由人类专家审查，以便在对抗压力下进行持续学习和快速适应。Sentra-Guard维护一个不断发展的良性和恶意提示的双标签知识库，提高检测可靠性并减少误报。评估结果显示检测率为99.96%（AUC = 1.00，F1 = 1.00），攻击成功率（ASR）仅为0.004%。这优于领先的基线，如LlamaGuard-2（1.3%）和OpenAI Moderation（3.7%）。与黑箱方法不同，Sentra-Guard是透明的、可微调的，并与多种LLM后端兼容。其模块化设计支持在商业和开源环境中的可扩展部署。该系统在对抗性LLM防御中建立了新的最先进水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concern of adversarial attacks on large language models (LLMs), specifically focusing on jailbreak and prompt injection attacks that compromise model integrity. Previous methods often lacked transparency and adaptability, leading to higher false positive rates and limited effectiveness against diverse attack vectors. The proposed Sentra-Guard framework introduces a hybrid architecture that combines FAISS-indexed SBERT embeddings with fine-tuned transformer classifiers, enabling real-time detection and mitigation of adversarial prompts. This approach is well-motivated as it incorporates a classifier-retriever fusion module that computes context-aware risk scores and includes a language-agnostic preprocessing layer for multilingual support. The system demonstrates exceptional performance with a 99.96% detection rate and an attack success rate of only 0.004%, significantly outperforming existing solutions like LlamaGuard-2 and OpenAI Moderation, thus establishing a new benchmark in adversarial defense for LLMs.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLM）面临的日益严重的对抗性攻击问题，特别是监狱突破和提示注入攻击，这些攻击会损害其功能。以往的方法在检测准确性和适应性方面存在不足，通常依赖于缺乏透明度的黑箱方法。提出的Sentra-Guard框架引入了一种混合架构，结合了FAISS索引的SBERT嵌入和微调的变换器分类器，从而有效识别各种攻击向量中的对抗性提示。这种方法具有良好的动机，通过分类器-检索器融合模块计算上下文感知风险评分，并包括人机反馈机制以实现持续学习，从而增强检测的可靠性。该方法展示了99.96%的检测率和仅0.004%的攻击成功率，显著优于现有系统如LlamaGuard-2和OpenAI Moderation，从而在对抗性LLM防御中建立了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks</div>
<div class="meta-line">Authors: Havva Alizadeh Noughabi, Julien Serbanescu, Fattane Zarrinkalam, Ali Dehghantanha</div>
<div class="meta-line">First: 2025-10-24T19:20:23+00:00 · Latest: 2025-10-24T19:20:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.21983v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.21983v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent advances, Large Language Models remain vulnerable to jailbreak
attacks that bypass alignment safeguards and elicit harmful outputs. While
prior research has proposed various attack strategies differing in human
readability and transferability, little attention has been paid to the
linguistic and psychological mechanisms that may influence a model&#x27;s
susceptibility to such attacks. In this paper, we examine an interdisciplinary
line of research that leverages foundational theories of persuasion from the
social sciences to craft adversarial prompts capable of circumventing alignment
constraints in LLMs. Drawing on well-established persuasive strategies, we
hypothesize that LLMs, having been trained on large-scale human-generated text,
may respond more compliantly to prompts with persuasive structures.
Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive
fingerprints that emerge in their jailbreak responses. Empirical evaluations
across multiple aligned LLMs reveal that persuasion-aware prompts significantly
bypass safeguards, demonstrating their potential to induce jailbreak behaviors.
This work underscores the importance of cross-disciplinary insight in
addressing the evolving challenges of LLM safety. The code and data are
available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>揭示大型语言模型在越狱攻击中的说服指纹</div>
<div class="mono" style="margin-top:8px">尽管最近取得了进展，大型语言模型仍然容易受到越狱攻击，这些攻击绕过了对齐保护措施并引发有害输出。虽然之前的研究提出了各种不同的人类可读性和可转移性的攻击策略，但对可能影响模型对这些攻击的易感性的语言和心理机制关注较少。本文考察了一条跨学科的研究路线，利用社会科学中的说服基础理论，设计能够绕过大型语言模型对齐约束的对抗性提示。基于成熟的说服策略，我们假设大型语言模型在接受大量人类生成文本的训练后，可能对具有说服结构的提示反应更顺从。此外，我们还研究大型语言模型是否在其越狱响应中表现出独特的说服指纹。对多个对齐大型语言模型的实证评估表明，关注说服的提示显著绕过了保护措施，展示了它们诱发越狱行为的潜力。这项工作强调了跨学科见解在应对大型语言模型安全不断演变的挑战中的重要性。代码和数据可用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of Large Language Models (LLMs) to jailbreak attacks that exploit alignment safeguards, a concern that has not been thoroughly examined in terms of the linguistic and psychological factors influencing model susceptibility. Previous methods focused primarily on various attack strategies but often overlooked the underlying persuasive mechanisms that could enhance effectiveness. This paper proposes an interdisciplinary approach that utilizes established theories of persuasion from social sciences to create adversarial prompts designed to bypass LLM alignment constraints, thereby addressing the limitations of past methods. The methodology involves empirical evaluations of multiple aligned LLMs to assess the impact of persuasion-aware prompts on their responses. The findings indicate that these prompts significantly increase the likelihood of bypassing safeguards, highlighting the potential of persuasive strategies in inducing jailbreak behaviors and emphasizing the need for cross-disciplinary insights in LLM safety research.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在越狱攻击中的脆弱性，这些攻击能够绕过对齐保护措施并产生有害输出。以往的方法侧重于各种攻击策略，但往往忽视了影响模型易受攻击性的语言和心理因素。所提出的方法结合了社会科学中的说服理论，创建了旨在利用这些因素的对抗性提示，从而增强越狱尝试的有效性。研究方法包括对多个对齐LLM的实证评估，结果表明，关注说服的提示可以显著绕过现有的保护措施。研究结果表明，这些提示可以诱发越狱行为，强调了跨学科方法在提高LLM安全性方面的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Shape it Up! Restoring LLM Safety during Finetuning</div>
<div class="meta-line">Authors: ShengYun Peng, Pin-Yu Chen, Jianfeng Chi, Seongmin Lee, Duen Horng Chau</div>
<div class="meta-line">First: 2025-05-22T18:05:16+00:00 · Latest: 2025-10-24T14:34:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.17196v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.17196v2">PDF</a> · <a href="https://github.com/poloclub/star-dss">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Finetuning large language models (LLMs) enables user-specific customization
but introduces critical safety risks: even a few harmful examples can
compromise safety alignment. A common mitigation strategy is to update the
model more strongly on examples deemed safe, while downweighting or excluding
those flagged as unsafe. However, because safety context can shift within a
single example, updating the model equally on both harmful and harmless parts
of a response is suboptimal-a coarse treatment we term static safety shaping.
In contrast, we propose dynamic safety shaping (DSS), a framework that uses
fine-grained safety signals to reinforce learning from safe segments of a
response while suppressing unsafe content. To enable such fine-grained control
during finetuning, we introduce a key insight: guardrail models, traditionally
used for filtering, can be repurposed to evaluate partial responses, tracking
how safety risk evolves throughout the response, segment by segment. This leads
to the Safety Trajectory Assessment of Response (STAR), a token-level signal
that enables shaping to operate dynamically over the training sequence.
Building on this, we present STAR-DSS, guided by STAR scores, that robustly
mitigates finetuning risks and delivers substantial safety improvements across
diverse threats, datasets, and model families-all without compromising
capability on intended tasks. We encourage future safety research to build on
dynamic shaping principles for stronger mitigation against evolving finetuning
risks. Our code is publicly available at https://github.com/poloclub/star-dss.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>塑造它！在微调过程中恢复大型语言模型的安全性</div>
<div class="mono" style="margin-top:8px">微调大型语言模型（LLMs）使用户能够进行特定定制，但引入了关键的安全风险：即使是少量有害示例也可能破坏安全对齐。一种常见的缓解策略是对被认为安全的示例进行更强的模型更新，同时降低或排除被标记为不安全的示例。然而，由于安全上下文可能在单个示例中发生变化，平等地更新模型对有害和无害部分的响应是次优的——我们称之为静态安全塑造。相反，我们提出了动态安全塑造（DSS），这是一个利用细粒度安全信号来强化从响应的安全部分学习，同时抑制不安全内容的框架。为了在微调过程中实现这种细粒度控制，我们引入了一个关键见解：传统上用于过滤的护栏模型可以重新用于评估部分响应，跟踪安全风险如何在整个响应中逐段演变。这导致了响应的安全轨迹评估（STAR），这是一个令牌级信号，使塑造能够在训练序列中动态运行。在此基础上，我们提出了STAR-DSS，基于STAR分数，稳健地减轻微调风险，并在各种威胁、数据集和模型家族中提供显著的安全改进——所有这些都不妨碍在预期任务上的能力。我们鼓励未来的安全研究基于动态塑造原则，以更强的缓解措施应对不断演变的微调风险。我们的代码已公开发布在https://github.com/poloclub/star-dss。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical safety risks associated with fine-tuning large language models (LLMs), where even a few harmful examples can jeopardize safety alignment. Previous methods typically employed static safety shaping, which inadequately managed the varying safety contexts within a single example by treating harmful and harmless parts equally. The proposed dynamic safety shaping (DSS) framework improves upon this by utilizing fine-grained safety signals to enhance learning from safe segments while suppressing unsafe content, leveraging guardrail models to evaluate responses segment by segment. The contribution of this paper lies in the introduction of the Safety Trajectory Assessment of Response (STAR) and STAR-DSS, which provide a dynamic approach to mitigate finetuning risks effectively, achieving significant safety improvements across various threats and datasets without sacrificing model performance on intended tasks. The methodology demonstrates that the STAR-DSS framework can robustly enhance safety during the fine-tuning process, supporting the goal of maintaining safety alignment in LLMs.</div>
<div class="mono" style="margin-top:8px">本研究解决了微调大型语言模型（LLMs）所带来的安全风险，少量有害示例可能会危及安全对齐。传统方法通常采用静态安全塑形，这种方法对响应中有害和无害部分的处理不够充分，导致结果不理想。提出的动态安全塑形（DSS）框架通过利用细粒度的安全信号来增强对安全片段的学习，同时抑制不安全内容，从而改进了这一点。该方法的动机在于，护栏模型可以评估部分响应，使得对安全风险的评估更加细致。该方法论被称为响应的安全轨迹评估（STAR），为训练过程中的动态塑形提供了基于标记的信号。STAR-DSS方法在各种威胁和数据集上显示出显著的安全改进，同时保持了模型在预期任务上的性能，支持了增强微调期间LLM安全性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety   Evaluation via Role-Specialized Collaboration</div>
<div class="meta-line">Authors: Xiuyuan Chen, Jian Zhao, Yuchen Yuan, Tianle Zhang, Huilin Zhou, Zheng Zhu, Ping Hu, Linghe Kong, Chi Zhang, Weiran Huang, Xuelong Li</div>
<div class="meta-line">First: 2025-09-28T09:35:32+00:00 · Latest: 2025-10-23T03:33:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.25271v4">Abs</a> · <a href="http://arxiv.org/pdf/2509.25271v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing safety evaluation methods for large language models (LLMs) suffer
from inherent limitations, including evaluator bias and detection failures
arising from model homogeneity, which collectively undermine the robustness of
risk evaluation processes. This paper seeks to re-examine the risk evaluation
paradigm by introducing a theoretical framework that reconstructs the
underlying risk concept space. Specifically, we decompose the latent risk
concept space into three mutually exclusive subspaces: the explicit risk
subspace (encompassing direct violations of safety guidelines), the implicit
risk subspace (capturing potential malicious content that requires contextual
reasoning for identification), and the non-risk subspace. Furthermore, we
propose RADAR, a multi-agent collaborative evaluation framework that leverages
multi-round debate mechanisms through four specialized complementary roles and
employs dynamic update mechanisms to achieve self-evolution of risk concept
distributions. This approach enables comprehensive coverage of both explicit
and implicit risks while mitigating evaluator bias. To validate the
effectiveness of our framework, we construct an evaluation dataset comprising
800 challenging cases. Extensive experiments on our challenging testset and
public benchmarks demonstrate that RADAR significantly outperforms baseline
evaluation methods across multiple dimensions, including accuracy, stability,
and self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87%
improvement in risk identification accuracy compared to the strongest baseline
evaluation method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RADAR：一种风险意识动态多智能体框架，用于通过角色专门化协作评估大型语言模型的安全性</div>
<div class="mono" style="margin-top:8px">现有的大型语言模型（LLMs）安全评估方法存在固有局限性，包括评估者偏见和由于模型同质性导致的检测失败，这些问题共同削弱了风险评估过程的稳健性。本文旨在通过引入一个理论框架重新审视风险评估范式，该框架重构了潜在风险概念空间。具体而言，我们将潜在风险概念空间分解为三个相互排斥的子空间：显性风险子空间（涵盖对安全指南的直接违反）、隐性风险子空间（捕捉需要上下文推理才能识别的潜在恶意内容）和非风险子空间。此外，我们提出了RADAR，一个多智能体协作评估框架，利用四个专门互补角色的多轮辩论机制，并采用动态更新机制实现风险概念分布的自我演化。这种方法能够全面覆盖显性和隐性风险，同时减轻评估者偏见。为了验证我们框架的有效性，我们构建了一个包含800个挑战性案例的评估数据集。在我们的挑战性测试集和公共基准上的广泛实验表明，RADAR在多个维度上显著优于基线评估方法，包括准确性、稳定性和自我评估风险敏感性。值得注意的是，RADAR在风险识别准确性方面相比最强基线评估方法提高了28.87%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the limitations of existing safety evaluation methods for large language models (LLMs), which are often plagued by evaluator bias and detection failures due to model homogeneity. Previous methods have struggled to effectively identify both explicit and implicit risks, leading to inadequate risk assessments. The proposed approach, RADAR, introduces a multi-agent collaborative evaluation framework that utilizes a theoretical reconstruction of the risk concept space, decomposing it into explicit, implicit, and non-risk subspaces. This framework employs a dynamic multi-round debate mechanism among four specialized roles to enhance the robustness of risk evaluations and reduce bias. The contribution of this paper lies in its innovative methodology, which has been validated through extensive experiments on a dataset of 800 challenging cases, demonstrating a 28.87% improvement in risk identification accuracy over the strongest baseline methods, thereby supporting its goals of comprehensive risk evaluation.</div>
<div class="mono" style="margin-top:8px">本文针对现有大型语言模型（LLMs）安全评估方法的局限性进行探讨，这些方法常常受到评估者偏见和由于模型同质性导致的检测失败的影响。以往的方法在提供稳健的风险评估方面存在困难，促使作者提出了一种新的理论框架，将风险概念空间重新定义为三个独立的子空间：显性风险、隐性风险和非风险。本文提出了RADAR，一个多智能体协作评估框架，利用多轮辩论机制和四个专业角色的动态更新来增强评估过程。通过对800个具有挑战性的案例的数据集进行广泛实验，验证了该研究的贡献，结果表明RADAR在风险识别准确性上比基线方法提高了28.87%，从而支持其全面风险评估和减少偏见的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities</div>
<div class="meta-line">Authors: Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng, Aidan San, Michel Galley, Jianfeng Gao</div>
<div class="meta-line">Venue: NAACL 2025 Oral</div>
<div class="meta-line">First: 2024-10-24T06:36:12+00:00 · Latest: 2025-10-23T00:57:57+00:00</div>
<div class="meta-line">Comments: Accepted to NAACL 2025 Main (Oral)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2410.18469v5">Abs</a> · <a href="http://arxiv.org/pdf/2410.18469v5">PDF</a> · <a href="https://github.com/SunChungEn/ADV-LLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research has shown that Large Language Models (LLMs) are vulnerable to
automated jailbreak attacks, where adversarial suffixes crafted by algorithms
appended to harmful queries bypass safety alignment and trigger unintended
responses. Current methods for generating these suffixes are computationally
expensive and have low Attack Success Rates (ASR), especially against
well-aligned models like Llama2 and Llama3. To overcome these limitations, we
introduce ADV-LLM, an iterative self-tuning process that crafts adversarial
LLMs with enhanced jailbreak ability. Our framework significantly reduces the
computational cost of generating adversarial suffixes while achieving nearly
100\% ASR on various open-source LLMs. Moreover, it exhibits strong attack
transferability to closed-source models, achieving 99\% ASR on GPT-3.5 and 49\%
ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving
jailbreak ability, ADV-LLM provides valuable insights for future safety
alignment research through its ability to generate large datasets for studying
LLM safety. Our code is available at: https://github.com/SunChungEn/ADV-LLM</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强越狱能力的迭代自调节大型语言模型</div>
<div class="mono" style="margin-top:8px">最近的研究表明，大型语言模型（LLMs）易受自动化越狱攻击，算法生成的对抗后缀附加在有害查询上，绕过安全对齐并触发意外响应。目前生成这些后缀的方法计算成本高，攻击成功率（ASR）低，尤其是针对像Llama2和Llama3这样的良好对齐模型。为克服这些限制，我们提出了ADV-LLM，一种迭代自调节过程，旨在制作具有增强越狱能力的对抗LLMs。我们的框架显著降低了生成对抗后缀的计算成本，同时在各种开源LLMs上实现了近100%的ASR。此外，它对闭源模型表现出强大的攻击可转移性，在GPT-3.5上实现了99%的ASR，在GPT-4上实现了49%的ASR，尽管仅在Llama3上进行了优化。除了提高越狱能力，ADV-LLM还通过其生成大规模数据集的能力，为未来的安全对齐研究提供了宝贵的见解。我们的代码可在以下网址获取：https://github.com/SunChungEn/ADV-LLM</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of Large Language Models (LLMs) to automated jailbreak attacks, where adversarial suffixes can bypass safety measures and elicit unintended responses. Previous methods for generating these suffixes were computationally intensive and had low Attack Success Rates (ASR), particularly against well-aligned models like Llama2 and Llama3. The proposed ADV-LLM introduces an iterative self-tuning process that not only reduces the computational cost but also achieves nearly 100% ASR across various open-source LLMs, with strong transferability to closed-source models, achieving 99% ASR on GPT-3.5 and 49% on GPT-4. This contribution not only enhances jailbreak capabilities but also aids future safety alignment research by generating large datasets for LLM safety studies.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在自动越狱攻击中的脆弱性，其中对抗后缀可以绕过安全机制并引发意外响应。以往生成这些后缀的方法计算成本高且攻击成功率（ASR）低，特别是针对像Llama2和Llama3这样的良好对齐模型。提出的ADV-LLM引入了一种迭代自调节过程，显著降低了计算成本，同时在各种开源LLM上实现了近100%的ASR，并在闭源模型上表现出强大的攻击可转移性，在GPT-3.5上达到99%的ASR，在GPT-4上达到49%的ASR。这项工作不仅增强了越狱能力，还通过生成大量数据集为未来的安全对齐研究做出了贡献。</div>
</details>
</div>
<div class="card">
<div class="title">NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn   LLM Jailbreaks</div>
<div class="meta-line">Authors: Javad Rafiei Asl, Sidhant Narula, Mohammad Ghasemigol, Eduardo Blanco, Daniel Takabi</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-10-03T18:24:14+00:00 · Latest: 2025-10-21T17:41:58+00:00</div>
<div class="meta-line">Comments: This paper has been accepted in the main conference proceedings of
  the 2025 Conference on Empirical Methods in Natural Language Processing
  (EMNLP 2025). Javad Rafiei Asl and Sidhant Narula are co-first authors</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.03417v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.03417v2">PDF</a> · <a href="https://github.com/inspire-lab/NEXUS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have revolutionized natural language processing
but remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks
that distribute malicious intent across benign exchanges and bypass alignment
mechanisms. Existing approaches often explore the adversarial space poorly,
rely on hand-crafted heuristics, or lack systematic query refinement. We
present NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular
framework for constructing, refining, and executing optimized multi-turn
attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a
harmful intent into a structured semantic network of topics, entities, and
query chains; (2) a feedback-driven Simulator that iteratively refines and
prunes these chains through attacker-victim-judge LLM collaboration using
harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser
that adaptively navigates the refined query space for real-time attacks. This
pipeline uncovers stealthy, high-success adversarial paths across LLMs. On
several closed-source and open-source LLMs, NEXUS increases attack success rate
by 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of Large Language Models (LLMs) to jailbreak attacks, particularly multi-turn attacks that obscure malicious intent within benign interactions, which existing methods inadequately explore and often rely on ineffective heuristics. The proposed NEXUS framework differs by systematically constructing and refining optimized multi-turn attacks through a modular approach, which includes a hierarchical semantic network expansion, a feedback-driven simulator for iterative refinement, and an adaptive network traverser for real-time execution. This paper contributes a novel methodology that enhances the exploration of adversarial paths, achieving an increase in attack success rates by 2.1% to 19.4% across various closed-source and open-source LLMs, thereby supporting the goal of more effective jailbreak exploitation. The methodology&#x27;s effectiveness is demonstrated through its application in real-time attacks, showcasing its potential to uncover stealthy adversarial strategies.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLM）在监狱攻击中的脆弱性，特别是多轮监狱攻击，这种攻击巧妙地将恶意意图分散在看似无害的交互中，从而规避现有的对齐机制。以往的方法在对抗空间的探索不足，依赖手工设计的启发式方法，并缺乏系统的查询优化，而NEXUS旨在改善这些问题。所提出的方法具有良好的动机，提供了一个模块化框架，包括ThoughtNet用于有害意图的分层扩展、基于反馈的模拟器用于迭代优化，以及网络遍历器用于自适应查询空间导航。该方法显著提高了多轮攻击的有效性，在多个LLM上实现了攻击成功率比以往方法提高2.1%至19.4%的成果，从而支持了发现隐蔽对抗路径的目标。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Safety Alignment is Divergence Estimation in Disguise</div>
<div class="meta-line">Authors: Rajdeep Haldar, Ziyi Wang, Qifan Song, Guang Lin, Yue Xing</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-02-02T04:09:42+00:00 · Latest: 2025-10-20T19:47:59+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.00657v3">Abs</a> · <a href="http://arxiv.org/pdf/2502.00657v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a theoretical framework showing that popular LLM alignment
methods, including RLHF and its variants, can be understood as divergence
estimators between aligned (safe or preferred) and unaligned (harmful or less
preferred) distributions. This perspective explains the emergence of separation
in the latent space between safe and harmful prompts after alignment. As an
application of our general divergence framework, we propose KLDO, a novel KL
divergence-based alignment method, and empirically validate its effectiveness.
We further show that using compliance-refusal datasets, rather than standard
preference-based datasets, leads to stronger separation and improved safety
alignment. Finally, to quantify the separation effect, we propose a
distance-based metric in the prompt representation space, which also acts as a
statistically significant indicator for model safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM安全对齐是伪装的发散估计</div>
<div class="mono" style="margin-top:8px">我们提出了一个理论框架，表明流行的LLM对齐方法，包括RLHF及其变体，可以理解为对齐（安全或优选）和未对齐（有害或不太优选）分布之间的发散估计器。这一视角解释了对齐后安全和有害提示在潜在空间中出现分离的现象。作为我们一般发散框架的应用，我们提出了KLDO，一种基于KL发散的新型对齐方法，并实证验证了其有效性。我们进一步表明，使用合规-拒绝数据集而非标准偏好数据集，能够导致更强的分离和改善的安全对齐。最后，为了量化分离效果，我们提出了一种基于距离的度量，适用于提示表示空间，这也作为模型安全的统计显著指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges in aligning large language models (LLMs) with safety and preference standards, highlighting that existing methods like Reinforcement Learning from Human Feedback (RLHF) often fail to adequately separate safe from harmful outputs. The proposed approach, KLDO, reinterprets these alignment methods as divergence estimators, offering a theoretical framework that clarifies the observed separation in latent spaces post-alignment. This paper contributes by introducing KLDO and demonstrating its effectiveness through empirical validation, particularly emphasizing the use of compliance-refusal datasets over traditional preference-based datasets to enhance safety alignment. The methodology involves a novel application of KL divergence and the introduction of a distance-based metric for prompt representation, achieving improved safety alignment and quantifiable separation in model outputs, thus supporting the goals of enhanced model safety.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有大型语言模型（LLM）对齐方法的局限性，例如基于人类反馈的强化学习（RLHF），这些方法通常无法有效地在潜在空间中分离安全和有害的提示。提出的方法KLDO将这些对齐方法重新框架为发散估计器，并引入了一种新的基于KL散度的对齐方法，增强了对齐和未对齐分布之间的分离。该框架具有良好的动机，因为它为理解对齐问题提供了理论基础。该方法论包括使用合规-拒绝数据集，这些数据集相比传统的偏好数据集能够产生更强的分离和改进的安全对齐。本文贡献了一种用于量化提示表示空间中分离的新距离度量，证明KLDO在模型安全对齐方面取得了显著改善，从而有效支持其目标。</div>
</details>
</div>
<div class="card">
<div class="title">CultureGuard: Towards Culturally-Aware Dataset and Guard Model for   Multilingual Safety Applications</div>
<div class="meta-line">Authors: Raviraj Joshi, Rakesh Paul, Kanishk Singla, Anusha Kamath, Michael Evans, Katherine Luna, Shaona Ghosh, Utkarsh Vaidya, Eileen Long, Sanjay Singh Chauhan, Niranjan Wartikar</div>
<div class="meta-line">First: 2025-08-03T10:35:05+00:00 · Latest: 2025-10-19T16:44:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.01710v3">Abs</a> · <a href="http://arxiv.org/pdf/2508.01710v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing use of Large Language Models (LLMs) in agentic applications
highlights the need for robust safety guard models. While content safety in
English is well-studied, non-English languages lack similar advancements due to
the high cost of collecting culturally aligned labeled datasets. We present
CultureGuard, a novel solution for curating culturally aligned, high-quality
safety datasets across multiple languages. Our approach introduces a four-stage
synthetic data generation and filtering pipeline: cultural data segregation,
cultural data adaptation, machine translation, and quality filtering. This
pipeline enables the conversion and expansion of the
Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct
languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.
The resulting dataset, Nemotron-Safety-Guard-Dataset-v3, comprises 386,661
samples in 9 languages and facilitates the training of
Llama-3.1-Nemotron-Safety-Guard-8B-v3 via LoRA-based fine-tuning. The final
model achieves state-of-the-art performance on several multilingual content
safety benchmarks. Furthermore, we show our moderately multilingual fine-tuning
enables robust cross-lingual transfer and strong zero-shot generalization to
unseen languages. We also benchmark the latest open LLMs on multilingual safety
and observe that these LLMs are more prone to give unsafe responses when
prompted in non-English languages. This work advances multilingual LLM safety
by enabling the development of culturally aware safety guard models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CultureGuard：面向多语言安全应用的文化意识数据集和保护模型</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在代理应用中的日益使用突显了对强大安全保护模型的需求。虽然英语内容安全研究较为充分，但由于收集文化对齐标注数据集的高成本，非英语语言缺乏类似的进展。我们提出了CultureGuard，这是一种新颖的解决方案，用于策划跨多种语言的文化对齐高质量安全数据集。我们的方法引入了一个四阶段的合成数据生成和过滤流程：文化数据分离、文化数据适应、机器翻译和质量过滤。该流程使得将Nemotron-Content-Safety-Dataset-V2英语安全数据集转换和扩展为八种不同语言成为可能：阿拉伯语、德语、西班牙语、法语、印地语、日语、泰语和中文。最终生成的数据集Nemotron-Safety-Guard-Dataset-v3包含9种语言的386,661个样本，并通过基于LoRA的微调促进了Llama-3.1-Nemotron-Safety-Guard-8B-v3的训练。最终模型在多个多语言内容安全基准上达到了最先进的性能。此外，我们展示了我们的中等多语言微调能够实现强大的跨语言迁移和对未见语言的强大零样本泛化。我们还对最新的开放LLMs在多语言安全方面进行了基准测试，观察到这些LLMs在非英语语言提示时更容易给出不安全的响应。这项工作通过促进文化意识安全保护模型的发展，推动了多语言LLM安全性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the gap in content safety for non-English languages in the context of Large Language Models (LLMs), where existing methods have struggled due to the high costs associated with collecting culturally aligned labeled datasets. The proposed CultureGuard approach differs from past methods by implementing a four-stage synthetic data generation and filtering pipeline that includes cultural data segregation, adaptation, machine translation, and quality filtering, effectively curating a multilingual safety dataset. This paper contributes by creating the Nemotron-Safety-Guard-Dataset-v3, which consists of 386,661 samples across nine languages, and by training the Llama-3.1-Nemotron-Safety-Guard-8B-v3 model through LoRA-based fine-tuning. The model demonstrates state-of-the-art performance on various multilingual content safety benchmarks, supporting the goal of enhancing multilingual LLM safety and showcasing strong cross-lingual transfer capabilities and zero-shot generalization to unseen languages.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）在非英语语言中安全防护模型的不足，现有方法因缺乏文化对齐的标注数据集而面临挑战。以往的方法主要集中在英语，导致多语言应用的安全措施不足。提出的CultureGuard方法引入了一个四阶段的合成数据生成和过滤流程，包括文化数据分离、适应、机器翻译和质量过滤，有效地将一个英语安全数据集扩展到八种语言。该贡献导致了Nemotron-Safety-Guard-Dataset-v3的创建，包含386,661个样本，支持Llama-3.1-Nemotron-Safety-Guard-8B-v3模型的训练。该模型在多语言内容安全基准测试中表现出色，展示了其促进强大的跨语言迁移和对未见语言的强零-shot泛化能力，从而增强了多语言LLM应用的安全性。</div>
</details>
</div>
<div class="card">
<div class="title">Black-box Optimization of LLM Outputs by Asking for Directions</div>
<div class="meta-line">Authors: Jie Zhang, Meng Ding, Yang Liu, Jue Hong, Florian Tramèr</div>
<div class="meta-line">First: 2025-10-19T11:13:45+00:00 · Latest: 2025-10-19T11:13:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.16794v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.16794v1">PDF</a> · <a href="https://github.com/zj-jayzhang/black_box_llm_optimization">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel approach for attacking black-box large language models
(LLMs) by exploiting their ability to express confidence in natural language.
Existing black-box attacks require either access to continuous model outputs
like logits or confidence scores (which are rarely available in practice), or
rely on proxy signals from other models. Instead, we demonstrate how to prompt
LLMs to express their internal confidence in a way that is sufficiently
calibrated to enable effective adversarial optimization. We apply our general
method to three attack scenarios: adversarial examples for vision-LLMs,
jailbreaks and prompt injections. Our attacks successfully generate malicious
inputs against systems that only expose textual outputs, thereby dramatically
expanding the attack surface for deployed LLMs. We further find that better and
larger models exhibit superior calibration when expressing confidence, creating
a concerning security paradox where model capability improvements directly
enhance vulnerability. Our code is available at this
[link](https://github.com/zj-jayzhang/black_box_llm_optimization).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过询问方向对LLM输出进行黑箱优化</div>
<div class="mono" style="margin-top:8px">我们提出了一种新颖的方法，通过利用黑箱大型语言模型（LLM）在自然语言中表达信心的能力来进行攻击。现有的黑箱攻击要么需要访问连续的模型输出，如logits或置信度分数（在实践中很少可用），要么依赖于其他模型的代理信号。相反，我们展示了如何提示LLM以一种足够校准的方式表达其内部信心，从而实现有效的对抗优化。我们将我们的一般方法应用于三种攻击场景：视觉LLM的对抗样本、越狱和提示注入。我们的攻击成功生成了针对仅暴露文本输出的系统的恶意输入，从而显著扩大了已部署LLM的攻击面。我们进一步发现，更好和更大的模型在表达信心时表现出更优的校准，形成了一个令人担忧的安全悖论，即模型能力的提升直接增强了脆弱性。我们的代码可在此[链接](https://github.com/zj-jayzhang/black_box_llm_optimization)获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing black-box attacks on large language models (LLMs), which typically require access to continuous model outputs or rely on proxy signals from other models. The proposed approach innovatively prompts LLMs to express their internal confidence in a calibrated manner, enabling effective adversarial optimization without needing direct access to model internals. The contribution of this paper lies in demonstrating the feasibility of generating adversarial examples, jailbreaks, and prompt injections using only textual outputs from LLMs, thereby significantly broadening the attack surface for deployed models. The methodology involves leveraging the confidence expressed by LLMs to create malicious inputs, and the experiments show that larger and better-calibrated models are more vulnerable, highlighting a security paradox. The results indicate that the proposed method effectively supports the goal of enhancing adversarial attack strategies against LLMs.</div>
<div class="mono" style="margin-top:8px">本研究解决了对大型语言模型（LLM）进行黑箱攻击的挑战，传统方法通常需要访问连续的模型输出或依赖其他模型的代理信号，这在实际中往往不切实际。所提出的方法创新性地提示LLM以经过校准的方式表达其内部信心，从而在不需要直接访问模型内部的情况下实现有效的对抗优化。该研究的贡献在于展示了仅通过文本输出生成视觉LLM的对抗示例、越狱和提示注入的可行性，从而显著扩大了已部署LLM的攻击面。该方法论涉及利用LLM的信心表达来创建恶意输入，揭示了更大和更好的模型由于改进的校准可能无意中增加脆弱性。实验表明，所提出的方法成功生成攻击，支持了增强对LLM安全漏洞理解的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Active Honeypot Guardrail System: Probing and Confirming Multi-Turn LLM   Jailbreaks</div>
<div class="meta-line">Authors: ChenYu Wu, Yi Wang, Yang Liao</div>
<div class="meta-line">First: 2025-10-16T17:41:09+00:00 · Latest: 2025-10-16T17:41:09+00:00</div>
<div class="meta-line">Comments: 6pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15017v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.15017v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly vulnerable to multi-turn
jailbreak attacks, where adversaries iteratively elicit harmful behaviors that
bypass single-turn safety filters. Existing defenses predominantly rely on
passive rejection, which either fails against adaptive attackers or overly
restricts benign users. We propose a honeypot-based proactive guardrail system
that transforms risk avoidance into risk utilization. Our framework fine-tunes
a bait model to generate ambiguous, non-actionable but semantically relevant
responses, which serve as lures to probe user intent. Combined with the
protected LLM&#x27;s safe reply, the system inserts proactive bait questions that
gradually expose malicious intent through multi-turn interactions. We further
introduce the Honeypot Utility Score (HUS), measuring both the attractiveness
and feasibility of bait responses, and use a Defense Efficacy Rate (DER) for
balancing safety and usability. Initial experiment on MHJ Datasets with recent
attack method across GPT-4o show that our system significantly disrupts
jailbreak success while preserving benign user experience.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>主动蜜罐护栏系统：探测和确认多轮LLM越狱</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）越来越容易受到多轮越狱攻击，攻击者通过迭代引导有害行为，绕过单轮安全过滤器。现有防御主要依赖于被动拒绝，这对自适应攻击者无效或过度限制良性用户。我们提出了一种基于蜜罐的主动护栏系统，将风险规避转化为风险利用。我们的框架微调了一个诱饵模型，以生成模糊、不可操作但语义相关的响应，作为引诱用户意图的诱饵。结合受保护的LLM的安全回复，该系统插入主动诱饵问题，通过多轮交互逐渐暴露恶意意图。我们进一步引入蜜罐效用评分（HUS），衡量诱饵响应的吸引力和可行性，并使用防御有效率（DER）来平衡安全性和可用性。在MHJ数据集上进行的初步实验显示，我们的系统显著干扰了越狱成功，同时保持了良性用户体验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the increasing vulnerability of large language models (LLMs) to multi-turn jailbreak attacks, where adversaries exploit iterative interactions to bypass safety measures. Previous methods primarily employed passive rejection strategies, which either inadequately counter adaptive attackers or excessively limit legitimate users. The proposed honeypot-based proactive guardrail system shifts the focus from risk avoidance to risk utilization, utilizing a bait model that generates ambiguous yet relevant responses to probe user intent. This approach is well-motivated as it aims to enhance both safety and usability. The methodology includes fine-tuning a bait model and introducing metrics like the Honeypot Utility Score (HUS) and Defense Efficacy Rate (DER) to evaluate the effectiveness of bait responses. Experimental results on the MHJ Datasets demonstrate that the proposed system significantly reduces jailbreak success rates while maintaining a positive experience for benign users.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在多轮越狱攻击中日益增加的脆弱性，这些攻击利用迭代交互绕过安全过滤器。以往的方法主要采用被动拒绝策略，这些策略要么无法有效对抗适应性攻击者，要么过度限制合法用户。所提出的基于蜜罐的主动护栏系统将重点从风险规避转向风险利用，通过模糊的诱饵响应有效探测用户意图，同时保持受保护的LLM的安全回复。这种方法动机明确，旨在平衡安全性和可用性，贡献了新颖的蜜罐效用评分（HUS）和防御有效性率（DER）指标。该方法在MHJ数据集上针对近期攻击方法进行了测试，显示出显著降低越狱成功率，同时确保了良性用户的积极体验。</div>
</details>
</div>
<div class="card">
<div class="title">When Style Breaks Safety: Defending LLMs Against Superficial Style   Alignment</div>
<div class="meta-line">Authors: Yuxin Xiao, Sana Tonekaboni, Walter Gerych, Vinith Suriyakumar, Marzyeh Ghassemi</div>
<div class="meta-line">First: 2025-06-09T05:57:39+00:00 · Latest: 2025-10-16T06:50:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.07452v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.07452v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) can be prompted with specific styles (e.g.,
formatting responses as lists), including in malicious queries. Prior jailbreak
research mainly augments these queries with additional string transformations
to maximize attack success rate (ASR). However, the impact of style patterns in
the original queries that are semantically irrelevant to the malicious intent
remains unclear. In this work, we seek to understand whether style patterns
compromise LLM safety, how superficial style alignment increases model
vulnerability, and how best to mitigate these risks during alignment. We first
define ASR inflation as the increase in ASR due to style patterns in existing
jailbreak benchmark queries. By evaluating 32 LLMs across seven benchmarks, we
find that nearly all models exhibit ASR inflation. Notably, the inflation
correlates with an LLM&#x27;s relative attention to style patterns, which also
overlap more with its instruction-tuning data when inflation occurs. We then
investigate superficial style alignment, and find that fine-tuning with
specific styles makes LLMs more vulnerable to jailbreaks of those same styles.
Finally, we propose SafeStyle, a defense strategy that incorporates a small
amount of safety training data augmented to match the distribution of style
patterns in the fine-tuning data. Across three LLMs, six fine-tuning style
settings, and two real-world instruction-tuning datasets, SafeStyle
consistently outperforms baselines in maintaining LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当风格破坏安全性：保护大型语言模型免受表面风格对齐的影响</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）可以通过特定风格（例如，将响应格式化为列表）进行提示，包括在恶意查询中。之前的越狱研究主要通过额外的字符串转换来增强这些查询，以最大化攻击成功率（ASR）。然而，原始查询中与恶意意图语义无关的风格模式的影响仍不清楚。在本研究中，我们旨在了解风格模式是否会危害LLM安全性，表面风格对齐如何增加模型脆弱性，以及在对齐过程中如何最好地减轻这些风险。我们首先将ASR膨胀定义为由于现有越狱基准查询中的风格模式而导致的ASR增加。通过评估32个LLM在七个基准上的表现，我们发现几乎所有模型都表现出ASR膨胀。值得注意的是，膨胀与LLM对风格模式的相对关注度相关，当膨胀发生时，这些模式与其指令调优数据的重叠也更多。然后，我们研究表面风格对齐，发现使用特定风格进行微调使LLM对这些相同风格的越狱更加脆弱。最后，我们提出了SafeStyle，一种防御策略，结合少量安全训练数据，增强以匹配微调数据中的风格模式分布。在三个LLM、六个微调风格设置和两个真实世界的指令调优数据集上，SafeStyle在维护LLM安全性方面始终优于基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the vulnerability of large language models (LLMs) to malicious queries that exploit specific stylistic patterns, a concern that has not been thoroughly investigated in prior jailbreak studies which primarily focused on string transformations to enhance attack success rates (ASR). The proposed approach, termed SafeStyle, differs from existing methods by examining how superficial style alignment can increase model susceptibility to attacks and by defining ASR inflation as the rise in ASR due to style patterns in jailbreak benchmarks. The study finds that nearly all evaluated LLMs exhibit ASR inflation, which correlates with their attention to style patterns. SafeStyle mitigates these risks by incorporating a small amount of safety training data that aligns with the stylistic distribution of the fine-tuning data. The methodology demonstrates that SafeStyle consistently outperforms baseline defenses across multiple LLMs and fine-tuning settings, effectively enhancing LLM safety against style-based attacks.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在恶意查询中利用特定风格模式的脆弱性，这一问题在以往方法中未得到充分解决，过去的方法主要通过增强查询的字符串变换来提高攻击成功率（ASR）。所提出的方法SafeStyle通过研究表面风格对模型安全性的影响以及定义ASR膨胀（即由于监狱突破基准查询中的风格模式而导致的ASR增加）而有所不同。研究发现，几乎所有评估的LLMs都表现出ASR膨胀，这与它们对风格模式的关注程度相关，揭示了特定风格的微调会增加对相应监狱突破的脆弱性。SafeStyle通过结合少量与微调数据中的风格模式分布相匹配的安全训练数据来减轻这些风险，证明在三种LLM、六种微调风格设置和两个真实世界的指令微调数据集上，在保持LLM安全性方面表现优于基线。</div>
</details>
</div>
<div class="card">
<div class="title">When &quot;Competency&quot; in Reasoning Opens the Door to Vulnerability:   Jailbreaking LLMs via Novel Complex Ciphers</div>
<div class="meta-line">Authors: Divij Handa, Zehua Zhang, Amir Saeidi, Shrinidhi Kumbhar, Md Nayem Uddin, Aswin RRV, Chitta Baral</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2024-02-16T11:37:05+00:00 · Latest: 2025-10-14T06:25:21+00:00</div>
<div class="meta-line">Comments: Published in Reliable ML from Unreliable Data workshop @ NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2402.10601v5">Abs</a> · <a href="http://arxiv.org/pdf/2402.10601v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Language Model (LLM) safety have primarily
focused on mitigating attacks crafted in natural language or common ciphers
(e.g. Base64), which are likely integrated into newer models&#x27; safety training.
However, we reveal a paradoxical vulnerability: as LLMs advance in reasoning,
they inadvertently become more susceptible to novel jailbreaking attacks.
Enhanced reasoning enables LLMs to interpret complex instructions and decode
complex user-defined ciphers, creating an exploitable security gap. To study
this vulnerability, we introduce Attacks using Custom Encryptions (ACE), a
jailbreaking technique that encodes malicious queries with novel ciphers.
Extending ACE, we introduce Layered Attacks using Custom Encryptions (LACE),
which applies multi-layer ciphers to amplify attack complexity. Furthermore, we
develop CipherBench, a benchmark designed to evaluate LLMs&#x27; accuracy in
decoding encrypted benign text. Our experiments reveal a critical trade-off:
LLMs that are more capable of decoding ciphers are more vulnerable to LACE,
with success rates on gpt-oss-20b escalating from 60% under ACE to 72% with
LACE. These findings highlight a critical insight: as LLMs become more adept at
deciphering complex user ciphers--many of which cannot be preemptively included
in safety training--they become increasingly exploitable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当推理中的“能力”打开脆弱性之门：通过新型复杂密码破解大型语言模型</div>
<div class="mono" style="margin-top:8px">最近在大型语言模型（LLM）安全性方面的进展主要集中在减轻使用自然语言或常见密码（如Base64）构建的攻击，这些攻击可能已被纳入新模型的安全训练中。然而，我们揭示了一种矛盾的脆弱性：随着LLM在推理方面的进步，它们无意中变得更容易受到新型破解攻击的影响。增强的推理能力使LLM能够解释复杂指令并解码复杂的用户定义密码，从而产生可利用的安全漏洞。为了研究这种脆弱性，我们引入了使用自定义加密的攻击（ACE），这是一种通过新型密码编码恶意查询的破解技术。扩展ACE，我们引入了使用自定义加密的分层攻击（LACE），它应用多层密码以增强攻击复杂性。此外，我们开发了CipherBench，这是一个旨在评估LLM解码加密良性文本准确性的基准测试。我们的实验揭示了一个关键的权衡：能够更好地解码密码的LLM在LACE下更脆弱，gpt-oss-20b的成功率从ACE下的60%上升到LACE下的72%。这些发现突显了一个关键的见解：随着LLM在解码复杂用户密码方面变得更加熟练——其中许多无法在安全训练中预先包含——它们变得越来越容易被利用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the emerging vulnerabilities in Large Language Models (LLMs) that arise from their enhanced reasoning capabilities, which have been primarily focused on defending against attacks using natural language or common ciphers. Previous methods have not adequately accounted for the risks posed by novel jailbreaking techniques, leading to a paradox where improved reasoning makes LLMs more susceptible to exploitation. The proposed approach introduces Attacks using Custom Encryptions (ACE) and Layered Attacks using Custom Encryptions (LACE), which utilize complex ciphers to enhance attack sophistication. The contribution of this research lies in the development of CipherBench, a benchmark for assessing LLMs&#x27; ability to decode encrypted benign text. Experimental results demonstrate that LLMs with better cipher-decoding capabilities exhibit increased vulnerability to LACE, with success rates rising from 60% to 72% on the gpt-oss-20b model, underscoring the critical trade-off between reasoning proficiency and security risks.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在推理能力日益增强的情况下出现的新兴脆弱性，这种增强无意中增加了它们对新型越狱攻击的易受攻击性。以往的方法主要集中在减轻自然语言或常见密码所带来的威胁，但这些方法未能考虑用户自定义密码所引入的复杂性。提出的方法“使用自定义加密的攻击”（ACE）及其扩展“使用自定义加密的分层攻击”（LACE）利用多层密码增强攻击复杂性，并利用LLMs的推理能力。该研究的贡献包括引入CipherBench，一个用于评估LLMs在解码加密良性文本方面表现的基准。实验结果表明存在显著的权衡，LACE在gpt-oss-20b上的成功率达到72%，这表明随着LLMs提高其解码能力，它们对这些复杂攻击的脆弱性也在增加。</div>
</details>
</div>
<div class="card">
<div class="title">Attention-Aware GNN-based Input Defense against Multi-Turn LLM Jailbreak</div>
<div class="meta-line">Authors: Zixuan Huang, Kecheng Huang, Lihao Yin, Bowei He, Huiling Zhen, Mingxuan Yuan, Zili Shao</div>
<div class="meta-line">First: 2025-07-09T07:55:03+00:00 · Latest: 2025-10-14T04:28:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.07146v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.07146v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have gained significant traction in various
applications, yet their capabilities present risks for both constructive and
malicious exploitation. Despite extensive training and fine-tuning efforts
aimed at enhancing safety, LLMs remain susceptible to jailbreak attacks.
Recently, the emergence of multi-turn attacks has intensified this
vulnerability. Unlike single-turn attacks, multi-turn attacks incrementally
escalate dialogue complexity, rendering them more challenging to detect and
mitigate.
  In this study, we introduce G-Guard, an innovative attention-aware Graph
Neural Network (GNN)-based input classifier specifically designed to defend
against multi-turn jailbreak attacks targeting LLMs. G-Guard constructs an
entity graph for multi-turn queries, which captures the interrelationships
between queries and harmful keywords that present in multi-turn queries.
Furthermore, we propose an attention-aware augmentation mechanism that
retrieves the most relevant single-turn query based on the ongoing multi-turn
conversation. The retrieved query is incorporated as a labeled node within the
graph, thereby enhancing the GNN&#x27;s capacity to classify the current query as
harmful or benign. Evaluation results show that G-Guard consistently
outperforms all baselines across diverse datasets and evaluation metrics,
demonstrating its efficacy as a robust defense mechanism against multi-turn
jailbreak attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于注意力的GNN输入防御多轮LLM越狱攻击</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在各种应用中获得了显著关注，但其能力也带来了建设性和恶意利用的风险。尽管进行了广泛的训练和微调以增强安全性，LLMs仍然容易受到越狱攻击。最近，多轮攻击的出现加剧了这一脆弱性。与单轮攻击不同，多轮攻击逐步增加对话复杂性，使其更难以检测和缓解。在本研究中，我们介绍了G-Guard，一种创新的基于注意力的图神经网络（GNN）输入分类器，专门设计用于防御针对LLMs的多轮越狱攻击。G-Guard为多轮查询构建了一个实体图，捕捉查询与多轮查询中存在的有害关键词之间的相互关系。此外，我们提出了一种基于注意力的增强机制，根据正在进行的多轮对话检索最相关的单轮查询。检索到的查询作为标记节点纳入图中，从而增强GNN将当前查询分类为有害或良性的能力。评估结果表明，G-Guard在不同数据集和评估指标上始终优于所有基线，证明其作为多轮越狱攻击的强大防御机制的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concern of jailbreak attacks on Large Language Models (LLMs), particularly the more complex multi-turn attacks that are harder to detect and mitigate compared to single-turn attacks. Previous methods have struggled to effectively counter these multi-turn attacks, leading to the development of G-Guard, an attention-aware Graph Neural Network (GNN)-based input classifier that constructs an entity graph to capture the relationships between queries and harmful keywords. This approach is well-motivated as it enhances the classification process by incorporating relevant single-turn queries into the graph, thereby improving the model&#x27;s ability to identify harmful inputs. The contribution of this paper lies in the introduction of G-Guard, which has been evaluated across various datasets and metrics, consistently outperforming existing methods and demonstrating its effectiveness in defending against multi-turn jailbreak attacks.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在面临越狱攻击时的脆弱性，特别是多轮攻击所带来的日益严重的威胁，这种攻击使得检测和缓解变得更加复杂。以往的方法在应对这些攻击时效果不佳，主要是因为它们无法有效处理对话的逐步复杂性。所提出的方法G-Guard利用注意力感知的图神经网络（GNN）通过构建实体图来分类输入，该图捕捉查询与有害关键词之间的关系，并结合注意力感知的增强机制以提高分类准确性。该方法的提出是针对现有防御措施的不足，具有良好的动机。论文贡献了一种新颖的防御机制，显著提高了对多轮越狱攻击的性能，评估结果显示G-Guard在各种数据集和指标上均优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses   Against Llm Jailbreaks and Prompt Injections</div>
<div class="meta-line">Authors: Milad Nasr, Nicholas Carlini, Chawin Sitawarin, Sander V. Schulhoff, Jamie Hayes, Michael Ilie, Juliette Pluto, Shuang Song, Harsh Chaudhari, Ilia Shumailov, Abhradeep Thakurta, Kai Yuanqing Xiao, Andreas Terzis, Florian Tramèr</div>
<div class="meta-line">First: 2025-10-10T05:51:04+00:00 · Latest: 2025-10-10T05:51:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09023v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How should we evaluate the robustness of language model defenses? Current
defenses against jailbreaks and prompt injections (which aim to prevent an
attacker from eliciting harmful knowledge or remotely triggering malicious
actions, respectively) are typically evaluated either against a static set of
harmful attack strings, or against computationally weak optimization methods
that were not designed with the defense in mind. We argue that this evaluation
process is flawed.
  Instead, we should evaluate defenses against adaptive attackers who
explicitly modify their attack strategy to counter a defense&#x27;s design while
spending considerable resources to optimize their objective. By systematically
tuning and scaling general optimization techniques-gradient descent,
reinforcement learning, random search, and human-guided exploration-we bypass
12 recent defenses (based on a diverse set of techniques) with attack success
rate above 90% for most; importantly, the majority of defenses originally
reported near-zero attack success rates. We believe that future defense work
must consider stronger attacks, such as the ones we describe, in order to make
reliable and convincing claims of robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>攻击者第二步行动：更强的自适应攻击绕过针对 LLM 监狱破解和提示注入的防御</div>
<div class="mono" style="margin-top:8px">我们应该如何评估语言模型防御的稳健性？当前针对监狱破解和提示注入的防御（旨在防止攻击者引发有害知识或远程触发恶意行为）通常是针对一组静态的有害攻击字符串，或针对未考虑防御设计的计算弱优化方法进行评估。我们认为这一评估过程存在缺陷。相反，我们应该针对自适应攻击者进行评估，他们明确修改攻击策略以对抗防御设计，同时花费大量资源来优化目标。通过系统地调整和扩展一般优化技术——梯度下降、强化学习、随机搜索和人类引导探索——我们绕过了12个最近的防御（基于多种技术），大多数攻击成功率超过90%；重要的是，大多数防御最初报告的攻击成功率接近零。我们认为，未来的防御工作必须考虑更强的攻击，例如我们所描述的，以便做出可靠和令人信服的稳健性声明。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inadequacies in evaluating the robustness of language model defenses against jailbreaks and prompt injections, which are currently assessed using static attack strings or weak optimization methods. These traditional methods fail to account for adaptive attackers who can modify their strategies to exploit defense weaknesses. The proposed approach emphasizes evaluating defenses against such adaptive attackers by employing advanced optimization techniques, including gradient descent and reinforcement learning, which significantly enhance attack success rates. The paper contributes to the field by demonstrating that 12 recent defenses can be bypassed with over 90% success, challenging the previously reported near-zero success rates of these defenses. This methodology reveals the necessity for future defense strategies to consider more sophisticated attack methods to ensure their reliability and robustness.</div>
<div class="mono" style="margin-top:8px">本文探讨了评估语言模型防御措施在抵御越狱和提示注入方面的有效性时存在的不足，这对于防止有害知识的引出和恶意行为的触发至关重要。以往的方法依赖于静态攻击字符串或弱优化技术，这些方法未能考虑防御设计，导致评估结果具有误导性。提出的方法主张应对抗主动修改策略并优化目标的适应性攻击者进行防御评估，从而提供更现实的评估框架。作者通过系统调优多种优化技术，包括梯度下降和强化学习，成功绕过12种最近的防御措施，攻击成功率超过90%，而这些防御措施之前报告的成功率接近于零。该研究通过强调在防御研究中需要更强的攻击评估，确保对鲁棒性声明的可信度，为该领域做出了贡献。</div>
</details>
</div>
<div class="card">
<div class="title">Machine Learning for Detection and Analysis of Novel LLM Jailbreaks</div>
<div class="meta-line">Authors: John Hawkins, Aditya Pramar, Rodney Beard, Rohitash Chandra</div>
<div class="meta-line">First: 2025-10-02T03:55:29+00:00 · Latest: 2025-10-10T05:08:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.01644v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.01644v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) suffer from a range of vulnerabilities that
allow malicious users to solicit undesirable responses through manipulation of
the input text. These so-called jailbreak prompts are designed to trick the LLM
into circumventing the safety guardrails put in place to keep responses
acceptable to the developer&#x27;s policies. In this study, we analyse the ability
of different machine learning models to distinguish jailbreak prompts from
genuine uses, including looking at our ability to identify jailbreaks that use
previously unseen strategies. Our results indicate that using current datasets
the best performance is achieved by fine tuning a Bidirectional Encoder
Representations from Transformers (BERT) model end-to-end for identifying
jailbreaks. We visualise the keywords that distinguish jailbreak from genuine
prompts and conclude that explicit reflexivity in prompt structure could be a
signal of jailbreak intention.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于检测和分析新型 LLM 越狱的机器学习</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）存在一系列漏洞，允许恶意用户通过操控输入文本来获取不良响应。这些所谓的越狱提示旨在欺骗 LLM 绕过为保持响应符合开发者政策而设立的安全防护措施。在本研究中，我们分析了不同机器学习模型区分越狱提示与真实使用的能力，包括识别使用以前未见策略的越狱。我们的结果表明，使用当前数据集，通过端到端微调双向编码器表示（BERT）模型来识别越狱的最佳性能得以实现。我们可视化了区分越狱与真实提示的关键词，并得出结论，提示结构中的显式反身性可能是越狱意图的信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of Large Language Models (LLMs) that can be exploited by malicious users through specially crafted inputs known as jailbreak prompts, which bypass safety measures. Previous methods struggled to effectively identify these prompts, particularly those employing novel strategies, highlighting a gap in the detection capabilities of existing models. This study proposes a fine-tuned Bidirectional Encoder Representations from Transformers (BERT) model, which significantly improves the identification of jailbreaks by leveraging explicit reflexivity in prompt structure as a distinguishing feature. The methodology involves training the BERT model end-to-end on current datasets, achieving superior performance in detecting both known and novel jailbreak strategies, thus supporting the goal of enhancing LLM security against manipulation.</div>
<div class="mono" style="margin-top:8px">本文研究了大型语言模型（LLM）的脆弱性，这些脆弱性可以被恶意用户通过特制的输入文本（称为越狱提示）利用。以往的检测方法往往无法识别攻击者使用的新策略，因此效果不佳。提出的方法通过对双向编码器表示的变换器（BERT）模型进行微调，专门用于识别越狱提示，从而改善了现有方法，这一做法在攻击手段日益复杂的背景下具有充分的动机。研究的贡献在于展示了这一微调模型在区分越狱提示和真实提示方面的有效性，在当前数据集上取得了优越的性能。研究结果表明，某些关键词和提示结构中的显性反身性可以作为越狱意图的指示，支持了增强LLM安全机制的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through   Formal Logical Expression</div>
<div class="meta-line">Authors: Jingyu Peng, Maolin Wang, Nan Wang, Jiatong Li, Yuchen Li, Yuyang Ye, Wanyu Wang, Pengyue Jia, Kai Zhang, Xiangyu Zhao</div>
<div class="meta-line">First: 2025-05-18T04:23:51+00:00 · Latest: 2025-10-09T06:29:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.13527v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.13527v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite substantial advancements in aligning large language models (LLMs)
with human values, current safety mechanisms remain susceptible to jailbreak
attacks. We hypothesize that this vulnerability stems from distributional
discrepancies between alignment-oriented prompts and malicious prompts. To
investigate this, we introduce LogiBreak, a novel and universal black-box
jailbreak method that leverages logical expression translation to circumvent
LLM safety systems. By converting harmful natural language prompts into formal
logical expressions, LogiBreak exploits the distributional gap between
alignment data and logic-based inputs, preserving the underlying semantic
intent and readability while evading safety constraints. We evaluate LogiBreak
on a multilingual jailbreak dataset spanning three languages, demonstrating its
effectiveness across various evaluation settings and linguistic contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>逻辑越狱：通过形式逻辑表达高效解锁大型语言模型安全限制</div>
<div class="mono" style="margin-top:8px">尽管在将大型语言模型（LLMs）与人类价值观对齐方面取得了重大进展，但当前的安全机制仍然容易受到越狱攻击。我们假设这种脆弱性源于对齐导向提示与恶意提示之间的分布差异。为此，我们引入了LogiBreak，这是一种新颖且通用的黑箱越狱方法，利用逻辑表达翻译来规避LLM安全系统。通过将有害的自然语言提示转换为形式逻辑表达，LogiBreak利用对齐数据与基于逻辑的输入之间的分布差距，保留潜在的语义意图和可读性，同时规避安全约束。我们在涵盖三种语言的多语言越狱数据集上评估LogiBreak，展示了其在各种评估设置和语言环境中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the ongoing challenge of ensuring the safety of large language models (LLMs), which remain vulnerable to jailbreak attacks despite advancements in aligning them with human values. Previous methods have struggled with the distributional discrepancies between alignment-oriented prompts and malicious prompts, leading to ineffective safety mechanisms. The proposed approach, LogiBreak, introduces a novel black-box jailbreak method that translates harmful natural language prompts into formal logical expressions, effectively bridging the distributional gap while maintaining semantic intent and readability. The contribution of this paper lies in demonstrating the effectiveness of LogiBreak across a multilingual jailbreak dataset, achieving significant performance in various evaluation settings and linguistic contexts, thereby supporting the goal of enhancing LLM safety mechanisms.</div>
<div class="mono" style="margin-top:8px">本文探讨了确保大型语言模型（LLMs）安全性以防止越狱攻击的持续挑战，这些攻击利用了当前安全机制的脆弱性。以往的方法主要集中在将LLMs与人类价值观对齐，但未能充分解决对齐导向提示与恶意提示之间的分布差异。提出的方法LogiBreak引入了一种新颖的黑箱越狱方法，通过将有害的自然语言提示转换为形式逻辑表达，有效规避安全系统，同时保持语义意图。本文的贡献在于展示LogiBreak在涵盖三种语言的多语言越狱数据集上的有效性，在各种评估设置中取得显著性能提升，从而支持增强LLM安全性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">$\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM   Systems with Optimized Prompt Attacks</div>
<div class="meta-line">Authors: Rana Muhammad Shahroz Khan, Zhen Tan, Sukwon Yun, Charles Fleming, Tianlong Chen</div>
<div class="meta-line">First: 2025-03-31T20:43:56+00:00 · Latest: 2025-10-08T22:17:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.00218v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.00218v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most discussions about Large Language Model (LLM) safety have focused on
single-agent settings but multi-agent LLM systems now create novel adversarial
risks because their behavior depends on communication between agents and
decentralized reasoning. In this work, we innovatively focus on attacking
pragmatic systems that have constrains such as limited token bandwidth, latency
between message delivery, and defense mechanisms. We design a
$\textit{permutation-invariant adversarial attack}$ that optimizes prompt
distribution across latency and bandwidth-constraint network topologies to
bypass distributed safety mechanisms within the system. Formulating the attack
path as a problem of $\textit{maximum-flow minimum-cost}$, coupled with the
novel $\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage
graph-based optimization to maximize attack success rate while minimizing
detection risk. Evaluating across models including $\texttt{Llama}$,
$\texttt{Mistral}$, $\texttt{Gemma}$, $\texttt{DeepSeek}$ and other variants on
various datasets like $\texttt{JailBreakBench}$ and
$\texttt{AdversarialBench}$, our method outperforms conventional attacks by up
to $7\times$, exposing critical vulnerabilities in multi-agent systems.
Moreover, we demonstrate that existing defenses, including variants of
$\texttt{Llama-Guard}$ and $\texttt{PromptGuard}$, fail to prohibit our attack,
emphasizing the urgent need for multi-agent specific safety mechanisms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>《被围攻的代理人》：通过优化提示攻击打破务实的多代理LLM系统</div>
<div class="mono" style="margin-top:8px">关于大型语言模型（LLM）安全性的讨论大多集中在单代理设置上，但多代理LLM系统现在带来了新的对抗风险，因为它们的行为依赖于代理之间的通信和分散推理。在这项工作中，我们创新性地关注于攻击具有限制的务实系统，例如有限的令牌带宽、消息传递延迟和防御机制。我们设计了一种“置换不变对抗攻击”，优化延迟和带宽受限网络拓扑中的提示分布，以绕过系统内的分布式安全机制。将攻击路径公式化为“最大流最小成本”问题，结合新颖的“置换不变规避损失（PIEL）”，我们利用基于图的优化来最大化攻击成功率，同时最小化检测风险。在包括“Llama”、“Mistral”、“Gemma”、“DeepSeek”等模型以及“JailBreakBench”和“AdversarialBench”等各种数据集上的评估中，我们的方法在攻击效果上超越传统攻击，提升了多达7倍，暴露了多代理系统中的关键漏洞。此外，我们证明现有的防御措施，包括“Llama-Guard”和“PromptGuard”的变体，无法阻止我们的攻击，强调了对多代理特定安全机制的迫切需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the emerging adversarial risks associated with multi-agent Large Language Model (LLM) systems, which have not been adequately explored compared to single-agent settings. Previous methods primarily focused on single-agent attacks and often failed to consider the complexities introduced by agent communication and decentralized reasoning. The proposed approach introduces a permutation-invariant adversarial attack that optimizes prompt distribution within bandwidth and latency constraints, effectively bypassing existing safety mechanisms. This method is well-motivated as it targets the unique vulnerabilities of multi-agent systems. The contribution of the paper lies in formulating the attack as a maximum-flow minimum-cost problem and utilizing the novel Permutation-Invariant Evasion Loss (PIEL) to enhance attack success rates while minimizing detection risks. The evaluation across various models and datasets demonstrates that the proposed method outperforms traditional attacks by up to seven times, highlighting significant weaknesses in current multi-agent safety defenses.</div>
<div class="mono" style="margin-top:8px">本文探讨了与多智能体大型语言模型（LLM）系统相关的新兴对抗风险，这些风险在以往主要关注单智能体设置的研究中并未得到充分探讨。过去的方法在多智能体环境中面临通信和去中心化推理带来的独特挑战，导致了漏洞，而提出的置换不变对抗攻击旨在利用这些漏洞。作者提出了一种新方法，将攻击形式化为最大流最小成本问题，利用新的置换不变规避损失（PIEL）来增强攻击效果，同时最小化检测风险。该方法在各种模型和数据集上实现了攻击成功率的显著提升，性能比传统攻击提高了多达七倍，从而揭示了现有多智能体安全防御的关键弱点，并强调了针对特定多智能体的安全机制的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Psycho-Lexical Approach for Constructing Value Systems in   Large Language Models</div>
<div class="meta-line">Authors: Haoran Ye, Tianze Zhang, Yuhang Xie, Liyuan Zhang, Yuanyi Ren, Xin Zhang, Guojie Song</div>
<div class="meta-line">Venue: ACL 2025</div>
<div class="meta-line">First: 2025-02-04T16:10:55+00:00 · Latest: 2025-10-07T14:57:19+00:00</div>
<div class="meta-line">Comments: ACL 2025 Main</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.02444v6">Abs</a> · <a href="http://arxiv.org/pdf/2502.02444v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Values are core drivers of individual and collective perception, cognition,
and behavior. Value systems, such as Schwartz&#x27;s Theory of Basic Human Values,
delineate the hierarchy and interplay among these values, enabling
cross-disciplinary investigations into decision-making and societal dynamics.
Recently, the rise of Large Language Models (LLMs) has raised concerns
regarding their elusive intrinsic values. Despite growing efforts in
evaluating, understanding, and aligning LLM values, a psychologically grounded
LLM value system remains underexplored. This study addresses the gap by
introducing the Generative Psycho-Lexical Approach (GPLA), a scalable,
adaptable, and theoretically informed method for constructing value systems.
Leveraging GPLA, we propose a psychologically grounded five-factor value system
tailored for LLMs. For systematic validation, we present three benchmarking
tasks that integrate psychological principles with cutting-edge AI priorities.
Our results reveal that the proposed value system meets standard psychological
criteria, better captures LLM values, improves LLM safety prediction, and
enhances LLM alignment, when compared to the canonical Schwartz&#x27;s values.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成性心理词汇方法构建大型语言模型的价值体系</div>
<div class="mono" style="margin-top:8px">价值观是个体和集体感知、认知和行为的核心驱动力。价值体系，如施瓦茨的基本人类价值理论，描绘了这些价值观之间的层次和相互作用，使跨学科研究决策和社会动态成为可能。最近，大型语言模型（LLMs）的兴起引发了对其难以捉摸的内在价值的担忧。尽管在评估、理解和对齐LLM价值方面的努力不断增加，但基于心理学的LLM价值体系仍然未得到充分探索。本研究通过引入生成性心理词汇方法（GPLA）来填补这一空白，这是一种可扩展、可适应且理论上有依据的构建价值体系的方法。利用GPLA，我们提出了一个为LLM量身定制的基于心理学的五因素价值体系。为了进行系统验证，我们提出了三个基准任务，将心理学原理与前沿AI优先事项相结合。我们的结果表明，所提出的价值体系符合标准心理学标准，更好地捕捉LLM价值，提高LLM安全预测，并增强LLM对齐，相较于经典的施瓦茨价值观。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the need for a psychologically informed value system in Large Language Models (LLMs), as existing methods have not adequately captured the intrinsic values of these models. Previous approaches, primarily based on Schwartz&#x27;s Theory of Basic Human Values, lack the adaptability and scalability required for LLMs, leading to insufficient alignment and safety predictions. The proposed Generative Psycho-Lexical Approach (GPLA) offers a novel framework that constructs a five-factor value system specifically designed for LLMs, integrating psychological principles with AI priorities. The methodology includes systematic validation through three benchmarking tasks, demonstrating that the GPLA-based value system not only meets established psychological criteria but also significantly enhances the alignment and safety prediction of LLMs compared to traditional methods.</div>
<div class="mono" style="margin-top:8px">本文探讨了在大型语言模型（LLMs）中建立心理学基础的价值体系的必要性，因为现有方法未能充分捕捉这些模型的内在价值。以施瓦茨的基本人类价值理论为代表的过去方法缺乏适应性和可扩展性，导致对齐和安全预测不足。提出的生成心理词汇方法（GPLA）提供了一种新颖的框架，构建了专门为LLMs设计的五因素价值体系，将心理学原则与人工智能优先事项相结合。该方法通过三个基准任务进行系统验证，结果表明，基于GPLA的价值体系不仅符合既定的心理学标准，而且显著改善了LLMs的对齐性和安全预测，相较于传统方法表现更佳。</div>
</details>
</div>
<div class="card">
<div class="title">Sampling-aware Adversarial Attacks Against Large Language Models</div>
<div class="meta-line">Authors: Tim Beyer, Yan Scholten, Leo Schwinn, Stephan Günnemann</div>
<div class="meta-line">First: 2025-07-06T16:13:33+00:00 · Latest: 2025-10-06T09:02:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.04446v3">Abs</a> · <a href="http://arxiv.org/pdf/2507.04446v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To guarantee safe and robust deployment of large language models (LLMs) at
scale, it is critical to accurately assess their adversarial robustness.
Existing adversarial attacks typically target harmful responses in single-point
greedy generations, overlooking the inherently stochastic nature of LLMs and
overestimating robustness. We show that for the goal of eliciting harmful
responses, repeated sampling of model outputs during the attack complements
prompt optimization and serves as a strong and efficient attack vector. By
casting attacks as a resource allocation problem between optimization and
sampling, we determine compute-optimal trade-offs and show that integrating
sampling into existing attacks boosts success rates by up to 37\% and improves
efficiency by up to two orders of magnitude. We further analyze how
distributions of output harmfulness evolve during an adversarial attack,
discovering that many common optimization strategies have little effect on
output harmfulness. Finally, we introduce a label-free proof-of-concept
objective based on entropy maximization, demonstrating how our sampling-aware
perspective enables new optimization targets. Overall, our findings establish
the importance of sampling in attacks to accurately assess and strengthen LLM
safety at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对大型语言模型的采样感知对抗攻击</div>
<div class="mono" style="margin-top:8px">为了确保大型语言模型（LLMs）在规模上的安全和稳健部署，准确评估其对抗鲁棒性至关重要。现有的对抗攻击通常针对单点贪婪生成中的有害响应，忽视了LLMs固有的随机性，并高估了鲁棒性。我们表明，在引发有害响应的目标下，攻击期间对模型输出的重复采样补充了提示优化，并作为一种强大而高效的攻击向量。通过将攻击视为优化与采样之间的资源分配问题，我们确定了计算最优的权衡，并显示将采样整合到现有攻击中可以将成功率提高多达37\%，并将效率提高两个数量级。我们进一步分析了对抗攻击期间输出有害性的分布如何演变，发现许多常见的优化策略对输出有害性几乎没有影响。最后，我们引入了一种基于熵最大化的无标签概念验证目标，展示了我们的采样感知视角如何启用新的优化目标。总体而言，我们的研究结果确立了在攻击中采样的重要性，以准确评估和增强LLM在规模上的安全性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the critical need for accurately assessing the adversarial robustness of large language models (LLMs) to ensure their safe deployment. Previous methods primarily focused on single-point greedy generations, which failed to account for the stochastic nature of LLMs, leading to an overestimation of their robustness. The proposed approach integrates repeated sampling of model outputs with prompt optimization, framing the attacks as a resource allocation problem that optimizes compute trade-offs. This method significantly enhances the success rates of adversarial attacks by up to 37% and improves efficiency by two orders of magnitude. The research contributes to understanding how harmfulness distributions change during attacks and introduces a label-free objective based on entropy maximization, thereby establishing the importance of sampling in evaluating and enhancing LLM safety.</div>
<div class="mono" style="margin-top:8px">本研究解决了准确评估大型语言模型（LLMs）对抗鲁棒性的重要性，以确保其安全部署。以往的方法主要集中在单点贪婪生成上，未能考虑LLMs的随机性，导致对其鲁棒性的高估。所提出的方法将模型输出的重复采样与提示优化相结合，将攻击框架设定为资源分配问题，以优化计算权衡。该方法显著提高了攻击成功率，最高可达37%，并将效率提高了两个数量级。本文的贡献在于证明了采样在对抗攻击中的必要性，并引入了一种基于熵最大化的无标签概念验证目标，最终为评估和增强LLM安全性提供了新的优化目标。</div>
</details>
</div>
<div class="card">
<div class="title">Chasing Moving Targets with Online Self-Play Reinforcement Learning for   Safer Language Models</div>
<div class="meta-line">Authors: Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, Natasha Jaques</div>
<div class="meta-line">First: 2025-06-09T06:35:12+00:00 · Latest: 2025-10-06T03:42:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.07468v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.07468v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过在线自我对弈强化学习追踪动态目标以提高语言模型的安全性</div>
<div class="mono" style="margin-top:8px">传统的语言模型（LM）安全对齐依赖于反应性、分离的程序：攻击者利用静态模型，随后进行防御性微调以修补暴露的漏洞。这种顺序方法造成了不匹配——攻击者过度拟合过时的防御，而防御者则始终滞后于新出现的威胁。为了解决这个问题，我们提出了Self-RedTeam，一种在线自我对弈强化学习算法，其中攻击者和防御者代理通过持续互动共同进化。我们将安全对齐视为一个双人零和游戏，其中单个模型在攻击者和防御者角色之间交替——生成对抗性提示并对其进行保护——同时奖励语言模型裁定结果。这使得动态共同适应成为可能。基于零和游戏的博弈论框架，我们建立了理论安全保证，这激励了我们方法的设计：如果自我对弈收敛到纳什均衡，防御者将可靠地产生对任何对抗性输入的安全响应。在实证上，Self-RedTeam发现了比针对静态防御者训练的攻击者更具多样性的攻击（+21.8% SBERT），并在安全基准上实现了更高的鲁棒性（例如，在WildJailBreak上提高65.5%），相比之下，针对静态攻击者训练的防御者表现较差。我们进一步提出了隐性思维链，允许代理私下规划，从而提高对抗性多样性并减少过度拒绝。我们的结果激励了从反应性修补转向主动共同进化的LM安全训练，使得通过多代理强化学习（MARL）实现可扩展、自主和鲁棒的自我改进成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the limitations of conventional language model safety alignment, which typically follows a reactive and disjointed process that leaves models vulnerable to evolving attacks. Previous methods have struggled with a static defense approach, leading to a mismatch where attackers exploit outdated models. The proposed Self-RedTeam method introduces an online self-play reinforcement learning framework, allowing an attacker and defender to co-evolve through continuous interaction, framed as a two-player zero-sum game. This approach not only establishes a theoretical safety guarantee through the concept of Nash Equilibrium but also demonstrates empirical improvements, uncovering 21.8% more diverse attacks and achieving 65.5% higher robustness on safety benchmarks compared to traditional methods. The findings suggest a significant shift towards proactive co-evolution in language model safety training, enhancing the models&#x27; ability to autonomously improve through multi-agent reinforcement learning.</div>
<div class="mono" style="margin-top:8px">本研究解决了传统语言模型安全对齐的局限性，该方法依赖于反应性和分离的方式，导致防御者在攻击者利用静态模型时滞后。以往的方法往往导致攻击者过度拟合过时的防御，迫切需要新的策略。提出的Self-RedTeam方法引入了一种在线自我对抗强化学习框架，其中攻击者和防御者通过持续互动共同进化，构建为一个双人零和游戏。这种方法不仅基于纳什均衡建立了理论安全保证，还在实证上显示出改进，发现了21.8%更多的多样化攻击，并在安全基准测试中实现了65.5%的更高鲁棒性，优于传统方法。研究结果支持在语言模型安全训练中从反应性修补转向主动进化，促进通过多智能体强化学习实现可扩展和自主的自我改进。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251104_1539.html">20251104_1539</a>
<a href="archive/20251104_0324.html">20251104_0324</a>
<a href="archive/20251103_0315.html">20251103_0315</a>
<a href="archive/20251102_1356.html">20251102_1356</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
