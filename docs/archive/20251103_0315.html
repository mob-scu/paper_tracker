<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-03 03:15</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251103_0315</div>
    <div class="row"><div class="card">
<div class="title">ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for   Audio-Language Models</div>
<div class="meta-line">Authors: Weifei Jin, Yuxin Cao, Junjie Su, Minhui Xue, Jie Hao, Ke Xu, Jin Song Dong, Derui Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-30T03:19:59+00:00 · Latest: 2025-10-30T03:19:59+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.26096v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.26096v1">PDF</a> · <a href="https://github.com/WeifeiJin/ALMGuard">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Audio-Language Models (ALMs) have significantly improved
multimodal understanding capabilities. However, the introduction of the audio
modality also brings new and unique vulnerability vectors. Previous studies
have proposed jailbreak attacks that specifically target ALMs, revealing that
defenses directly transferred from traditional audio adversarial attacks or
text-based Large Language Model (LLM) jailbreaks are largely ineffective
against these ALM-specific threats. To address this issue, we propose ALMGuard,
the first defense framework tailored to ALMs. Based on the assumption that
safety-aligned shortcuts naturally exist in ALMs, we design a method to
identify universal Shortcut Activation Perturbations (SAPs) that serve as
triggers that activate the safety shortcuts to safeguard ALMs at inference
time. To better sift out effective triggers while preserving the model&#x27;s
utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM),
which restricts perturbations to Mel-frequency bins that are sensitive to
jailbreaks but insensitive to speech understanding. Both theoretical analyses
and empirical results demonstrate the robustness of our method against both
seen and unseen attacks. Overall, \MethodName reduces the average success rate
of advanced ALM-specific jailbreak attacks to 4.6% across four models, while
maintaining comparable utility on benign benchmarks, establishing it as the new
state of the art. Our code and data are available at
https://github.com/WeifeiJin/ALMGuard.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ALMGuard：音频语言模型的安全捷径及其发现方式</div>
<div class="mono" style="margin-top:8px">音频语言模型（ALMs）的最新进展显著提高了多模态理解能力。然而，音频模态的引入也带来了新的独特脆弱性向量。之前的研究提出了专门针对ALMs的越狱攻击，揭示了直接从传统音频对抗攻击或基于文本的大型语言模型（LLM）越狱转移的防御在这些ALM特定威胁面前大多无效。为了解决这个问题，我们提出了ALMGuard，这是第一个针对ALMs量身定制的防御框架。基于安全对齐捷径自然存在于ALMs的假设，我们设计了一种方法来识别通用捷径激活扰动（SAPs），作为触发器以激活安全捷径，在推理时保护ALMs。为了更好地筛选有效触发器，同时保持模型在良性任务上的效用，我们进一步提出了梅尔梯度稀疏掩码（M-GSM），它将扰动限制在对越狱敏感但对语音理解不敏感的梅尔频率区间。理论分析和实证结果均表明我们的方法对已知和未知攻击的鲁棒性。总体而言，\MethodName将四个模型中先进的ALM特定越狱攻击的平均成功率降低到4.6%，同时在良性基准上保持了可比的效用，确立了其作为新的最先进技术。我们的代码和数据可在https://github.com/WeifeiJin/ALMGuard获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities introduced by Audio-Language Models (ALMs) which, despite their advancements in multimodal understanding, are susceptible to unique jailbreak attacks that traditional defenses fail to mitigate. Previous methods relied on adaptations from audio adversarial attacks or text-based Large Language Model defenses, which proved ineffective against ALM-specific threats. The proposed approach, ALMGuard, is motivated by the existence of safety-aligned shortcuts within ALMs and introduces a novel defense framework that identifies universal Shortcut Activation Perturbations (SAPs) to activate these safety mechanisms during inference. The methodology includes the Mel-Gradient Sparse Mask (M-GSM) to ensure that perturbations are effective against jailbreaks while preserving model utility for benign tasks. Experimental results show that ALMGuard reduces the success rate of advanced jailbreak attacks to 4.6% across four models while maintaining comparable performance on benign benchmarks, establishing a new state of the art in ALM safety.</div>
<div class="mono" style="margin-top:8px">本研究关注音频语言模型（ALMs）引入的脆弱性，这些模型在多模态理解方面取得了进展，但也容易受到特定威胁，如越狱攻击。以往的方法通过从传统音频对抗攻击或基于文本的大型语言模型（LLM）越狱中转移防御措施，但这些方法在应对这些特定脆弱性时效果不佳。提出的ALMGuard方法基于安全对齐的快捷方式在ALMs中存在的假设，设计了一种新方法来识别通用的快捷激活扰动（SAPs），以在推理时激活这些安全机制。该方法还引入了梅尔梯度稀疏掩码（M-GSM），确保扰动针对敏感的梅尔频率区间，同时保持模型在良性任务上的效用。实验结果表明，ALMGuard将四个模型中先进的ALM特定越狱攻击的平均成功率降低到4.6%，同时在良性基准上保持了可比的性能，从而在该领域确立了新的技术标准。</div>
</details>
</div>
<div class="card">
<div class="title">Improving LLM Safety Alignment with Dual-Objective Optimization</div>
<div class="meta-line">Authors: Xuandong Zhao, Will Cai, Tianneng Shi, David Huang, Licong Lin, Song Mei, Dawn Song</div>
<div class="meta-line">Venue: ICML 2025</div>
<div class="meta-line">First: 2025-03-05T18:01:05+00:00 · Latest: 2025-10-30T01:16:06+00:00</div>
<div class="meta-line">Comments: ICML 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.03710v3">Abs</a> · <a href="http://arxiv.org/pdf/2503.03710v3">PDF</a> · <a href="https://github.com/wicai24/DOOR-Alignment">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing training-time safety alignment techniques for large language models
(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization
(DPO), a widely deployed alignment method, exhibits limitations in both
experimental and theoretical contexts as its loss function proves suboptimal
for refusal learning. Through gradient-based analysis, we identify these
shortcomings and propose an improved safety alignment that disentangles DPO
objectives into two components: (1) robust refusal training, which encourages
refusal even when partial unsafe generations are produced, and (2) targeted
unlearning of harmful knowledge. This approach significantly increases LLM
robustness against a wide range of jailbreak attacks, including prefilling,
suffix, and multi-turn attacks across both in-distribution and
out-of-distribution scenarios. Furthermore, we introduce a method to emphasize
critical refusal tokens by incorporating a reward-based token-level weighting
mechanism for refusal learning, which further improves the robustness against
adversarial exploits. Our research also suggests that robustness to jailbreak
attacks is correlated with token distribution shifts in the training process
and internal representations of refusal and harmful tokens, offering valuable
directions for future research in LLM safety alignment. The code is available
at https://github.com/wicai24/DOOR-Alignment</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过双目标优化提高大型语言模型的安全对齐</div>
<div class="mono" style="margin-top:8px">现有的大型语言模型（LLMs）训练时安全对齐技术仍然容易受到越狱攻击。直接偏好优化（DPO）作为一种广泛应用的对齐方法，在实验和理论背景下均表现出局限性，因为其损失函数在拒绝学习中被证明是次优的。通过基于梯度的分析，我们识别了这些缺陷，并提出了一种改进的安全对齐方法，将DPO目标分解为两个组成部分：（1）强健的拒绝训练，鼓励在产生部分不安全生成时仍然拒绝，以及（2）针对有害知识的有针对性的去学习。这种方法显著提高了LLM对各种越狱攻击的鲁棒性，包括预填充、后缀和多轮攻击，涵盖了分布内和分布外场景。此外，我们引入了一种通过结合基于奖励的令牌级加权机制来强调关键拒绝令牌的方法，以进一步提高对抗性利用的鲁棒性。我们的研究还表明，对越狱攻击的鲁棒性与训练过程中的令牌分布变化以及拒绝和有害令牌的内部表示相关，为未来LLM安全对齐的研究提供了宝贵的方向。代码可在 https://github.com/wicai24/DOOR-Alignment 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of large language models (LLMs) to jailbreak attacks, highlighting the limitations of existing safety alignment techniques, particularly direct preference optimization (DPO), which is suboptimal for refusal learning. The proposed method improves upon DPO by disentangling its objectives into robust refusal training and targeted unlearning of harmful knowledge, effectively addressing the shortcomings of previous methods. This dual-objective optimization approach enhances LLM robustness against various jailbreak attacks, including prefilling and multi-turn attacks, demonstrating significant improvements in both in-distribution and out-of-distribution scenarios. The methodology includes a reward-based token-level weighting mechanism to emphasize critical refusal tokens, further bolstering the model&#x27;s defenses against adversarial exploits, and the findings suggest a correlation between robustness and token distribution shifts, paving the way for future research in LLM safety alignment.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在面对越狱攻击时的脆弱性，强调了现有训练时安全对齐技术的局限性，特别是直接偏好优化（DPO），其在拒绝学习方面表现不佳。所提出的方法通过将DPO的目标分解为稳健的拒绝训练和有针对性的有害知识去除，有效解决了识别出的缺陷。该研究的贡献在于通过一种新颖的基于奖励的令牌级加权机制来增强LLM对各种越狱攻击的鲁棒性。该方法论涉及基于梯度的分析和双目标优化框架的实施，已显示出在抵御分布内和分布外攻击方面的显著性能提升，从而支持了提高LLM安全对齐的目标。</div>
</details>
</div>
<div class="card">
<div class="title">MixAT: Combining Continuous and Discrete Adversarial Training for LLMs</div>
<div class="meta-line">Authors: Csaba Dékány, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, Martin Vechev</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-22T17:32:50+00:00 · Latest: 2025-10-28T09:41:22+00:00</div>
<div class="meta-line">Comments: Published at 39th Conference on Neural Information Processing Systems
  (NeurIPS 2025)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.16947v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.16947v2">PDF</a> · <a href="https://github.com/insait-institute/MixAT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent efforts in Large Language Model (LLM) safety and alignment,
current adversarial attacks on frontier LLMs can still consistently force
harmful generations. Although adversarial training has been widely studied and
shown to significantly improve the robustness of traditional machine learning
models, its strengths and weaknesses in the context of LLMs are less
understood. Specifically, while existing discrete adversarial attacks are
effective at producing harmful content, training LLMs with concrete adversarial
prompts is often computationally expensive, leading to reliance on continuous
relaxations. At the same time, despite their effectiveness and generalization
capabilities, training with continuous perturbations does not always capture
the full spectrum of vulnerabilities exploited by discrete attacks. In this
work, we aim to bridge this gap by introducing MixAT, a novel method that
combines stronger discrete and faster continuous attacks during training. We
rigorously evaluate MixAT across a wide spectrum of state-of-the-art attacks,
proposing the At Least One Attack Success Rate (ALO-ASR) metric to capture the
worst-case vulnerability of models. We show MixAT achieves substantially better
robustness (ALO-ASR &lt; 20%) compared to prior defenses (ALO-ASR &gt; 50%), while
maintaining a runtime comparable to methods based on continuous relaxations. We
further analyze MixAT in realistic deployment settings, exploring how chat
templates, quantization, low-rank adapters, and temperature affect both
adversarial training and evaluation, revealing additional blind spots in
current methodologies. Our results demonstrate that MixAT&#x27;s discrete-continuous
defense offers a principled and superior robustness-accuracy tradeoff with
minimal computational overhead, highlighting its promise for building safer
LLMs. We provide our code and models at
https://github.com/insait-institute/MixAT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MixAT：结合连续和离散对抗训练的LLM</div>
<div class="mono" style="margin-top:8px">尽管近期在大型语言模型（LLM）安全性和对齐方面进行了努力，当前对前沿LLM的对抗攻击仍能持续强制生成有害内容。虽然对抗训练已被广泛研究并显示出显著提高传统机器学习模型的鲁棒性，但在LLM背景下其优缺点尚不清楚。具体而言，现有的离散对抗攻击在生成有害内容方面有效，但使用具体对抗提示训练LLM通常计算成本高，导致依赖于连续松弛。同时，尽管连续扰动的训练有效且具有泛化能力，但并不总能捕捉到离散攻击所利用的全部脆弱性。在本研究中，我们旨在通过引入MixAT这一新方法来填补这一空白，该方法在训练过程中结合了更强的离散攻击和更快的连续攻击。我们对MixAT在广泛的最先进攻击中进行了严格评估，提出了至少一个攻击成功率（ALO-ASR）指标，以捕捉模型的最坏情况脆弱性。我们展示了MixAT在鲁棒性方面显著优于先前的防御（ALO-ASR &lt; 20%），而运行时间与基于连续松弛的方法相当。我们进一步分析了MixAT在现实部署环境中的表现，探讨了聊天模板、量化、低秩适配器和温度如何影响对抗训练和评估，揭示了当前方法中的额外盲点。我们的结果表明，MixAT的离散-连续防御提供了原则性和优越的鲁棒性-准确性权衡，计算开销最小，突显了其构建更安全LLM的潜力。我们在https://github.com/insait-institute/MixAT提供了我们的代码和模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the ongoing challenges in ensuring the safety and alignment of Large Language Models (LLMs), particularly in the context of adversarial attacks that can lead to harmful outputs. Previous methods primarily relied on discrete adversarial training, which, while effective, is computationally expensive and does not fully capture the vulnerabilities exploited by these attacks. The proposed MixAT method innovatively combines discrete and continuous adversarial training, effectively bridging the gap between the two approaches. The contribution of this paper lies in the introduction of the At Least One Attack Success Rate (ALO-ASR) metric for evaluating model vulnerabilities and demonstrating that MixAT achieves significantly improved robustness (ALO-ASR &lt; 20%) compared to existing defenses (ALO-ASR &gt; 50%) while maintaining comparable computational efficiency. The methodology is rigorously tested across various state-of-the-art attacks, revealing its effectiveness in realistic deployment scenarios and enhancing the robustness-accuracy tradeoff for safer LLMs.</div>
<div class="mono" style="margin-top:8px">本文探讨了确保大型语言模型（LLMs）安全性和对齐性的持续挑战，特别是在对抗性攻击可能导致有害输出的背景下。以往的方法主要依赖于离散对抗训练，虽然有效，但计算成本高，通常需要使用无法充分捕捉离散攻击脆弱性的连续扰动。提出的方法MixAT创新性地结合了离散和连续对抗训练，有效弥合了两者之间的差距。本文的贡献在于引入了至少一次攻击成功率（ALO-ASR）指标来评估模型脆弱性，并证明MixAT显著提高了鲁棒性（ALO-ASR &lt; 20%），相比现有防御方法（ALO-ASR &gt; 50%）在运行时间上也保持了高效。研究结果表明，MixAT在鲁棒性和准确性之间提供了优越的平衡，使其成为开发更安全LLMs的有前景的策略。</div>
</details>
</div>
<div class="card">
<div class="title">SAGE: A Generic Framework for LLM Safety Evaluation</div>
<div class="meta-line">Authors: Madhur Jindal, Hari Shrawgi, Parag Agrawal, Sandipan Dandapat</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-04-28T11:01:08+00:00 · Latest: 2025-10-27T10:19:55+00:00</div>
<div class="meta-line">Comments: Accepted to EMNLP 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.19674v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.19674v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Language Models are rapidly deployed across diverse applications
from healthcare to financial advice, safety evaluation struggles to keep pace.
Current benchmarks focus on single-turn interactions with generic policies,
failing to capture the conversational dynamics of real-world usage and the
application-specific harms that emerge in context. Such potential oversights
can lead to harms that go unnoticed in standard safety benchmarks and other
current evaluation methodologies. To address these needs for robust AI safety
evaluation, we introduce SAGE (Safety AI Generic Evaluation), an automated
modular framework designed for customized and dynamic harm evaluations. SAGE
employs prompted adversarial agents with diverse personalities based on the Big
Five model, enabling system-aware multi-turn conversations that adapt to target
applications and harm policies. We evaluate seven state-of-the-art LLMs across
three applications and harm policies. Multi-turn experiments show that harm
increases with conversation length, model behavior varies significantly when
exposed to different user personalities and scenarios, and some models minimize
harm via high refusal rates that reduce usefulness. We also demonstrate policy
sensitivity within a harm category where tightening a child-focused sexual
policy substantially increases measured defects across applications. These
results motivate adaptive, policy-aware, and context-specific testing for safer
real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAGE：大型语言模型安全评估的通用框架</div>
<div class="mono" style="margin-top:8px">随着大型语言模型在医疗保健到金融建议等多种应用中迅速部署，安全评估难以跟上步伐。当前基准测试侧重于与通用策略的单轮交互，未能捕捉现实使用中的对话动态和特定应用中出现的危害。这些潜在的忽视可能导致在标准安全基准和其他当前评估方法中未被注意的危害。为满足对强大AI安全评估的需求，我们引入了SAGE（安全AI通用评估），这是一个旨在定制和动态危害评估的自动化模块化框架。SAGE采用基于五大人格模型的多样化个性化对抗代理，能够进行系统感知的多轮对话，适应目标应用和危害政策。我们在三个应用和危害政策中评估了七个最先进的LLM。多轮实验表明，随着对话长度的增加，危害也在增加；当暴露于不同用户个性和场景时，模型行为显著变化；一些模型通过高拒绝率来最小化危害，从而降低了实用性。我们还展示了在一个危害类别内的政策敏感性，其中收紧以儿童为中心的性政策显著增加了跨应用的缺陷。这些结果促使我们进行适应性、政策感知和特定上下文的测试，以实现更安全的现实世界部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the urgent need for effective safety evaluation of Large Language Models (LLMs) as they are increasingly utilized in various applications, highlighting the limitations of existing benchmarks that primarily focus on single-turn interactions and generic policies. Previous methods fail to account for the complexities of real-world conversational dynamics and specific application harms, leading to potential oversights in safety assessments. The proposed SAGE framework offers a novel solution by employing prompted adversarial agents with diverse personalities to facilitate multi-turn conversations that are tailored to specific applications and harm policies. This approach is well-motivated as it allows for a more nuanced evaluation of LLMs, revealing that harm tends to increase with conversation length and that model behavior varies significantly based on user personality. The methodology was tested on seven state-of-the-art LLMs across three applications, demonstrating that adaptive and context-specific evaluations can lead to better safety outcomes, thereby supporting the goal of safer real-world deployment.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）在各种应用中日益增长的安全评估需求，强调现有基准测试的局限性，这些测试主要评估单轮交互，忽视了多轮对话和特定应用危害的复杂性。以往的方法未能捕捉到现实环境中出现的对话动态和潜在危害，导致安全评估不足。提出的SAGE框架通过利用基于五大人格模型的对抗性代理，提供了一种新颖的方法，促进动态和定制化的评估，反映现实使用情况。本文的贡献在于展示SAGE在评估七种最先进的LLMs在三种应用中的有效性，揭示了随着对话长度的增加，危害也在增加，并且模型行为受到用户个性的显著影响。这些发现强调了进行适应性和上下文感知的安全评估的必要性，以确保LLMs在实际场景中的安全部署。</div>
</details>
</div>
<div class="card">
<div class="title">Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense   Against Adversarial LLM Jailbreaks</div>
<div class="meta-line">Authors: Md. Mehedi Hasan, Ziaur Rahman, Rafid Mostafiz, Md. Abir Hossain</div>
<div class="meta-line">First: 2025-10-26T11:19:47+00:00 · Latest: 2025-10-26T11:19:47+00:00</div>
<div class="meta-line">Comments: 11 pages, 5 figures. Preprint version under review in the area of
  Artificial Intelligence (cs.AI)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22628v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.22628v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a real-time modular defense system named Sentra-Guard.
The system detects and mitigates jailbreak and prompt injection attacks
targeting large language models (LLMs). The framework uses a hybrid
architecture with FAISS-indexed SBERT embedding representations that capture
the semantic meaning of prompts, combined with fine-tuned transformer
classifiers, which are machine learning models specialized for distinguishing
between benign and adversarial language inputs. It identifies adversarial
prompts in both direct and obfuscated attack vectors. A core innovation is the
classifier-retriever fusion module, which dynamically computes context-aware
risk scores that estimate how likely a prompt is to be adversarial based on its
content and context. The framework ensures multilingual resilience with a
language-agnostic preprocessing layer. This component automatically translates
non-English prompts into English for semantic evaluation, enabling consistent
detection across over 100 languages. The system includes a HITL feedback loop,
where decisions made by the automated system are reviewed by human experts for
continual learning and rapid adaptation under adversarial pressure.
Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and
malicious prompts, enhancing detection reliability and reducing false
positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 =
1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading
baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike
black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible
with diverse LLM backends. Its modular design supports scalable deployment in
both commercial and open-source environments. The system establishes a new
state-of-the-art in adversarial LLM defense.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Sentra-Guard：一种多语言人机框架，用于实时防御对抗性LLM越狱</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为Sentra-Guard的实时模块化防御系统。该系统检测并缓解针对大型语言模型（LLMs）的越狱和提示注入攻击。该框架采用混合架构，使用FAISS索引的SBERT嵌入表示来捕捉提示的语义含义，并结合微调的变换器分类器，这些机器学习模型专门用于区分良性和对抗性语言输入。它识别直接和模糊攻击向量中的对抗性提示。核心创新是分类器-检索器融合模块，它动态计算上下文感知风险评分，估计提示基于其内容和上下文的对抗性可能性。该框架通过语言无关的预处理层确保多语言的韧性。该组件自动将非英语提示翻译成英语进行语义评估，从而在100多种语言中实现一致检测。该系统包括一个人机反馈循环，自动系统做出的决策由人类专家审查，以便在对抗压力下进行持续学习和快速适应。Sentra-Guard维护一个不断演变的良性和恶意提示的双标签知识库，提高检测可靠性并减少误报。评估结果显示检测率为99.96%（AUC = 1.00，F1 = 1.00），攻击成功率（ASR）仅为0.004%。这优于领先的基线，如LlamaGuard-2（1.3%）和OpenAI Moderation（3.7%）。与黑箱方法不同，Sentra-Guard是透明的、可微调的，并与多种LLM后端兼容。其模块化设计支持在商业和开源环境中的可扩展部署。该系统在对抗性LLM防御中建立了新的最先进水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concern of adversarial attacks on large language models (LLMs), specifically focusing on jailbreak and prompt injection threats. Previous methods often relied on black-box approaches that lacked transparency and adaptability, leading to higher false positive rates and limited effectiveness. The proposed Sentra-Guard framework introduces a modular defense system that combines FAISS-indexed SBERT embeddings with fine-tuned transformer classifiers, allowing for real-time detection and mitigation of adversarial prompts in over 100 languages. This system features a classifier-retriever fusion module that computes context-aware risk scores and includes a human-in-the-loop feedback mechanism for continual learning. The methodology demonstrates a remarkable 99.96% detection rate and a mere 0.004% attack success rate, significantly outperforming existing solutions like LlamaGuard-2 and OpenAI Moderation, thereby establishing a new benchmark in adversarial LLM defense.</div>
<div class="mono" style="margin-top:8px">本研究关注日益严重的对抗性攻击，特别是针对大型语言模型（LLM）的越狱和提示注入攻击，这对其完整性和功能构成重大风险。以往的方法在检测准确性和适应性方面存在不足，通常依赖于缺乏透明度的黑箱方法。提出的Sentra-Guard框架引入了一种混合架构，将FAISS索引的SBERT嵌入与微调的变换器分类器相结合，通过分类器-检索器融合模块计算上下文感知风险评分，从而增强对对抗性提示的检测。这种创新方法因其对多语言和透明防御系统的需求而具有良好的动机，取得了99.96%的卓越检测率和仅0.004%的攻击成功率，显著优于现有解决方案如LlamaGuard-2和OpenAI Moderation。该方法支持对超过100种语言的对抗输入进行实时防御，确立了对抗性LLM防御的新基准。</div>
</details>
</div>
<div class="card">
<div class="title">Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks</div>
<div class="meta-line">Authors: Havva Alizadeh Noughabi, Julien Serbanescu, Fattane Zarrinkalam, Ali Dehghantanha</div>
<div class="meta-line">First: 2025-10-24T19:20:23+00:00 · Latest: 2025-10-24T19:20:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.21983v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.21983v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent advances, Large Language Models remain vulnerable to jailbreak
attacks that bypass alignment safeguards and elicit harmful outputs. While
prior research has proposed various attack strategies differing in human
readability and transferability, little attention has been paid to the
linguistic and psychological mechanisms that may influence a model&#x27;s
susceptibility to such attacks. In this paper, we examine an interdisciplinary
line of research that leverages foundational theories of persuasion from the
social sciences to craft adversarial prompts capable of circumventing alignment
constraints in LLMs. Drawing on well-established persuasive strategies, we
hypothesize that LLMs, having been trained on large-scale human-generated text,
may respond more compliantly to prompts with persuasive structures.
Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive
fingerprints that emerge in their jailbreak responses. Empirical evaluations
across multiple aligned LLMs reveal that persuasion-aware prompts significantly
bypass safeguards, demonstrating their potential to induce jailbreak behaviors.
This work underscores the importance of cross-disciplinary insight in
addressing the evolving challenges of LLM safety. The code and data are
available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>揭示大型语言模型在越狱攻击中的说服指纹</div>
<div class="mono" style="margin-top:8px">尽管最近取得了进展，大型语言模型仍然容易受到越狱攻击，这些攻击绕过了对齐保护措施并引发有害输出。虽然之前的研究提出了不同的人类可读性和可转移性的各种攻击策略，但对可能影响模型对这些攻击的易感性的语言和心理机制关注较少。本文考察了一条跨学科的研究路线，利用社会科学中的说服基础理论，设计能够绕过大型语言模型对齐约束的对抗性提示。基于成熟的说服策略，我们假设大型语言模型在大规模人类生成文本上训练后，可能对具有说服结构的提示反应更顺从。此外，我们还研究大型语言模型是否在其越狱响应中表现出独特的说服指纹。对多个对齐大型语言模型的实证评估表明，关注说服的提示显著绕过了保护措施，展示了它们诱发越狱行为的潜力。这项工作强调了跨学科洞察在应对大型语言模型安全不断演变的挑战中的重要性。代码和数据可用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the vulnerability of Large Language Models (LLMs) to jailbreak attacks that can bypass alignment safeguards and produce harmful outputs. Previous methods have focused on various attack strategies but often overlooked the linguistic and psychological factors influencing a model&#x27;s susceptibility. The proposed approach integrates theories of persuasion from social sciences to create adversarial prompts that exploit persuasive structures, hypothesizing that LLMs may respond more favorably to these prompts due to their training on human-generated text. The paper contributes to the understanding of LLM safety by demonstrating that persuasion-aware prompts can effectively bypass existing safeguards, with empirical evaluations showing significant success across multiple aligned LLMs in inducing jailbreak behaviors. This research highlights the necessity of interdisciplinary approaches in tackling the challenges of LLM safety.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在越狱攻击中的脆弱性，这些攻击利用对齐保护措施，而这一问题在语言和心理因素对模型易受攻击性的影响方面尚未得到充分研究。以往的方法侧重于各种攻击策略，但往往忽视了可能增强攻击效果的说服机制。本文提出了一种跨学科的方法，利用社会科学中的基础说服理论，创建旨在绕过LLM对齐约束的对抗性提示。该方法论涉及对多个对齐LLM的实证评估，结果表明，围绕说服策略构建的提示显著增加了诱发越狱行为的可能性。这些发现有助于理解LLM安全性，强调了说服技术在增强攻击有效性方面的作用，从而支持了对抗此类脆弱性改进防御的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Shape it Up! Restoring LLM Safety during Finetuning</div>
<div class="meta-line">Authors: ShengYun Peng, Pin-Yu Chen, Jianfeng Chi, Seongmin Lee, Duen Horng Chau</div>
<div class="meta-line">First: 2025-05-22T18:05:16+00:00 · Latest: 2025-10-24T14:34:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.17196v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.17196v2">PDF</a> · <a href="https://github.com/poloclub/star-dss">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Finetuning large language models (LLMs) enables user-specific customization
but introduces critical safety risks: even a few harmful examples can
compromise safety alignment. A common mitigation strategy is to update the
model more strongly on examples deemed safe, while downweighting or excluding
those flagged as unsafe. However, because safety context can shift within a
single example, updating the model equally on both harmful and harmless parts
of a response is suboptimal-a coarse treatment we term static safety shaping.
In contrast, we propose dynamic safety shaping (DSS), a framework that uses
fine-grained safety signals to reinforce learning from safe segments of a
response while suppressing unsafe content. To enable such fine-grained control
during finetuning, we introduce a key insight: guardrail models, traditionally
used for filtering, can be repurposed to evaluate partial responses, tracking
how safety risk evolves throughout the response, segment by segment. This leads
to the Safety Trajectory Assessment of Response (STAR), a token-level signal
that enables shaping to operate dynamically over the training sequence.
Building on this, we present STAR-DSS, guided by STAR scores, that robustly
mitigates finetuning risks and delivers substantial safety improvements across
diverse threats, datasets, and model families-all without compromising
capability on intended tasks. We encourage future safety research to build on
dynamic shaping principles for stronger mitigation against evolving finetuning
risks. Our code is publicly available at https://github.com/poloclub/star-dss.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical safety risks associated with fine-tuning large language models (LLMs), where even a few harmful examples can undermine safety alignment. Previous methods typically employed static safety shaping, which inadequately handled the nuanced safety context within individual examples by treating harmful and harmless segments equally. The proposed dynamic safety shaping (DSS) approach improves upon this by utilizing fine-grained safety signals to enhance learning from safe parts of responses while suppressing unsafe content. This method introduces the Safety Trajectory Assessment of Response (STAR), which evaluates partial responses and tracks safety risk evolution at a token level, allowing for dynamic adjustments during training. The STAR-DSS framework demonstrates significant safety improvements across various threats and datasets without sacrificing performance on intended tasks, thus contributing to the field by providing a more effective strategy for mitigating finetuning risks in LLMs.</div>
<div class="mono" style="margin-top:8px">本研究解决了微调大型语言模型（LLMs）所带来的关键安全风险，甚至少量有害示例也可能破坏安全对齐。以往的方法通常采用静态安全塑形，这种方法对有害和无害部分的处理不够充分，导致结果不理想。提出的动态安全塑形（DSS）框架通过利用细粒度的安全信号来增强对安全部分的学习，同时抑制不安全内容，从而改善了这一点。该方法的动机充分，因为它允许通过护栏模型对响应进行更细致的评估，逐段跟踪安全风险，从而产生响应的安全轨迹评估（STAR）。STAR-DSS方法在微调过程中展示了在各种威胁、数据集和模型系列中显著的安全改进，能够有效减轻风险，同时不影响模型在预期任务上的性能。</div>
</details>
</div>
<div class="card">
<div class="title">RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety   Evaluation via Role-Specialized Collaboration</div>
<div class="meta-line">Authors: Xiuyuan Chen, Jian Zhao, Yuchen Yuan, Tianle Zhang, Huilin Zhou, Zheng Zhu, Ping Hu, Linghe Kong, Chi Zhang, Weiran Huang, Xuelong Li</div>
<div class="meta-line">First: 2025-09-28T09:35:32+00:00 · Latest: 2025-10-23T03:33:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.25271v4">Abs</a> · <a href="http://arxiv.org/pdf/2509.25271v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing safety evaluation methods for large language models (LLMs) suffer
from inherent limitations, including evaluator bias and detection failures
arising from model homogeneity, which collectively undermine the robustness of
risk evaluation processes. This paper seeks to re-examine the risk evaluation
paradigm by introducing a theoretical framework that reconstructs the
underlying risk concept space. Specifically, we decompose the latent risk
concept space into three mutually exclusive subspaces: the explicit risk
subspace (encompassing direct violations of safety guidelines), the implicit
risk subspace (capturing potential malicious content that requires contextual
reasoning for identification), and the non-risk subspace. Furthermore, we
propose RADAR, a multi-agent collaborative evaluation framework that leverages
multi-round debate mechanisms through four specialized complementary roles and
employs dynamic update mechanisms to achieve self-evolution of risk concept
distributions. This approach enables comprehensive coverage of both explicit
and implicit risks while mitigating evaluator bias. To validate the
effectiveness of our framework, we construct an evaluation dataset comprising
800 challenging cases. Extensive experiments on our challenging testset and
public benchmarks demonstrate that RADAR significantly outperforms baseline
evaluation methods across multiple dimensions, including accuracy, stability,
and self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87%
improvement in risk identification accuracy compared to the strongest baseline
evaluation method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RADAR：一种风险意识动态多智能体框架，用于通过角色专门化协作评估大型语言模型的安全性</div>
<div class="mono" style="margin-top:8px">现有的大型语言模型（LLM）安全评估方法存在固有局限性，包括评估者偏见和由于模型同质性导致的检测失败，这些问题共同削弱了风险评估过程的稳健性。本文旨在通过引入一个理论框架重新审视风险评估范式，该框架重构了潜在风险概念空间。具体而言，我们将潜在风险概念空间分解为三个相互排斥的子空间：显性风险子空间（包括对安全指南的直接违反）、隐性风险子空间（捕捉需要上下文推理才能识别的潜在恶意内容）和非风险子空间。此外，我们提出了RADAR，一个多智能体协作评估框架，利用四个专门互补角色的多轮辩论机制，并采用动态更新机制实现风险概念分布的自我演化。这种方法能够全面覆盖显性和隐性风险，同时减轻评估者偏见。为了验证我们框架的有效性，我们构建了一个包含800个挑战性案例的评估数据集。在我们的挑战性测试集和公共基准上的广泛实验表明，RADAR在多个维度上显著优于基线评估方法，包括准确性、稳定性和自我评估风险敏感性。值得注意的是，RADAR在风险识别准确性方面相比最强基线评估方法提高了28.87%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing safety evaluation methods for large language models (LLMs), which often suffer from evaluator bias and detection failures due to model homogeneity. Previous methods have not effectively tackled these issues, leading to inadequate risk assessments. The proposed approach, RADAR, introduces a theoretical framework that redefines the risk concept space by categorizing it into explicit, implicit, and non-risk subspaces, and employs a multi-agent collaborative evaluation framework that utilizes specialized roles and dynamic updates to enhance risk identification. This method is well-motivated as it aims to provide a more robust evaluation process. The contribution of the paper lies in its innovative framework and methodology, which includes extensive experiments on a dataset of 800 challenging cases, demonstrating that RADAR significantly outperforms baseline methods, achieving a 28.87% improvement in risk identification accuracy.</div>
<div class="mono" style="margin-top:8px">本文针对现有大型语言模型（LLMs）安全评估方法的局限性进行了研究，这些方法常常受到评估者偏见和由于模型同质性导致的检测失败的影响。以往的方法在有效识别显性和隐性风险方面存在困难，导致风险评估不足。提出的方法RADAR引入了一种多智能体协作评估框架，利用风险概念空间的理论重构，并通过专业角色促进动态更新和风险分布的自我演变。本文的贡献在于全面覆盖风险类型，并能够减轻评估中的偏见。该方法论涉及智能体之间的多轮辩论机制，并通过800个案例的评估数据集进行验证，结果表明RADAR在风险识别准确性上比基线方法显著提高28.87%，从而支持其增强LLM安全评估的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities</div>
<div class="meta-line">Authors: Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng, Aidan San, Michel Galley, Jianfeng Gao</div>
<div class="meta-line">Venue: NAACL 2025 Oral</div>
<div class="meta-line">First: 2024-10-24T06:36:12+00:00 · Latest: 2025-10-23T00:57:57+00:00</div>
<div class="meta-line">Comments: Accepted to NAACL 2025 Main (Oral)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2410.18469v5">Abs</a> · <a href="http://arxiv.org/pdf/2410.18469v5">PDF</a> · <a href="https://github.com/SunChungEn/ADV-LLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research has shown that Large Language Models (LLMs) are vulnerable to
automated jailbreak attacks, where adversarial suffixes crafted by algorithms
appended to harmful queries bypass safety alignment and trigger unintended
responses. Current methods for generating these suffixes are computationally
expensive and have low Attack Success Rates (ASR), especially against
well-aligned models like Llama2 and Llama3. To overcome these limitations, we
introduce ADV-LLM, an iterative self-tuning process that crafts adversarial
LLMs with enhanced jailbreak ability. Our framework significantly reduces the
computational cost of generating adversarial suffixes while achieving nearly
100\% ASR on various open-source LLMs. Moreover, it exhibits strong attack
transferability to closed-source models, achieving 99\% ASR on GPT-3.5 and 49\%
ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving
jailbreak ability, ADV-LLM provides valuable insights for future safety
alignment research through its ability to generate large datasets for studying
LLM safety. Our code is available at: https://github.com/SunChungEn/ADV-LLM</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强越狱能力的迭代自调节大型语言模型</div>
<div class="mono" style="margin-top:8px">最近的研究表明，大型语言模型（LLMs）易受自动化越狱攻击，算法生成的对抗后缀附加在有害查询上，绕过安全对齐并触发意外响应。目前生成这些后缀的方法计算成本高，攻击成功率（ASR）低，尤其是针对像Llama2和Llama3这样的良好对齐模型。为克服这些限制，我们提出了ADV-LLM，一种迭代自调节过程，旨在制作具有增强越狱能力的对抗LLMs。我们的框架显著降低了生成对抗后缀的计算成本，同时在各种开源LLMs上实现了近100%的ASR。此外，它对闭源模型表现出强大的攻击可转移性，在GPT-3.5上实现了99%的ASR，在GPT-4上实现了49%的ASR，尽管仅在Llama3上进行了优化。除了提高越狱能力，ADV-LLM还通过其生成大规模数据集的能力，为未来的安全对齐研究提供了宝贵的见解。我们的代码可在以下网址获取：https://github.com/SunChungEn/ADV-LLM</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of Large Language Models (LLMs) to automated jailbreak attacks, where adversarial suffixes can bypass safety mechanisms and elicit unintended responses. Previous methods for generating these suffixes have been criticized for being computationally intensive and yielding low Attack Success Rates (ASR), particularly against well-aligned models like Llama2 and Llama3. The proposed ADV-LLM introduces an iterative self-tuning process that significantly reduces computational costs while achieving nearly 100% ASR across various open-source LLMs and demonstrating strong transferability to closed-source models, with 99% ASR on GPT-3.5 and 49% ASR on GPT-4. This contribution not only enhances jailbreak capabilities but also aids future safety alignment research by generating large datasets for LLM safety studies.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）对自动越狱攻击的脆弱性，其中对抗后缀可以绕过安全措施并引发意外响应。以往生成这些后缀的方法计算成本高且攻击成功率（ASR）低，尤其是在对抗像Llama2和Llama3这样的良好对齐模型时。提出的ADV-LLM引入了一种迭代自调节过程，显著降低了计算成本，同时在各种开源LLM上实现了近100%的ASR，并在封闭源模型上表现出强大的迁移能力，在GPT-3.5上达到99%的ASR，在GPT-4上达到49%的ASR。该贡献不仅增强了越狱能力，还通过生成大型数据集为未来的安全对齐研究提供了帮助。</div>
</details>
</div>
<div class="card">
<div class="title">NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn   LLM Jailbreaks</div>
<div class="meta-line">Authors: Javad Rafiei Asl, Sidhant Narula, Mohammad Ghasemigol, Eduardo Blanco, Daniel Takabi</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-10-03T18:24:14+00:00 · Latest: 2025-10-21T17:41:58+00:00</div>
<div class="meta-line">Comments: This paper has been accepted in the main conference proceedings of
  the 2025 Conference on Empirical Methods in Natural Language Processing
  (EMNLP 2025). Javad Rafiei Asl and Sidhant Narula are co-first authors</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.03417v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.03417v2">PDF</a> · <a href="https://github.com/inspire-lab/NEXUS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have revolutionized natural language processing
but remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks
that distribute malicious intent across benign exchanges and bypass alignment
mechanisms. Existing approaches often explore the adversarial space poorly,
rely on hand-crafted heuristics, or lack systematic query refinement. We
present NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular
framework for constructing, refining, and executing optimized multi-turn
attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a
harmful intent into a structured semantic network of topics, entities, and
query chains; (2) a feedback-driven Simulator that iteratively refines and
prunes these chains through attacker-victim-judge LLM collaboration using
harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser
that adaptively navigates the refined query space for real-time attacks. This
pipeline uncovers stealthy, high-success adversarial paths across LLMs. On
several closed-source and open-source LLMs, NEXUS increases attack success rate
by 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NEXUS：用于利用多轮 LLM 越狱中不安全序列的网络探索</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）已彻底改变自然语言处理，但仍然容易受到越狱攻击，尤其是将恶意意图分散在良性交流中的多轮越狱，绕过对齐机制。现有方法往往对对抗空间探索不足，依赖手工设计的启发式方法，或缺乏系统的查询优化。我们提出了 NEXUS（用于利用不安全序列的网络探索），这是一个构建、优化和执行多轮攻击的模块化框架。NEXUS 包括：（1）ThoughtNet，它将有害意图分层扩展为主题、实体和查询链的结构化语义网络；（2）一个基于反馈的模拟器，通过攻击者-受害者-评判者 LLM 协作，使用有害性和语义相似性基准迭代优化和修剪这些链；（3）一个网络遍历器，适应性地导航优化后的查询空间以进行实时攻击。该流程揭示了 LLM 中隐秘的高成功率对抗路径。在多个闭源和开源 LLM 上，NEXUS 的攻击成功率比之前的方法提高了 2.1% 到 19.4%。代码：https://github.com/inspire-lab/NEXUS</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of Large Language Models (LLMs) to jailbreak attacks, particularly multi-turn jailbreaks that cleverly distribute malicious intent across seemingly benign interactions, thus evading existing alignment mechanisms. Previous methods have struggled with inadequate exploration of the adversarial space, reliance on hand-crafted heuristics, and lack of systematic query refinement. The proposed NEXUS framework improves upon these limitations by providing a modular approach that includes ThoughtNet for hierarchical expansion of harmful intents, a feedback-driven Simulator for iterative refinement, and a Network Traverser for adaptive navigation of query spaces. This methodology significantly enhances the effectiveness of multi-turn attacks, achieving an increase in attack success rates by 2.1% to 19.4% compared to prior techniques across various LLMs, thereby supporting the goal of more robust adversarial exploration.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在多轮越狱攻击中的脆弱性，尤其是那些在良性互动中掩盖恶意意图的攻击，而现有方法由于对对抗空间的探索不足和依赖启发式方法，未能有效应对这一问题。所提出的NEXUS框架通过提供一个模块化系统，系统地构建、优化和执行多轮攻击，采用结构化语义网络和迭代反馈机制，区别于现有方法。该方法的动机明确，因为它在提升对抗攻击有效性的同时保持隐蔽性。其研究方法包括ThoughtNet用于恶意意图的层次扩展、反馈驱动的模拟器用于优化查询链，以及网络遍历器用于自适应导航查询空间。NEXUS在多个LLM上显示出攻击成功率的显著提升，增加幅度在2.1%到19.4%之间，从而支持其增强越狱攻击有效性的目标。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251102_1356.html">20251102_1356</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
