<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-11 03:49</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251111_0349</div>
    <div class="row"><div class="card">
<div class="title">XBreaking: Understanding how LLMs security alignment can be broken</div>
<div class="meta-line">Authors: Marco Arazzi, Vignesh Kumar Kembu, Antonino Nocera, Vinod P</div>
<div class="meta-line">First: 2025-04-30T14:44:24+00:00 · Latest: 2025-11-07T16:21:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.21700v3">Abs</a> · <a href="http://arxiv.org/pdf/2504.21700v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models are fundamental actors in the modern IT landscape
dominated by AI solutions. However, security threats associated with them might
prevent their reliable adoption in critical application scenarios such as
government organizations and medical institutions. For this reason, commercial
LLMs typically undergo a sophisticated censoring mechanism to eliminate any
harmful output they could possibly produce. These mechanisms maintain the
integrity of LLM alignment by guaranteeing that the models respond safely and
ethically. In response to this, attacks on LLMs are a significant threat to
such protections, and many previous approaches have already demonstrated their
effectiveness across diverse domains. Existing LLM attacks mostly adopt a
generate-and-test strategy to craft malicious input. To improve the
comprehension of censoring mechanisms and design a targeted attack, we propose
an Explainable-AI solution that comparatively analyzes the behavior of censored
and uncensored models to derive unique exploitable alignment patterns. Then, we
propose XBreaking, a novel approach that exploits these unique patterns to
break the security and alignment constraints of LLMs by targeted noise
injection. Our thorough experimental campaign returns important insights about
the censoring mechanisms and demonstrates the effectiveness and performance of
our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>XBreaking：理解大型语言模型的安全对齐如何被破坏</div>
<div class="mono" style="margin-top:8px">大型语言模型是现代IT环境中由AI解决方案主导的基本参与者。然而，与之相关的安全威胁可能会阻碍它们在政府组织和医疗机构等关键应用场景中的可靠采用。因此，商业大型语言模型通常会经过复杂的审查机制，以消除它们可能产生的任何有害输出。这些机制通过确保模型安全和伦理地响应来维护大型语言模型的对齐完整性。对此，针对大型语言模型的攻击对这些保护构成了重大威胁，许多先前的方法已经在不同领域展示了其有效性。现有的大型语言模型攻击主要采用生成与测试策略来制作恶意输入。为了提高对审查机制的理解并设计针对性的攻击，我们提出了一种可解释的AI解决方案，比较分析审查和未审查模型的行为，以推导出独特的可利用对齐模式。然后，我们提出了XBreaking，这是一种新颖的方法，通过有针对性的噪声注入利用这些独特模式来破坏大型语言模型的安全性和对齐约束。我们全面的实验活动提供了关于审查机制的重要见解，并展示了我们方法的有效性和性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the security vulnerabilities of Large Language Models (LLMs), which are increasingly used in critical applications but face threats that hinder their safe adoption. Previous methods primarily utilized a generate-and-test strategy for crafting malicious inputs, which often lacked a systematic understanding of the underlying censoring mechanisms. The proposed approach, XBreaking, leverages Explainable-AI to analyze the behavior of both censored and uncensored models, identifying exploitable alignment patterns that can be targeted through noise injection. This methodology contributes to a deeper understanding of LLM security and alignment, demonstrating its effectiveness through comprehensive experiments that reveal insights into censoring mechanisms and validate the performance of the proposed attack strategy.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）的安全漏洞，这些模型在人工智能应用中至关重要，但面临威胁，阻碍其在政府和医疗等敏感领域的可靠使用。以往的方法主要采用生成-测试策略来制作恶意输入，通常缺乏对底层审查机制的深入理解。提出的方法XBreaking利用可解释人工智能分析审查和未审查模型的行为，识别可通过噪声注入进行针对性攻击的可利用对齐模式。该方法具有良好的动机，因为它旨在增强对LLM安全性的理解，同时提供一种系统化的方式来利用识别出的弱点。实验结果揭示了审查机制的重要见解，并证明XBreaking有效地破坏了LLM的安全性和对齐性，支持其提高模型抵御攻击能力的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities</div>
<div class="meta-line">Authors: Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng, Aidan San, Michel Galley, Jianfeng Gao</div>
<div class="meta-line">Venue: NAACL 2025 Oral</div>
<div class="meta-line">First: 2024-10-24T06:36:12+00:00 · Latest: 2025-11-07T10:17:59+00:00</div>
<div class="meta-line">Comments: Accepted to NAACL 2025 Main (Oral)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2410.18469v6">Abs</a> · <a href="http://arxiv.org/pdf/2410.18469v6">PDF</a> · <a href="https://github.com/SunChungEn/ADV-LLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research has shown that Large Language Models (LLMs) are vulnerable to
automated jailbreak attacks, where adversarial suffixes crafted by algorithms
appended to harmful queries bypass safety alignment and trigger unintended
responses. Current methods for generating these suffixes are computationally
expensive and have low Attack Success Rates (ASR), especially against
well-aligned models like Llama2 and Llama3. To overcome these limitations, we
introduce ADV-LLM, an iterative self-tuning process that crafts adversarial
LLMs with enhanced jailbreak ability. Our framework significantly reduces the
computational cost of generating adversarial suffixes while achieving nearly
100\% ASR on various open-source LLMs. Moreover, it exhibits strong attack
transferability to closed-source models, achieving 99\% ASR on GPT-3.5 and 49\%
ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving
jailbreak ability, ADV-LLM provides valuable insights for future safety
alignment research through its ability to generate large datasets for studying
LLM safety. Our code is available at: https://github.com/SunChungEn/ADV-LLM</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强越狱能力的迭代自调节大型语言模型</div>
<div class="mono" style="margin-top:8px">最近的研究表明，大型语言模型（LLMs）易受自动化越狱攻击，算法生成的对抗后缀附加在有害查询上，绕过安全对齐并触发意外响应。目前生成这些后缀的方法计算成本高且攻击成功率（ASR）低，尤其是针对像Llama2和Llama3这样的良好对齐模型。为克服这些限制，我们提出了ADV-LLM，一种迭代自调节过程，旨在制作具有增强越狱能力的对抗LLMs。我们的框架显著降低了生成对抗后缀的计算成本，同时在各种开源LLMs上实现了近100%的ASR。此外，它对闭源模型表现出强大的攻击可转移性，在GPT-3.5上实现了99%的ASR，在GPT-4上实现了49%的ASR，尽管仅在Llama3上进行了优化。除了提高越狱能力，ADV-LLM还通过其生成大规模数据集的能力，为未来的安全对齐研究提供了宝贵的见解。我们的代码可在以下网址获取：https://github.com/SunChungEn/ADV-LLM</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of Large Language Models (LLMs) to automated jailbreak attacks, where adversarial suffixes can bypass safety measures and elicit unintended responses. Previous methods for generating these suffixes were computationally intensive and had low Attack Success Rates (ASR), particularly against well-aligned models like Llama2 and Llama3. The proposed ADV-LLM introduces an iterative self-tuning process that significantly reduces computational costs while achieving nearly 100% ASR across various open-source LLMs, and demonstrates strong attack transferability to closed-source models, achieving 99% ASR on GPT-3.5 and 49% ASR on GPT-4. This contribution not only enhances jailbreak capabilities but also aids future safety alignment research by generating large datasets for studying LLM safety, thus addressing critical gaps in existing methodologies.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在自动越狱攻击中的脆弱性，这些攻击利用对抗后缀绕过安全机制。以往生成这些后缀的方法计算成本高且攻击成功率（ASR）低，尤其是在像Llama2和Llama3这样的良好对齐模型上。提出的ADV-LLM引入了一种迭代自调节过程，显著降低了计算成本，同时在各种开源LLMs上实现了近100%的ASR，并在封闭源模型上表现出强大的迁移能力，在GPT-3.5上达到99%的ASR，在GPT-4上达到49%的ASR。该贡献不仅增强了越狱能力，还通过生成大型数据集为未来的安全对齐研究提供了帮助。</div>
</details>
</div>
<div class="card">
<div class="title">AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research</div>
<div class="meta-line">Authors: Tim Beyer, Jonas Dornbusch, Jakob Steimle, Moritz Ladenburger, Leo Schwinn, Stephan Günnemann</div>
<div class="meta-line">First: 2025-11-06T12:38:09+00:00 · Latest: 2025-11-06T12:38:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.04316v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.04316v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid expansion of research on Large Language Model (LLM) safety and
robustness has produced a fragmented and oftentimes buggy ecosystem of
implementations, datasets, and evaluation methods. This fragmentation makes
reproducibility and comparability across studies challenging, hindering
meaningful progress. To address these issues, we introduce AdversariaLLM, a
toolbox for conducting LLM jailbreak robustness research. Its design centers on
reproducibility, correctness, and extensibility. The framework implements
twelve adversarial attack algorithms, integrates seven benchmark datasets
spanning harmfulness, over-refusal, and utility evaluation, and provides access
to a wide range of open-weight LLMs via Hugging Face. The implementation
includes advanced features for comparability and reproducibility such as
compute-resource tracking, deterministic results, and distributional evaluation
techniques. \name also integrates judging through the companion package
JudgeZoo, which can also be used independently. Together, these components aim
to establish a robust foundation for transparent, comparable, and reproducible
research in LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdversariaLLM：一个统一的模块化LLM鲁棒性研究工具箱</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）安全性和鲁棒性研究的快速扩展产生了一个支离破碎且常常存在缺陷的实现、数据集和评估方法生态系统。这种碎片化使得研究之间的可重复性和可比性变得具有挑战性，阻碍了有意义的进展。为了解决这些问题，我们推出了AdversariaLLM，一个用于进行LLM越狱鲁棒性研究的工具箱。其设计以可重复性、正确性和可扩展性为中心。该框架实现了十二种对抗攻击算法，整合了七个涵盖有害性、过度拒绝和效用评估的基准数据集，并通过Hugging Face提供对各种开放权重LLM的访问。该实现包括可比性和可重复性的高级特性，如计算资源跟踪、确定性结果和分布评估技术。\name还通过伴随包JudgeZoo集成了评判功能，后者也可以独立使用。这些组件共同旨在为LLM安全研究建立一个透明、可比和可重复的坚实基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges posed by the fragmented ecosystem of implementations, datasets, and evaluation methods in Large Language Model (LLM) safety and robustness, which complicates reproducibility and comparability across studies. Previous methods have often resulted in buggy implementations and lack of standardization, leading to difficulties in meaningful progress. The proposed AdversariaLLM toolbox aims to unify and modularize LLM robustness research by providing a comprehensive framework that includes twelve adversarial attack algorithms, seven benchmark datasets, and access to various open-weight LLMs, all designed with a focus on reproducibility, correctness, and extensibility. This toolbox contributes to the field by establishing a robust foundation for transparent and comparable research, incorporating advanced features such as compute-resource tracking and deterministic results. The methodology allows researchers to conduct thorough evaluations of LLM jailbreak robustness, facilitating improved performance in safety assessments and supporting the overarching goals of enhancing LLM reliability.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLM）安全性和鲁棒性研究中，由于实现、数据集和评估方法的碎片化而带来的挑战，这使得研究的可重复性和可比性变得复杂。以往的方法常常导致实现存在缺陷，并缺乏标准化评估，进而导致评估模型鲁棒性时的困难。所提出的AdversariaLLM工具箱旨在通过提供一个全面的框架来统一和模块化LLM鲁棒性研究，该框架包括十二种对抗攻击算法、七个基准数据集以及对多种开放权重LLM的访问，从而增强可重复性、正确性和可扩展性。该方法强调计算资源跟踪和确定性结果等特性，以促进透明和可比的研究。该工具箱的设计旨在支持LLM的鲁棒性评估，为可重复研究奠定了坚实的基础，最终改善了对LLM安全性和鲁棒性的评估。</div>
</details>
</div>
<div class="card">
<div class="title">An Automated Framework for Strategy Discovery, Retrieval, and Evolution   in LLM Jailbreak Attacks</div>
<div class="meta-line">Authors: Xu Liu, Yan Chen, Kan Ling, Yichi Zhu, Hengrun Zhang, Guisheng Fan, Huiqun Yu</div>
<div class="meta-line">First: 2025-11-04T08:24:22+00:00 · Latest: 2025-11-04T08:24:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.02356v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.02356v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread deployment of Large Language Models (LLMs) as public-facing
web services and APIs has made their security a core concern for the web
ecosystem. Jailbreak attacks, as one of the significant threats to LLMs, have
recently attracted extensive research. In this paper, we reveal a jailbreak
strategy which can effectively evade current defense strategies. It can extract
valuable information from failed or partially successful attack attempts and
contains self-evolution from attack interactions, resulting in sufficient
strategy diversity and adaptability. Inspired by continuous learning and
modular design principles, we propose ASTRA, a jailbreak framework that
autonomously discovers, retrieves, and evolves attack strategies to achieve
more efficient and adaptive attacks. To enable this autonomous evolution, we
design a closed-loop &quot;attack-evaluate-distill-reuse&quot; core mechanism that not
only generates attack prompts but also automatically distills and generalizes
reusable attack strategies from every interaction. To systematically accumulate
and apply this attack knowledge, we introduce a three-tier strategy library
that categorizes strategies into Effective, Promising, and Ineffective based on
their performance scores. The strategy library not only provides precise
guidance for attack generation but also possesses exceptional extensibility and
transferability. We conduct extensive experiments under a black-box setting,
and the results show that ASTRA achieves an average Attack Success Rate (ASR)
of 82.7%, significantly outperforming baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对LLM越狱攻击的策略发现、检索和演化的自动化框架</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）作为面向公众的网络服务和API的广泛部署，使其安全性成为网络生态系统的核心关注点。越狱攻击作为对LLMs的重要威胁之一，最近引起了广泛研究。本文揭示了一种能够有效规避当前防御策略的越狱策略。它可以从失败或部分成功的攻击尝试中提取有价值的信息，并包含来自攻击交互的自我演化，导致足够的策略多样性和适应性。受到持续学习和模块化设计原则的启发，我们提出了ASTRA，一个能够自主发现、检索和演化攻击策略的越狱框架，以实现更高效和适应性的攻击。为了实现这种自主演化，我们设计了一个闭环的“攻击-评估-提炼-重用”核心机制，不仅生成攻击提示，还自动提炼和概括每次交互中的可重用攻击策略。为了系统地积累和应用这些攻击知识，我们引入了一个三层策略库，根据其性能评分将策略分类为有效、前景良好和无效。策略库不仅为攻击生成提供精确指导，还具有卓越的可扩展性和可转移性。我们在黑箱设置下进行了广泛实验，结果表明ASTRA的平均攻击成功率（ASR）达到82.7%，显著优于基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the security vulnerabilities of Large Language Models (LLMs) in the context of jailbreak attacks, which pose significant threats to their deployment as public services. Previous methods have struggled with limited adaptability and effectiveness against evolving attack strategies, leading to a need for a more dynamic approach. The proposed framework, ASTRA, distinguishes itself by autonomously discovering, retrieving, and evolving attack strategies through a closed-loop mechanism that enhances strategy diversity and adaptability. This paper contributes a systematic methodology for accumulating and applying attack knowledge via a three-tier strategy library, categorizing strategies based on performance. Experimental results demonstrate that ASTRA achieves an average Attack Success Rate (ASR) of 82.7%, indicating its effectiveness in supporting the goal of more efficient and adaptive jailbreak attacks.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLM）在越狱攻击背景下的安全漏洞，这对其作为公共服务的部署构成了重大威胁。以往的方法在适应性和有效性方面存在不足，无法有效规避防御，因此需要一种更动态的方法。所提出的框架ASTRA通过自主发现、检索和演化攻击策略，采用闭环机制增强策略的多样性和适应性，与现有方法有所不同。该方法受到持续学习和模块化设计原则的启发，通过三层策略库系统地积累攻击知识，具有良好的动机。该方法在黑箱环境下进行了测试，平均攻击成功率（ASR）达到82.7%，证明其在提高攻击效率和适应性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Prompt Injection as an Emerging Threat: Evaluating the Resilience of   Large Language Models</div>
<div class="meta-line">Authors: Daniyal Ganiuly, Assel Smaiyl</div>
<div class="meta-line">First: 2025-11-03T14:43:56+00:00 · Latest: 2025-11-03T14:43:56+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.01634v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.01634v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly used in intelligent systems
that perform reasoning, summarization, and code generation. Their ability to
follow natural-language instructions, while powerful, also makes them
vulnerable to a new class of attacks known as prompt injection. In these
attacks, hidden or malicious instructions are inserted into user inputs or
external content, causing the model to ignore its intended task or produce
unsafe responses. This study proposes a unified framework for evaluating how
resistant Large Language Models (LLMs) are to prompt injection attacks. The
framework defines three complementary metrics such as the Resilience
Degradation Index (RDI), Safety Compliance Coefficient (SCC), and Instructional
Integrity Metric (IIM) to jointly measure robustness, safety, and semantic
stability. We evaluated four instruction-tuned models (GPT-4, GPT-4o, LLaMA-3
8B Instruct, and Flan-T5-Large) on five common language tasks: question
answering, summarization, translation, reasoning, and code generation. Results
show that GPT-4 performs best overall, while open-weight models remain more
vulnerable. The findings highlight that strong alignment and safety tuning are
more important for resilience than model size alone. Results show that all
models remain partially vulnerable, especially to indirect and direct-override
attacks. GPT-4 achieved the best overall resilience (RDR = 9.8 %, SCR = 96.4
%), while open-source models exhibited higher performance degradation and lower
safety scores. The findings demonstrate that alignment strength and safety
tuning play a greater role in resilience than model size alone. The proposed
framework offers a structured, reproducible approach for assessing model
robustness and provides practical insights for improving LLM safety and
reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提示注入作为新兴威胁：评估大型语言模型的韧性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在执行推理、摘要和代码生成的智能系统中越来越多地被使用。它们遵循自然语言指令的能力虽然强大，但也使其容易受到一种新型攻击的威胁，即提示注入。在这些攻击中，隐藏或恶意的指令被插入到用户输入或外部内容中，导致模型忽略其预期任务或产生不安全的响应。本研究提出了一个统一框架，用于评估大型语言模型（LLMs）对提示注入攻击的抵抗力。该框架定义了三个互补指标，如韧性降级指数（RDI）、安全合规系数（SCC）和指令完整性指标（IIM），以共同衡量鲁棒性、安全性和语义稳定性。我们对四个经过指令调优的模型（GPT-4、GPT-4o、LLaMA-3 8B Instruct 和 Flan-T5-Large）在五个常见语言任务上进行了评估：问答、摘要、翻译、推理和代码生成。结果显示，GPT-4的整体表现最佳，而开放权重模型则更容易受到攻击。研究结果强调，强对齐和安全调优对韧性的重要性超过了模型大小。结果表明，所有模型仍然部分脆弱，尤其是对间接和直接覆盖攻击。GPT-4实现了最佳的整体韧性（RDR = 9.8%，SCR = 96.4%），而开源模型表现出更高的性能降级和较低的安全评分。研究结果表明，对齐强度和安全调优在韧性中发挥的作用大于模型大小。所提出的框架提供了一种结构化、可重复的方法来评估模型的鲁棒性，并为提高LLM的安全性和可靠性提供了实用见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the emerging threat of prompt injection attacks on Large Language Models (LLMs), which can compromise their intended tasks and safety. Previous methods lacked a comprehensive evaluation framework for assessing model resilience against such attacks. This study introduces a unified framework that incorporates three metrics: Resilience Degradation Index (RDI), Safety Compliance Coefficient (SCC), and Instructional Integrity Metric (IIM), which together measure robustness, safety, and semantic stability. The methodology involves evaluating four instruction-tuned models across five language tasks, revealing that while GPT-4 shows the best overall performance, all models exhibit vulnerabilities, particularly to direct and indirect attacks. The findings underscore the importance of alignment and safety tuning over model size in enhancing resilience, contributing valuable insights for improving LLM safety and reliability.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）面临的提示注入攻击新威胁，这种攻击通过在用户输入中插入恶意指令来损害模型性能。以往的方法缺乏全面的框架来评估模型对这种攻击的抵御能力，导致对其脆弱性的理解不足。本文提出了一个统一框架，包括三个指标：抵御降级指数（RDI）、安全合规系数（SCC）和指令完整性指标（IIM），共同评估模型的鲁棒性、安全性和语义稳定性。该方法通过在五个语言任务上评估四个指令调优模型，结果显示虽然GPT-4表现出最高的抵御能力，但所有模型仍然部分脆弱，尤其是对某些攻击类型。研究结果强调了对齐和安全调优在增强抵御能力方面的重要性，贡献了一个结构化的方法来评估和改善LLM的安全性和可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM   Judges</div>
<div class="meta-line">Authors: Hamin Koo, Minseon Kim, Jaehyung Kim</div>
<div class="meta-line">First: 2025-11-03T09:18:27+00:00 · Latest: 2025-11-03T09:18:27+00:00</div>
<div class="meta-line">Comments: under review, 28 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.01375v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.01375v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对齐与错位：使用元优化的LLM评估者进行自动LLM越狱</div>
<div class="mono" style="margin-top:8px">识别大型语言模型（LLM）的脆弱性对于通过解决固有弱点来提高其安全性至关重要。越狱是指对手通过精心设计的输入提示绕过保护措施，在红队测试中发挥核心作用，通过探测LLM以引发意外或不安全的行为。最近的基于优化的越狱方法通过利用LLM迭代地优化攻击提示。然而，它们通常严重依赖于稀疏的二元攻击成功率（ASR）信号或手动制作的评分模板，这引入了人为偏见和评分结果的不确定性。为了解决这些局限性，我们引入了AMIS（对齐与错位），一个通过双层结构共同演化越狱提示和评分模板的元优化框架。在内循环中，使用固定评分模板通过细粒度和密集反馈来优化提示。在外循环中，使用ASR对齐分数优化模板，逐渐演变以更好地反映查询的真实攻击结果。这个共同优化过程产生了逐渐更强的越狱提示和更校准的评分信号。在AdvBench和JBB-Behaviors上的评估表明，AMIS实现了最先进的性能，包括在Claude-3.5-Haiku上达到88.0%的ASR和在Claude-4-Sonnet上达到100.0%的ASR，显著超越现有基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical need to identify vulnerabilities in large language models (LLMs) to enhance their safety, particularly focusing on the issue of jailbreaks where adversaries exploit weaknesses through crafted prompts. Previous methods primarily relied on binary attack success rate signals or manually created scoring templates, both of which presented challenges such as sparsity and human bias. The proposed approach, AMIS (Align to MISalign), introduces a meta-optimization framework that simultaneously evolves jailbreak prompts and scoring templates in a bi-level structure, thereby mitigating the limitations of past methods. This framework utilizes fine-grained feedback for prompt refinement and optimizes the scoring template based on an attack success rate alignment score, resulting in improved prompt effectiveness and scoring accuracy. The methodology was evaluated on AdvBench and JBB-Behaviors, achieving state-of-the-art performance with an 88.0% attack success rate on Claude-3.5-Haiku and a perfect 100.0% on Claude-4-Sonnet, significantly surpassing existing benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究针对识别大型语言模型（LLMs）漏洞的关键需求，以增强其抵御对抗性攻击的安全性，特别关注利用这些弱点的越狱攻击。以往的越狱提示优化方法依赖于稀疏的二元攻击成功率（ASR）信号或偏见的手工评分模板，导致效率低下和不准确。提出的方法AMIS（Align to MISalign）引入了一种元优化框架，通过双层结构同时演化越狱提示和评分模板，有效解决了现有方法的局限性。该框架在内循环中使用细粒度反馈进行提示优化，在外循环中基于ASR对齐分数优化评分模板。该方法在AdvBench和JBB-Behaviors等任务上取得了显著贡献，表现出色，包括在Claude-3.5-Haiku上达到88.0%的ASR和在Claude-4-Sonnet上达到100.0%的ASR，从而验证了所提方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Exploiting Latent Space Discontinuities for Building Universal LLM   Jailbreaks and Data Extraction Attacks</div>
<div class="meta-line">Authors: Kayua Oleques Paim, Rodrigo Brandao Mansilha, Diego Kreutz, Muriel Figueredo Franco, Weverton Cordeiro</div>
<div class="meta-line">First: 2025-11-01T01:19:12+00:00 · Latest: 2025-11-01T01:19:12+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures, 4 tables, Published at the Brazilian Symposium
  on Cybersecurity (SBSeg 2025)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.00346v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.00346v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid proliferation of Large Language Models (LLMs) has raised
significant concerns about their security against adversarial attacks. In this
work, we propose a novel approach to crafting universal jailbreaks and data
extraction attacks by exploiting latent space discontinuities, an architectural
vulnerability related to the sparsity of training data. Unlike previous
methods, our technique generalizes across various models and interfaces,
proving highly effective in seven state-of-the-art LLMs and one image
generation model. Initial results indicate that when these discontinuities are
exploited, they can consistently and profoundly compromise model behavior, even
in the presence of layered defenses. The findings suggest that this strategy
has substantial potential as a systemic attack vector.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用潜在空间不连续性构建通用LLM越狱和数据提取攻击</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的快速普及引发了对其安全性在对抗性攻击下的重大担忧。在本研究中，我们提出了一种新颖的方法，通过利用潜在空间不连续性（与训练数据稀疏性相关的架构漏洞）来制作通用越狱和数据提取攻击。与之前的方法不同，我们的技术在各种模型和接口中具有广泛的适用性，在七个最先进的LLM和一个图像生成模型中证明了其高效性。初步结果表明，当利用这些不连续性时，它们可以持续而深刻地影响模型行为，即使在存在分层防御的情况下。这些发现表明，这一策略作为系统性攻击向量具有巨大的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing security concerns surrounding Large Language Models (LLMs) due to their vulnerability to adversarial attacks. Previous methods for exploiting these vulnerabilities have been limited in their applicability and effectiveness across different models. The proposed approach leverages latent space discontinuities, a specific architectural flaw linked to training data sparsity, allowing for the creation of universal jailbreaks and data extraction attacks that generalize across multiple models and interfaces. This method is well-motivated as it targets a systemic issue within LLM architectures. The study demonstrates that this technique is effective in compromising the behavior of seven state-of-the-art LLMs and one image generation model, indicating its potential as a significant attack vector even against layered defenses.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）日益严重的安全问题，尤其是它们对对抗性攻击的脆弱性。以往的攻击方法在范围和有效性上存在局限，通常无法在不同模型和接口之间进行泛化。本文提出了一种新方法，通过利用潜在空间的不连续性，这是一种与训练数据稀疏性相关的特定架构脆弱性，从而创建通用的越狱和数据提取攻击。该方法在七个最先进的LLM和一个图像生成模型中表现出显著的有效性，表明利用这些不连续性可以在层级防御下持续破坏模型行为。研究结果表明，这种方法为理解LLM安全弱点提供了有希望的系统攻击途径。</div>
</details>
</div>
<div class="card">
<div class="title">SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM   Agents</div>
<div class="meta-line">Authors: Sheng Yin, Xianghe Pang, Yuanzhuo Ding, Menglan Chen, Yutong Bi, Yichen Xiong, Wenhao Huang, Zhen Xiang, Jing Shao, Siheng Chen</div>
<div class="meta-line">First: 2024-12-17T18:55:58+00:00 · Latest: 2025-10-31T08:18:50+00:00</div>
<div class="meta-line">Comments: 28 pages, 19 tables, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.13178v5">Abs</a> · <a href="http://arxiv.org/pdf/2412.13178v5">PDF</a> · <a href="https://github.com/shengyin1224/SafeAgentBench">Code1</a> · <a href="https://huggingface.co/datasets/safeagentbench/SafeAgentBench">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the integration of large language models (LLMs), embodied agents have
strong capabilities to understand and plan complicated natural language
instructions. However, a foreseeable issue is that those embodied agents can
also flawlessly execute some hazardous tasks, potentially causing damages in
the real world. Existing benchmarks predominantly overlook critical safety
risks, focusing solely on planning performance, while a few evaluate LLMs&#x27;
safety awareness only on non-interactive image-text data. To address this gap,
we present SafeAgentBench -- the first comprehensive benchmark for safety-aware
task planning of embodied LLM agents in interactive simulation environments,
covering both explicit and implicit hazards. SafeAgentBench includes: (1) an
executable, diverse, and high-quality dataset of 750 tasks, rigorously curated
to cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal
embodied environment with a low-level controller, supporting multi-agent
execution with 17 high-level actions for 9 state-of-the-art baselines; and (3)
reliable evaluation methods from both execution and semantic perspectives.
Experimental results show that, although agents based on different design
frameworks exhibit substantial differences in task success rates, their overall
safety awareness remains weak. The most safety-conscious baseline achieves only
a 10% rejection rate for detailed hazardous tasks. Moreover, simply replacing
the LLM driving the agent does not lead to notable improvements in safety
awareness. Dataset and codes are available in
https://github.com/shengyin1224/SafeAgentBench and
https://huggingface.co/datasets/safeagentbench/SafeAgentBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeAgentBench：具身LLM代理安全任务规划基准</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）的集成，具身代理具备理解和规划复杂自然语言指令的强大能力。然而，一个可预见的问题是，这些具身代理也可能无误地执行一些危险任务，从而在现实世界中造成损害。现有基准主要忽视关键的安全风险，仅关注规划性能，而少数评估LLMs的安全意识仅基于非交互式的图像-文本数据。为了解决这一空白，我们提出了SafeAgentBench——第一个全面的具身LLM代理在交互式仿真环境中安全任务规划的基准，涵盖显性和隐性危害。SafeAgentBench包括：（1）一个可执行的、多样化的高质量数据集，包含750个任务，严格策划以覆盖10种潜在危害和3种任务类型；（2）SafeAgentEnv，一个具有低级控制器的通用具身环境，支持多代理执行，提供9个最先进基线的17个高级动作；（3）从执行和语义角度出发的可靠评估方法。实验结果表明，尽管基于不同设计框架的代理在任务成功率上存在显著差异，但它们的整体安全意识仍然较弱。最具安全意识的基线在详细危险任务中的拒绝率仅为10%。此外，仅仅更换驱动代理的LLM并未显著改善安全意识。数据集和代码可在https://github.com/shengyin1224/SafeAgentBench和https://huggingface.co/datasets/safeagentbench/SafeAgentBench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical issue of safety in task planning for embodied agents utilizing large language models (LLMs), as existing benchmarks primarily focus on planning performance while neglecting safety risks. Previous methods have inadequately assessed safety awareness, often relying on non-interactive data, which fails to capture real-world hazards. The proposed SafeAgentBench introduces a comprehensive benchmark that evaluates safety-aware task planning in interactive environments, featuring a diverse dataset of 750 tasks that encompass various hazards and a universal environment for multi-agent execution. The methodology includes rigorous evaluation from both execution and semantic perspectives, revealing that current agents, despite differing design frameworks, demonstrate weak safety awareness, with the best baseline achieving only a 10% rejection rate for hazardous tasks. This highlights the need for improved safety measures in embodied agents, supporting the goal of enhancing their operational safety in real-world applications.</div>
<div class="mono" style="margin-top:8px">本研究关注由大型语言模型（LLMs）驱动的具身代理在任务规划中的安全性问题，因为现有基准主要关注规划性能，而忽视了安全风险。以往的方法要么忽略了安全考虑，要么在非交互环境中评估LLMs的安全意识，导致对现实世界危害的评估不足。提出的SafeAgentBench提供了一个全面的基准，包括一个涵盖多种危害的750个任务的多样化数据集、一个用于多代理执行的交互环境以及稳健的评估方法。这种方法使得在交互模拟中对安全意识的任务规划进行全面检验。实验结果显示，尽管设计框架存在差异，代理的安全意识仍然较弱，表现最好的基线在危险任务中的拒绝率仅为10%，表明在安全性能方面存在显著差距，而该基准旨在解决这一问题。</div>
</details>
</div>
<div class="card">
<div class="title">ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for   Audio-Language Models</div>
<div class="meta-line">Authors: Weifei Jin, Yuxin Cao, Junjie Su, Minhui Xue, Jie Hao, Ke Xu, Jin Song Dong, Derui Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-30T03:19:59+00:00 · Latest: 2025-10-30T03:19:59+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.26096v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.26096v1">PDF</a> · <a href="https://github.com/WeifeiJin/ALMGuard">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Audio-Language Models (ALMs) have significantly improved
multimodal understanding capabilities. However, the introduction of the audio
modality also brings new and unique vulnerability vectors. Previous studies
have proposed jailbreak attacks that specifically target ALMs, revealing that
defenses directly transferred from traditional audio adversarial attacks or
text-based Large Language Model (LLM) jailbreaks are largely ineffective
against these ALM-specific threats. To address this issue, we propose ALMGuard,
the first defense framework tailored to ALMs. Based on the assumption that
safety-aligned shortcuts naturally exist in ALMs, we design a method to
identify universal Shortcut Activation Perturbations (SAPs) that serve as
triggers that activate the safety shortcuts to safeguard ALMs at inference
time. To better sift out effective triggers while preserving the model&#x27;s
utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM),
which restricts perturbations to Mel-frequency bins that are sensitive to
jailbreaks but insensitive to speech understanding. Both theoretical analyses
and empirical results demonstrate the robustness of our method against both
seen and unseen attacks. Overall, \MethodName reduces the average success rate
of advanced ALM-specific jailbreak attacks to 4.6% across four models, while
maintaining comparable utility on benign benchmarks, establishing it as the new
state of the art. Our code and data are available at
https://github.com/WeifeiJin/ALMGuard.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ALMGuard：音频语言模型的安全捷径及其发现方式</div>
<div class="mono" style="margin-top:8px">最近音频语言模型（ALMs）的进展显著提升了多模态理解能力。然而，音频模态的引入也带来了新的独特脆弱性向量。之前的研究提出了专门针对ALMs的越狱攻击，揭示了从传统音频对抗攻击或基于文本的大型语言模型（LLM）越狱直接转移的防御在这些ALM特定威胁面前大多无效。为了解决这个问题，我们提出了ALMGuard，这是第一个针对ALMs量身定制的防御框架。基于安全对齐捷径自然存在于ALMs的假设，我们设计了一种方法来识别通用捷径激活扰动（SAPs），作为触发器以激活安全捷径，在推理时保护ALMs。为了更好地筛选有效触发器，同时保持模型在良性任务上的效用，我们进一步提出了梅尔梯度稀疏掩码（M-GSM），该方法将扰动限制在对越狱敏感但对语音理解不敏感的梅尔频率区间。理论分析和实证结果均表明我们的方法对已知和未知攻击的鲁棒性。总体而言，\MethodName将四个模型中先进的ALM特定越狱攻击的平均成功率降低到4.6%，同时在良性基准上保持了可比的效用，确立了其作为新的最先进技术。我们的代码和数据可在https://github.com/WeifeiJin/ALMGuard获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities introduced by Audio-Language Models (ALMs), which have improved multimodal understanding but are susceptible to unique attacks. Previous methods, which adapted defenses from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks, have proven ineffective against ALM-specific threats. The proposed approach, ALMGuard, is motivated by the existence of safety-aligned shortcuts within ALMs and introduces a novel method to identify Shortcut Activation Perturbations (SAPs) that activate these safety measures during inference. The methodology includes the Mel-Gradient Sparse Mask (M-GSM) to ensure that perturbations target sensitive Mel-frequency bins while preserving model performance on benign tasks. The experimental results show that ALMGuard reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, thus establishing a new state of the art in this domain.</div>
<div class="mono" style="margin-top:8px">本研究关注音频语言模型（ALMs）引入的脆弱性，这些模型在多模态理解方面取得了显著进展，但也容易受到独特的越狱攻击。以往的方法是从传统音频对抗攻击或基于文本的大型语言模型越狱中转移防御措施，但这些措施在应对ALM特定威胁时效果不佳。提出的ALMGuard方法基于安全对齐快捷方式在ALMs中自然存在的假设，设计了一种识别通用快捷激活扰动（SAPs）的方法，以在推理时激活这些安全机制。该方法还引入了梅尔梯度稀疏掩码（M-GSM），确保扰动仅针对对越狱敏感但对语音理解不敏感的梅尔频率区间。实验结果表明，ALMGuard将四个模型中先进的ALM特定越狱攻击的平均成功率降低到4.6%，同时在良性基准上保持了可比的效用，标志着该领域的重要进展。</div>
</details>
</div>
<div class="card">
<div class="title">Improving LLM Safety Alignment with Dual-Objective Optimization</div>
<div class="meta-line">Authors: Xuandong Zhao, Will Cai, Tianneng Shi, David Huang, Licong Lin, Song Mei, Dawn Song</div>
<div class="meta-line">Venue: ICML 2025</div>
<div class="meta-line">First: 2025-03-05T18:01:05+00:00 · Latest: 2025-10-30T01:16:06+00:00</div>
<div class="meta-line">Comments: ICML 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.03710v3">Abs</a> · <a href="http://arxiv.org/pdf/2503.03710v3">PDF</a> · <a href="https://github.com/wicai24/DOOR-Alignment">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing training-time safety alignment techniques for large language models
(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization
(DPO), a widely deployed alignment method, exhibits limitations in both
experimental and theoretical contexts as its loss function proves suboptimal
for refusal learning. Through gradient-based analysis, we identify these
shortcomings and propose an improved safety alignment that disentangles DPO
objectives into two components: (1) robust refusal training, which encourages
refusal even when partial unsafe generations are produced, and (2) targeted
unlearning of harmful knowledge. This approach significantly increases LLM
robustness against a wide range of jailbreak attacks, including prefilling,
suffix, and multi-turn attacks across both in-distribution and
out-of-distribution scenarios. Furthermore, we introduce a method to emphasize
critical refusal tokens by incorporating a reward-based token-level weighting
mechanism for refusal learning, which further improves the robustness against
adversarial exploits. Our research also suggests that robustness to jailbreak
attacks is correlated with token distribution shifts in the training process
and internal representations of refusal and harmful tokens, offering valuable
directions for future research in LLM safety alignment. The code is available
at https://github.com/wicai24/DOOR-Alignment</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过双目标优化提高大型语言模型的安全对齐</div>
<div class="mono" style="margin-top:8px">现有的大型语言模型（LLMs）训练时安全对齐技术仍然容易受到越狱攻击。直接偏好优化（DPO）作为一种广泛应用的对齐方法，在实验和理论背景下均表现出局限性，因为其损失函数在拒绝学习中被证明是次优的。通过基于梯度的分析，我们识别了这些缺陷，并提出了一种改进的安全对齐方法，将DPO目标分解为两个组成部分：（1）强健的拒绝训练，鼓励在产生部分不安全生成时仍然拒绝，以及（2）针对有害知识的有针对性的去学习。这种方法显著提高了LLM对各种越狱攻击的鲁棒性，包括预填充、后缀和多轮攻击，涵盖了分布内和分布外场景。此外，我们引入了一种通过结合基于奖励的令牌级加权机制来强调关键拒绝令牌的方法，以进一步提高对抗性利用的鲁棒性。我们的研究还表明，对越狱攻击的鲁棒性与训练过程中的令牌分布变化以及拒绝和有害令牌的内部表示相关，为未来LLM安全对齐的研究提供了宝贵的方向。代码可在 https://github.com/wicai24/DOOR-Alignment 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of large language models (LLMs) to jailbreak attacks, highlighting the limitations of existing safety alignment techniques, particularly direct preference optimization (DPO), which is suboptimal for refusal learning. The proposed method improves upon DPO by disentangling its objectives into robust refusal training and targeted unlearning of harmful knowledge, effectively enhancing LLM robustness against various jailbreak attacks. This dual-objective optimization approach is well-motivated as it directly addresses the shortcomings of previous methods. The methodology involves gradient-based analysis and a reward-based token-level weighting mechanism to emphasize critical refusal tokens, resulting in significant improvements in performance against adversarial exploits. The findings indicate that the proposed method substantially increases robustness in both in-distribution and out-of-distribution scenarios, supporting the goal of enhancing LLM safety alignment.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）在面对越狱攻击时的脆弱性，强调了现有训练时安全对齐技术的局限性，特别是直接偏好优化（DPO），其在拒绝学习方面表现不佳。所提出的方法通过将DPO的目标分解为稳健的拒绝训练和有针对性的有害知识去学习，有效解决了识别出的不足之处。本文的贡献在于通过一种新颖的基于奖励的令牌级加权机制显著增强了LLM对各种越狱攻击的鲁棒性。该方法论涉及基于梯度的分析和双目标优化框架的实施，展示了在抵御分布内和分布外攻击方面的改进性能，从而支持了提高LLM安全对齐的目标。</div>
</details>
</div>
<div class="card">
<div class="title">MixAT: Combining Continuous and Discrete Adversarial Training for LLMs</div>
<div class="meta-line">Authors: Csaba Dékány, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, Martin Vechev</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-22T17:32:50+00:00 · Latest: 2025-10-28T09:41:22+00:00</div>
<div class="meta-line">Comments: Published at 39th Conference on Neural Information Processing Systems
  (NeurIPS 2025)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.16947v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.16947v2">PDF</a> · <a href="https://github.com/insait-institute/MixAT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent efforts in Large Language Model (LLM) safety and alignment,
current adversarial attacks on frontier LLMs can still consistently force
harmful generations. Although adversarial training has been widely studied and
shown to significantly improve the robustness of traditional machine learning
models, its strengths and weaknesses in the context of LLMs are less
understood. Specifically, while existing discrete adversarial attacks are
effective at producing harmful content, training LLMs with concrete adversarial
prompts is often computationally expensive, leading to reliance on continuous
relaxations. At the same time, despite their effectiveness and generalization
capabilities, training with continuous perturbations does not always capture
the full spectrum of vulnerabilities exploited by discrete attacks. In this
work, we aim to bridge this gap by introducing MixAT, a novel method that
combines stronger discrete and faster continuous attacks during training. We
rigorously evaluate MixAT across a wide spectrum of state-of-the-art attacks,
proposing the At Least One Attack Success Rate (ALO-ASR) metric to capture the
worst-case vulnerability of models. We show MixAT achieves substantially better
robustness (ALO-ASR &lt; 20%) compared to prior defenses (ALO-ASR &gt; 50%), while
maintaining a runtime comparable to methods based on continuous relaxations. We
further analyze MixAT in realistic deployment settings, exploring how chat
templates, quantization, low-rank adapters, and temperature affect both
adversarial training and evaluation, revealing additional blind spots in
current methodologies. Our results demonstrate that MixAT&#x27;s discrete-continuous
defense offers a principled and superior robustness-accuracy tradeoff with
minimal computational overhead, highlighting its promise for building safer
LLMs. We provide our code and models at
https://github.com/insait-institute/MixAT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MixAT：结合连续和离散对抗训练的LLM</div>
<div class="mono" style="margin-top:8px">尽管近期在大型语言模型（LLM）安全性和对齐方面进行了努力，当前对前沿LLM的对抗攻击仍能持续强制生成有害内容。虽然对抗训练已被广泛研究并显示出显著提高传统机器学习模型的鲁棒性，但在LLM背景下其优缺点尚不清楚。具体而言，现有的离散对抗攻击在生成有害内容方面有效，但使用具体对抗提示训练LLM通常计算成本高，导致依赖于连续松弛。同时，尽管连续扰动的训练有效且具有泛化能力，但并不总能捕捉到离散攻击所利用的全部脆弱性。在本研究中，我们旨在通过引入MixAT这一新方法来填补这一空白，该方法在训练过程中结合了更强的离散攻击和更快的连续攻击。我们对MixAT在广泛的最先进攻击中进行了严格评估，提出了至少一个攻击成功率（ALO-ASR）指标，以捕捉模型的最坏情况脆弱性。我们展示了MixAT在鲁棒性方面显著优于先前防御（ALO-ASR &lt; 20%），而运行时间与基于连续松弛的方法相当。我们进一步分析了MixAT在现实部署环境中的表现，探讨了聊天模板、量化、低秩适配器和温度如何影响对抗训练和评估，揭示了当前方法中的额外盲点。我们的结果表明，MixAT的离散-连续防御提供了原则性和优越的鲁棒性-准确性权衡，计算开销最小，突显了其构建更安全LLM的潜力。我们在https://github.com/insait-institute/MixAT提供了我们的代码和模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the ongoing challenges in ensuring the safety and alignment of Large Language Models (LLMs), particularly in the face of adversarial attacks that can lead to harmful outputs. Previous methods primarily focused on either discrete adversarial training, which is effective but computationally intensive, or continuous perturbations, which lack the ability to fully capture the vulnerabilities targeted by discrete attacks. The proposed MixAT method combines both discrete and continuous adversarial training, effectively bridging the gap between these approaches. The contribution of this paper lies in its introduction of the At Least One Attack Success Rate (ALO-ASR) metric for evaluating model vulnerabilities and demonstrating that MixAT achieves significantly improved robustness (ALO-ASR &lt; 20%) compared to existing defenses (ALO-ASR &gt; 50%), while maintaining comparable computational efficiency. The methodology includes rigorous evaluations across various state-of-the-art attacks and analyses of deployment settings, revealing critical insights into adversarial training and its implications for LLM safety.</div>
<div class="mono" style="margin-top:8px">本文解决了大型语言模型（LLMs）在安全性和对齐方面面临的持续挑战，特别是在面对可能导致有害输出的对抗性攻击时。以往的方法主要集中在离散对抗训练上，虽然有效，但计算成本高，且通常依赖于无法捕捉全部脆弱性的连续扰动。提出的MixAT方法创新性地结合了离散和连续对抗训练，有效弥补了两者之间的差距。本文的贡献在于引入了至少一个攻击成功率（ALO-ASR）指标来评估模型脆弱性，研究表明MixAT显著增强了鲁棒性（ALO-ASR &lt; 20%），相比之下，现有防御方法的ALO-ASR超过50%，同时保持了相似的计算效率。这项研究为改善LLMs的鲁棒性与准确性权衡提供了希望，使其在部署时更加安全。</div>
</details>
</div>
<div class="card">
<div class="title">SAGE: A Generic Framework for LLM Safety Evaluation</div>
<div class="meta-line">Authors: Madhur Jindal, Hari Shrawgi, Parag Agrawal, Sandipan Dandapat</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-04-28T11:01:08+00:00 · Latest: 2025-10-27T10:19:55+00:00</div>
<div class="meta-line">Comments: Accepted to EMNLP 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.19674v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.19674v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Language Models are rapidly deployed across diverse applications
from healthcare to financial advice, safety evaluation struggles to keep pace.
Current benchmarks focus on single-turn interactions with generic policies,
failing to capture the conversational dynamics of real-world usage and the
application-specific harms that emerge in context. Such potential oversights
can lead to harms that go unnoticed in standard safety benchmarks and other
current evaluation methodologies. To address these needs for robust AI safety
evaluation, we introduce SAGE (Safety AI Generic Evaluation), an automated
modular framework designed for customized and dynamic harm evaluations. SAGE
employs prompted adversarial agents with diverse personalities based on the Big
Five model, enabling system-aware multi-turn conversations that adapt to target
applications and harm policies. We evaluate seven state-of-the-art LLMs across
three applications and harm policies. Multi-turn experiments show that harm
increases with conversation length, model behavior varies significantly when
exposed to different user personalities and scenarios, and some models minimize
harm via high refusal rates that reduce usefulness. We also demonstrate policy
sensitivity within a harm category where tightening a child-focused sexual
policy substantially increases measured defects across applications. These
results motivate adaptive, policy-aware, and context-specific testing for safer
real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAGE：大型语言模型安全评估的通用框架</div>
<div class="mono" style="margin-top:8px">随着大型语言模型在医疗保健到金融建议等多种应用中迅速部署，安全评估难以跟上步伐。目前的基准测试侧重于与通用策略的单轮交互，未能捕捉到真实使用中的对话动态和上下文中出现的特定应用危害。这些潜在的忽视可能导致在标准安全基准和其他当前评估方法中未被注意的危害。为满足对强大AI安全评估的需求，我们引入了SAGE（安全AI通用评估），这是一个旨在定制和动态危害评估的自动化模块化框架。SAGE采用基于五大人格模型的多样化个性化对抗代理，能够进行系统感知的多轮对话，适应目标应用和危害政策。我们在三个应用和危害政策中评估了七个最先进的LLM。多轮实验表明，随着对话长度的增加，危害也在增加；当暴露于不同用户个性和场景时，模型行为显著变化；一些模型通过高拒绝率来最小化危害，从而降低了实用性。我们还展示了在一个危害类别内的政策敏感性，其中收紧以儿童为中心的性政策显著增加了跨应用的测量缺陷。这些结果激励了针对更安全的现实世界部署进行自适应、政策感知和上下文特定的测试。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the urgent need for effective safety evaluation of Large Language Models (LLMs) as they are increasingly utilized in various applications, highlighting the limitations of existing benchmarks that primarily assess single-turn interactions without considering the complexities of real-world conversational dynamics. Previous methods often overlook context-specific harms, leading to potential oversights in safety assessments. The proposed SAGE framework offers a novel approach by employing prompted adversarial agents with diverse personalities based on the Big Five model, facilitating system-aware multi-turn conversations that adapt to specific applications and harm policies. This paper contributes by demonstrating that harm can escalate with longer conversations and that model behavior is significantly influenced by user personality and scenario variations. The methodology involves evaluating seven state-of-the-art LLMs across three applications and harm policies, revealing critical insights into policy sensitivity and the need for adaptive testing to enhance safety in practical deployments.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLM）在各种应用中日益增长的使用需求，提出了有效的安全评估方法，现有基准测试仅评估单轮交互，忽视了现实世界对话动态的复杂性。以往的方法未能充分捕捉特定应用的危害，导致安全评估中可能存在的疏漏。提出的SAGE框架通过利用具有多样化个性的对抗性代理，促进针对特定应用和危害政策的多轮对话，提供了一种新颖的解决方案。这种方法的动机明确，旨在增强人工智能安全评估的稳健性。该方法论涉及对七种最先进的LLM在三个应用和危害政策下进行评估，结果显示，随着对话长度的增加，危害往往增加，模型行为在用户个性影响下显著变化。研究发现，一些模型通过高拒绝率来减少危害，这可能会影响其有效性，并强调了适应性和特定上下文测试在现实世界安全部署中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense   Against Adversarial LLM Jailbreaks</div>
<div class="meta-line">Authors: Md. Mehedi Hasan, Ziaur Rahman, Rafid Mostafiz, Md. Abir Hossain</div>
<div class="meta-line">First: 2025-10-26T11:19:47+00:00 · Latest: 2025-10-26T11:19:47+00:00</div>
<div class="meta-line">Comments: 11 pages, 5 figures. Preprint version under review in the area of
  Artificial Intelligence (cs.AI)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22628v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.22628v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a real-time modular defense system named Sentra-Guard.
The system detects and mitigates jailbreak and prompt injection attacks
targeting large language models (LLMs). The framework uses a hybrid
architecture with FAISS-indexed SBERT embedding representations that capture
the semantic meaning of prompts, combined with fine-tuned transformer
classifiers, which are machine learning models specialized for distinguishing
between benign and adversarial language inputs. It identifies adversarial
prompts in both direct and obfuscated attack vectors. A core innovation is the
classifier-retriever fusion module, which dynamically computes context-aware
risk scores that estimate how likely a prompt is to be adversarial based on its
content and context. The framework ensures multilingual resilience with a
language-agnostic preprocessing layer. This component automatically translates
non-English prompts into English for semantic evaluation, enabling consistent
detection across over 100 languages. The system includes a HITL feedback loop,
where decisions made by the automated system are reviewed by human experts for
continual learning and rapid adaptation under adversarial pressure.
Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and
malicious prompts, enhancing detection reliability and reducing false
positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 =
1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading
baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike
black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible
with diverse LLM backends. Its modular design supports scalable deployment in
both commercial and open-source environments. The system establishes a new
state-of-the-art in adversarial LLM defense.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Sentra-Guard：一种多语言人机框架，用于实时防御对抗性LLM越狱</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为Sentra-Guard的实时模块化防御系统。该系统检测并缓解针对大型语言模型（LLMs）的越狱和提示注入攻击。该框架采用混合架构，使用FAISS索引的SBERT嵌入表示来捕捉提示的语义意义，结合微调的变换器分类器，这些机器学习模型专门用于区分良性和对抗性语言输入。它识别直接和模糊攻击向量中的对抗性提示。核心创新是分类器-检索器融合模块，它动态计算上下文感知风险评分，估计提示基于其内容和上下文的对抗性可能性。该框架确保多语言的韧性，具有语言无关的预处理层。该组件自动将非英语提示翻译成英语进行语义评估，从而在100多种语言中实现一致检测。该系统包括一个人机反馈循环，自动系统做出的决策由人类专家审查，以便在对抗压力下进行持续学习和快速适应。Sentra-Guard维护一个不断发展的良性和恶意提示的双标签知识库，提高检测可靠性并减少误报。评估结果显示检测率为99.96%（AUC = 1.00，F1 = 1.00），攻击成功率（ASR）仅为0.004%。这优于领先的基线，如LlamaGuard-2（1.3%）和OpenAI Moderation（3.7%）。与黑箱方法不同，Sentra-Guard是透明的、可微调的，并与多种LLM后端兼容。其模块化设计支持在商业和开源环境中的可扩展部署。该系统在对抗性LLM防御中建立了新的最先进水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concern of adversarial attacks on large language models (LLMs), specifically focusing on jailbreak and prompt injection attacks that compromise their integrity. Previous methods often relied on black-box approaches, which lacked transparency and adaptability, leading to higher false positive rates and limited effectiveness. The proposed Sentra-Guard framework introduces a hybrid architecture that combines FAISS-indexed SBERT embeddings with fine-tuned transformer classifiers, allowing for real-time detection and mitigation of adversarial prompts in multiple languages. This system features a classifier-retriever fusion module that computes context-aware risk scores and includes a human-in-the-loop feedback mechanism for continuous improvement. Evaluation results demonstrate a 99.96% detection rate and an attack success rate of only 0.004%, significantly outperforming existing solutions like LlamaGuard-2 and OpenAI Moderation, thereby establishing a new benchmark in adversarial defense for LLMs.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLM）面临的越狱和提示注入攻击的日益威胁，这些攻击在现有的黑箱防御方法中未能得到充分缓解，后者缺乏透明性和适应性。提出的Sentra-Guard框架引入了一种模块化防御系统，结合FAISS索引的SBERT嵌入和微调的变换器分类器，以有效地实时识别对抗性提示。这种方法具有良好的动机，因为它结合了分类器-检索器融合模块进行上下文感知风险评估，以及语言无关的预处理层以支持多语言，增强了检测的可靠性。该方法包括一个人机反馈机制以实现持续学习，最终实现了99.96%的检测率和仅0.004%的攻击成功率，显著优于LlamaGuard-2和OpenAI Moderation等领先基线，从而确立了对抗性LLM防御的新状态。</div>
</details>
</div>
<div class="card">
<div class="title">Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks</div>
<div class="meta-line">Authors: Havva Alizadeh Noughabi, Julien Serbanescu, Fattane Zarrinkalam, Ali Dehghantanha</div>
<div class="meta-line">First: 2025-10-24T19:20:23+00:00 · Latest: 2025-10-24T19:20:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.21983v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.21983v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent advances, Large Language Models remain vulnerable to jailbreak
attacks that bypass alignment safeguards and elicit harmful outputs. While
prior research has proposed various attack strategies differing in human
readability and transferability, little attention has been paid to the
linguistic and psychological mechanisms that may influence a model&#x27;s
susceptibility to such attacks. In this paper, we examine an interdisciplinary
line of research that leverages foundational theories of persuasion from the
social sciences to craft adversarial prompts capable of circumventing alignment
constraints in LLMs. Drawing on well-established persuasive strategies, we
hypothesize that LLMs, having been trained on large-scale human-generated text,
may respond more compliantly to prompts with persuasive structures.
Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive
fingerprints that emerge in their jailbreak responses. Empirical evaluations
across multiple aligned LLMs reveal that persuasion-aware prompts significantly
bypass safeguards, demonstrating their potential to induce jailbreak behaviors.
This work underscores the importance of cross-disciplinary insight in
addressing the evolving challenges of LLM safety. The code and data are
available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>揭示大型语言模型在越狱攻击中的说服指纹</div>
<div class="mono" style="margin-top:8px">尽管最近取得了进展，大型语言模型仍然容易受到越狱攻击，这些攻击绕过了对齐保护措施并引发有害输出。虽然之前的研究提出了不同的人类可读性和可转移性的各种攻击策略，但对可能影响模型对这些攻击的易感性的语言和心理机制关注较少。本文考察了一条跨学科的研究路线，利用社会科学中的说服基础理论，设计能够绕过大型语言模型对齐约束的对抗性提示。基于成熟的说服策略，我们假设大型语言模型在大规模人类生成文本的训练下，可能对具有说服结构的提示反应更顺从。此外，我们还研究了大型语言模型是否在其越狱响应中表现出独特的说服指纹。对多个对齐大型语言模型的实证评估表明，关注说服的提示显著绕过了保护措施，展示了它们诱发越狱行为的潜力。这项工作强调了跨学科洞察在应对大型语言模型安全不断演变的挑战中的重要性。代码和数据可用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of Large Language Models (LLMs) to jailbreak attacks, which exploit weaknesses in alignment safeguards to produce harmful outputs. Previous methods focused on various attack strategies but largely overlooked the linguistic and psychological factors that could affect a model&#x27;s vulnerability. This paper proposes an interdisciplinary approach that utilizes theories of persuasion from social sciences to create adversarial prompts designed to bypass alignment constraints. The methodology involves empirical evaluations of multiple aligned LLMs, revealing that prompts structured around persuasive techniques significantly enhance the likelihood of inducing jailbreak behaviors. The findings highlight the effectiveness of persuasion-aware prompts in compromising LLM safety, thus contributing valuable insights into the ongoing challenges of securing these models.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在监狱攻击中面临的脆弱性，这些攻击能够绕过对齐保护措施并产生有害输出。以往的方法主要集中在各种攻击策略上，但往往忽视了影响模型易受攻击性的语言和心理因素。提出的方法结合了社会科学中的说服理论，创建了利用说服结构的对抗性提示，从而增强了监狱攻击的有效性。研究方法包括对多个对齐的LLMs进行实证评估，结果表明，关注说服的提示能够显著绕过现有的保护措施。研究结果表明，这些提示能够诱发监狱行为，强调了跨学科方法在提高LLM安全性方面的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Shape it Up! Restoring LLM Safety during Finetuning</div>
<div class="meta-line">Authors: ShengYun Peng, Pin-Yu Chen, Jianfeng Chi, Seongmin Lee, Duen Horng Chau</div>
<div class="meta-line">First: 2025-05-22T18:05:16+00:00 · Latest: 2025-10-24T14:34:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.17196v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.17196v2">PDF</a> · <a href="https://github.com/poloclub/star-dss">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Finetuning large language models (LLMs) enables user-specific customization
but introduces critical safety risks: even a few harmful examples can
compromise safety alignment. A common mitigation strategy is to update the
model more strongly on examples deemed safe, while downweighting or excluding
those flagged as unsafe. However, because safety context can shift within a
single example, updating the model equally on both harmful and harmless parts
of a response is suboptimal-a coarse treatment we term static safety shaping.
In contrast, we propose dynamic safety shaping (DSS), a framework that uses
fine-grained safety signals to reinforce learning from safe segments of a
response while suppressing unsafe content. To enable such fine-grained control
during finetuning, we introduce a key insight: guardrail models, traditionally
used for filtering, can be repurposed to evaluate partial responses, tracking
how safety risk evolves throughout the response, segment by segment. This leads
to the Safety Trajectory Assessment of Response (STAR), a token-level signal
that enables shaping to operate dynamically over the training sequence.
Building on this, we present STAR-DSS, guided by STAR scores, that robustly
mitigates finetuning risks and delivers substantial safety improvements across
diverse threats, datasets, and model families-all without compromising
capability on intended tasks. We encourage future safety research to build on
dynamic shaping principles for stronger mitigation against evolving finetuning
risks. Our code is publicly available at https://github.com/poloclub/star-dss.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>形状优化！在微调过程中恢复大型语言模型的安全性</div>
<div class="mono" style="margin-top:8px">微调大型语言模型（LLMs）使用户能够进行特定定制，但引入了关键的安全风险：即使是少量有害示例也可能破坏安全对齐。一种常见的缓解策略是对被认为安全的示例进行更强的模型更新，同时降低或排除被标记为不安全的示例。然而，由于安全上下文可能在单个示例中发生变化，平等地更新模型对有害和无害部分的响应是次优的——我们称之为静态安全塑形。相反，我们提出了动态安全塑形（DSS），这是一个利用细粒度安全信号来强化从响应的安全部分学习，同时抑制不安全内容的框架。为了在微调过程中实现这种细粒度控制，我们引入了一个关键见解：传统上用于过滤的护栏模型可以重新用于评估部分响应，跟踪安全风险如何在整个响应中逐段演变。这导致了响应的安全轨迹评估（STAR），这是一个令牌级信号，使塑形能够在训练序列中动态运行。在此基础上，我们提出了STAR-DSS，基于STAR分数，稳健地减轻微调风险，并在各种威胁、数据集和模型家族中提供显著的安全改进——所有这些都不影响预期任务的能力。我们鼓励未来的安全研究基于动态塑形原则，以更强的缓解措施应对不断演变的微调风险。我们的代码已公开发布在 https://github.com/poloclub/star-dss。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical safety risks associated with finetuning large language models (LLMs), where even a few harmful examples can undermine safety alignment. Previous methods typically employed static safety shaping, which inadequately handled the nuanced safety context within individual examples by treating harmful and harmless content equally. The proposed dynamic safety shaping (DSS) framework improves upon this by utilizing fine-grained safety signals to enhance learning from safe segments while suppressing unsafe content. This approach is motivated by the insight that guardrail models can be repurposed to evaluate partial responses, leading to the development of the Safety Trajectory Assessment of Response (STAR) for dynamic shaping. The STAR-DSS method demonstrates significant safety improvements across various threats and datasets without sacrificing performance on intended tasks, thereby contributing to more robust safety measures in LLM finetuning.</div>
<div class="mono" style="margin-top:8px">本研究针对微调大型语言模型（LLMs）所带来的关键安全风险，尤其是有害示例可能破坏安全对齐的问题。以往的方法通常采用静态安全塑形，这种方法未能妥善处理单个响应中细微的安全上下文，因为它对有害和无害部分的处理相同。提出的动态安全塑形（DSS）框架通过利用细粒度的安全信号来改善这一点，增强对安全部分的学习，同时抑制不安全内容，利用护栏模型评估响应中的安全风险。本文的贡献在于引入了响应安全轨迹评估（STAR），为训练过程中的动态塑形提供了基于标记的信号。STAR-DSS方法在多种威胁和数据集上展示了显著的安全改进，同时不牺牲模型在预期任务上的性能，支持了在LLM微调过程中增强安全性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety   Evaluation via Role-Specialized Collaboration</div>
<div class="meta-line">Authors: Xiuyuan Chen, Jian Zhao, Yuchen Yuan, Tianle Zhang, Huilin Zhou, Zheng Zhu, Ping Hu, Linghe Kong, Chi Zhang, Weiran Huang, Xuelong Li</div>
<div class="meta-line">First: 2025-09-28T09:35:32+00:00 · Latest: 2025-10-23T03:33:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.25271v4">Abs</a> · <a href="http://arxiv.org/pdf/2509.25271v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing safety evaluation methods for large language models (LLMs) suffer
from inherent limitations, including evaluator bias and detection failures
arising from model homogeneity, which collectively undermine the robustness of
risk evaluation processes. This paper seeks to re-examine the risk evaluation
paradigm by introducing a theoretical framework that reconstructs the
underlying risk concept space. Specifically, we decompose the latent risk
concept space into three mutually exclusive subspaces: the explicit risk
subspace (encompassing direct violations of safety guidelines), the implicit
risk subspace (capturing potential malicious content that requires contextual
reasoning for identification), and the non-risk subspace. Furthermore, we
propose RADAR, a multi-agent collaborative evaluation framework that leverages
multi-round debate mechanisms through four specialized complementary roles and
employs dynamic update mechanisms to achieve self-evolution of risk concept
distributions. This approach enables comprehensive coverage of both explicit
and implicit risks while mitigating evaluator bias. To validate the
effectiveness of our framework, we construct an evaluation dataset comprising
800 challenging cases. Extensive experiments on our challenging testset and
public benchmarks demonstrate that RADAR significantly outperforms baseline
evaluation methods across multiple dimensions, including accuracy, stability,
and self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87%
improvement in risk identification accuracy compared to the strongest baseline
evaluation method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RADAR：一种基于角色专门化协作的风险意识动态多智能体框架，用于大型语言模型安全评估</div>
<div class="mono" style="margin-top:8px">现有的大型语言模型（LLM）安全评估方法存在固有局限性，包括评估者偏见和由于模型同质性导致的检测失败，这些问题共同削弱了风险评估过程的稳健性。本文旨在通过引入一个理论框架重新审视风险评估范式，该框架重构了潜在风险概念空间。具体而言，我们将潜在风险概念空间分解为三个相互排斥的子空间：显性风险子空间（包括对安全指南的直接违反）、隐性风险子空间（捕捉需要上下文推理才能识别的潜在恶意内容）和非风险子空间。此外，我们提出了RADAR，一个多智能体协作评估框架，利用四个专门互补角色的多轮辩论机制，并采用动态更新机制实现风险概念分布的自我演化。这种方法能够全面覆盖显性和隐性风险，同时减轻评估者偏见。为了验证我们框架的有效性，我们构建了一个包含800个挑战性案例的评估数据集。在我们的挑战性测试集和公共基准上的广泛实验表明，RADAR在多个维度上显著优于基线评估方法，包括准确性、稳定性和自我评估风险敏感性。值得注意的是，RADAR在风险识别准确性方面相比最强基线评估方法提高了28.87%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing safety evaluation methods for large language models (LLMs), which are often hindered by evaluator bias and detection failures due to model homogeneity. Previous methods have struggled to effectively identify both explicit and implicit risks, leading to inadequate risk assessments. The proposed approach, RADAR, introduces a multi-agent collaborative evaluation framework that utilizes a theoretical reconstruction of the risk concept space and incorporates dynamic updates through specialized roles to enhance risk identification. This method is well-motivated as it aims to provide a more robust evaluation process by covering a broader range of risks while reducing bias. The methodology involves a comprehensive evaluation dataset of 800 cases, and experimental results show that RADAR significantly outperforms baseline methods, achieving a 28.87% improvement in risk identification accuracy, thus supporting its goals of enhancing safety evaluation for LLMs.</div>
<div class="mono" style="margin-top:8px">本研究针对现有大型语言模型（LLM）安全评估方法的局限性进行探讨，这些方法常常受到评估者偏见和由于模型同质性导致的检测失败的困扰。以往的方法在有效识别显性和隐性风险方面存在困难，导致风险评估不足。提出的方法RADAR引入了一种多智能体协作评估框架，通过对风险概念空间的理论重构，将其划分为显性风险、隐性风险和非风险子空间。该框架利用专业角色和动态更新来增强评估过程，从而减少偏见并改善风险覆盖。该方法通过800个具有挑战性的案例构建的评估数据集进行了验证，RADAR在风险识别准确性上相比最佳现有方法提高了28.87%，显著提升了性能，支持了其增强LLM安全评估的目标。</div>
</details>
</div>
<div class="card">
<div class="title">NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn   LLM Jailbreaks</div>
<div class="meta-line">Authors: Javad Rafiei Asl, Sidhant Narula, Mohammad Ghasemigol, Eduardo Blanco, Daniel Takabi</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-10-03T18:24:14+00:00 · Latest: 2025-10-21T17:41:58+00:00</div>
<div class="meta-line">Comments: This paper has been accepted in the main conference proceedings of
  the 2025 Conference on Empirical Methods in Natural Language Processing
  (EMNLP 2025). Javad Rafiei Asl and Sidhant Narula are co-first authors</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.03417v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.03417v2">PDF</a> · <a href="https://github.com/inspire-lab/NEXUS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have revolutionized natural language processing
but remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks
that distribute malicious intent across benign exchanges and bypass alignment
mechanisms. Existing approaches often explore the adversarial space poorly,
rely on hand-crafted heuristics, or lack systematic query refinement. We
present NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular
framework for constructing, refining, and executing optimized multi-turn
attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a
harmful intent into a structured semantic network of topics, entities, and
query chains; (2) a feedback-driven Simulator that iteratively refines and
prunes these chains through attacker-victim-judge LLM collaboration using
harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser
that adaptively navigates the refined query space for real-time attacks. This
pipeline uncovers stealthy, high-success adversarial paths across LLMs. On
several closed-source and open-source LLMs, NEXUS increases attack success rate
by 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NEXUS：用于利用多轮LLM越狱中不安全序列的网络探索</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）已彻底改变自然语言处理，但仍然容易受到越狱攻击，尤其是将恶意意图分散在良性交流中的多轮越狱，绕过对齐机制。现有方法通常对对抗空间探索不足，依赖手工设计的启发式方法，或缺乏系统的查询优化。我们提出了NEXUS（用于利用不安全序列的网络探索），这是一个模块化框架，用于构建、优化和执行多轮攻击。NEXUS包括：（1）ThoughtNet，它将有害意图层次化扩展为主题、实体和查询链的结构化语义网络；（2）一个基于反馈的模拟器，通过攻击者-受害者-评判者LLM协作，使用有害性和语义相似性基准迭代优化和修剪这些链；（3）一个网络遍历器，适应性地导航优化后的查询空间以进行实时攻击。该流程揭示了LLM之间隐秘且高成功率的对抗路径。在多个闭源和开源LLM上，NEXUS的攻击成功率比之前的方法提高了2.1%到19.4%。代码：https://github.com/inspire-lab/NEXUS</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of Large Language Models (LLMs) to jailbreak attacks, particularly multi-turn jailbreaks that cleverly distribute malicious intent across seemingly benign exchanges, thereby circumventing existing alignment mechanisms. Previous methods have struggled with inadequate exploration of the adversarial space, reliance on hand-crafted heuristics, and lack of systematic query refinement, which NEXUS aims to improve upon. The proposed NEXUS framework introduces a modular approach that includes ThoughtNet for hierarchical expansion of harmful intents, a feedback-driven Simulator for iterative refinement, and a Network Traverser for adaptive navigation of query spaces. This methodology has demonstrated a significant increase in attack success rates, achieving improvements of 2.1% to 19.4% over existing techniques across various LLMs, thereby supporting the goal of enhancing the effectiveness of multi-turn jailbreak attacks.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLM）在监狱攻击中的脆弱性，特别是将恶意意图隐藏在良性交互中的多轮攻击。以往的方法在对抗空间的探索不足、依赖手工启发式方法以及缺乏系统的查询优化方面存在问题，而NEXUS旨在克服这些问题。所提出的方法具有良好的动机，提供了一个模块化框架，包括ThoughtNet用于有害意图的分层扩展、反馈驱动的模拟器用于迭代优化，以及网络遍历器用于自适应查询空间导航。该方法显著提高了多轮攻击的有效性，在不同的LLM上攻击成功率比现有技术提高了2.1%到19.4%，从而支持了改善LLM对抗探索的目标。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Safety Alignment is Divergence Estimation in Disguise</div>
<div class="meta-line">Authors: Rajdeep Haldar, Ziyi Wang, Qifan Song, Guang Lin, Yue Xing</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-02-02T04:09:42+00:00 · Latest: 2025-10-20T19:47:59+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.00657v3">Abs</a> · <a href="http://arxiv.org/pdf/2502.00657v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a theoretical framework showing that popular LLM alignment
methods, including RLHF and its variants, can be understood as divergence
estimators between aligned (safe or preferred) and unaligned (harmful or less
preferred) distributions. This perspective explains the emergence of separation
in the latent space between safe and harmful prompts after alignment. As an
application of our general divergence framework, we propose KLDO, a novel KL
divergence-based alignment method, and empirically validate its effectiveness.
We further show that using compliance-refusal datasets, rather than standard
preference-based datasets, leads to stronger separation and improved safety
alignment. Finally, to quantify the separation effect, we propose a
distance-based metric in the prompt representation space, which also acts as a
statistically significant indicator for model safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM安全对齐是伪装的发散估计</div>
<div class="mono" style="margin-top:8px">我们提出了一个理论框架，表明流行的LLM对齐方法，包括RLHF及其变体，可以理解为对齐（安全或优选）和未对齐（有害或不太优选）分布之间的发散估计器。这一视角解释了对齐后安全和有害提示在潜在空间中出现分离的现象。作为我们一般发散框架的应用，我们提出了KLDO，一种基于KL发散的新型对齐方法，并实证验证了其有效性。我们进一步表明，使用合规-拒绝数据集而非标准偏好数据集，能够导致更强的分离和改善的安全对齐。最后，为了量化分离效果，我们提出了一种基于距离的度量，适用于提示表示空间，这也作为模型安全的统计显著指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing large language model (LLM) alignment methods, such as Reinforcement Learning from Human Feedback (RLHF), which often struggle with effectively distinguishing between safe and harmful outputs. The proposed approach, KLDO, reframes these alignment methods as divergence estimators, providing a theoretical foundation that clarifies the observed separation in latent space between aligned and unaligned prompts. This framework is well-motivated as it enhances understanding of model behavior and safety. The methodology involves using KL divergence for alignment and emphasizes the use of compliance-refusal datasets over traditional preference-based datasets, resulting in improved separation and safety alignment. The experiments demonstrate that this new method achieves stronger performance in safety alignment tasks, supporting the goal of enhancing model safety through better prompt representation metrics.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLM）对齐以确保其安全性和有效性的问题，强调现有方法如基于人类反馈的强化学习（RLHF）往往无法充分区分安全输出与有害输出。提出的方法KLDO将这些对齐方法重新解释为散度估计器，这为理解其功能提供了更清晰的视角，并通过使用合规-拒绝数据集而非标准偏好数据集来改进先前的方法。该新方法展示了对齐和未对齐分布之间的更强分离，促进了LLM的安全对齐。该方法论涉及散度估计的理论框架和KLDO有效性的实证验证，在安全对齐任务中取得了更好的性能，从而支持了增强模型安全性的研究目标。</div>
</details>
</div>
<div class="card">
<div class="title">CultureGuard: Towards Culturally-Aware Dataset and Guard Model for   Multilingual Safety Applications</div>
<div class="meta-line">Authors: Raviraj Joshi, Rakesh Paul, Kanishk Singla, Anusha Kamath, Michael Evans, Katherine Luna, Shaona Ghosh, Utkarsh Vaidya, Eileen Long, Sanjay Singh Chauhan, Niranjan Wartikar</div>
<div class="meta-line">First: 2025-08-03T10:35:05+00:00 · Latest: 2025-10-19T16:44:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.01710v3">Abs</a> · <a href="http://arxiv.org/pdf/2508.01710v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing use of Large Language Models (LLMs) in agentic applications
highlights the need for robust safety guard models. While content safety in
English is well-studied, non-English languages lack similar advancements due to
the high cost of collecting culturally aligned labeled datasets. We present
CultureGuard, a novel solution for curating culturally aligned, high-quality
safety datasets across multiple languages. Our approach introduces a four-stage
synthetic data generation and filtering pipeline: cultural data segregation,
cultural data adaptation, machine translation, and quality filtering. This
pipeline enables the conversion and expansion of the
Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct
languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.
The resulting dataset, Nemotron-Safety-Guard-Dataset-v3, comprises 386,661
samples in 9 languages and facilitates the training of
Llama-3.1-Nemotron-Safety-Guard-8B-v3 via LoRA-based fine-tuning. The final
model achieves state-of-the-art performance on several multilingual content
safety benchmarks. Furthermore, we show our moderately multilingual fine-tuning
enables robust cross-lingual transfer and strong zero-shot generalization to
unseen languages. We also benchmark the latest open LLMs on multilingual safety
and observe that these LLMs are more prone to give unsafe responses when
prompted in non-English languages. This work advances multilingual LLM safety
by enabling the development of culturally aware safety guard models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CultureGuard：面向多语言安全应用的文化意识数据集和保护模型</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在代理应用中的日益使用突显了对强大安全保护模型的需求。虽然英语内容安全研究较为充分，但由于收集文化对齐标注数据集的高成本，非英语语言缺乏类似的进展。我们提出了CultureGuard，这是一种新颖的解决方案，用于策划跨多种语言的文化对齐高质量安全数据集。我们的方法引入了一个四阶段的合成数据生成和过滤管道：文化数据分离、文化数据适应、机器翻译和质量过滤。该管道使得将Nemotron-Content-Safety-Dataset-V2英语安全数据集转换和扩展为八种不同语言成为可能：阿拉伯语、德语、西班牙语、法语、印地语、日语、泰语和中文。最终生成的数据集Nemotron-Safety-Guard-Dataset-v3包含9种语言的386,661个样本，并通过基于LoRA的微调促进了Llama-3.1-Nemotron-Safety-Guard-8B-v3的训练。最终模型在多个多语言内容安全基准上实现了最先进的性能。此外，我们展示了我们的适度多语言微调能够实现强大的跨语言迁移和对未见语言的强大零样本泛化。我们还对最新的开放LLMs在多语言安全方面进行了基准测试，观察到这些LLMs在非英语语言提示时更容易给出不安全的响应。这项工作通过促进文化意识安全保护模型的发展，推动了多语言LLM的安全性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing need for effective safety guard models in multilingual applications of Large Language Models (LLMs), particularly highlighting the lack of culturally aligned datasets for non-English languages. Previous methods have struggled due to the high costs associated with collecting labeled data that reflects cultural nuances, leading to inadequate safety measures in these languages. The proposed CultureGuard approach overcomes these challenges by implementing a four-stage synthetic data generation and filtering pipeline, which includes cultural data segregation, adaptation, machine translation, and quality filtering. This methodology results in the creation of the Nemotron-Safety-Guard-Dataset-v3, a comprehensive dataset containing 386,661 samples across nine languages, which supports the training of a fine-tuned model achieving state-of-the-art performance on multilingual content safety benchmarks. The findings demonstrate that the model not only excels in multilingual safety tasks but also exhibits strong cross-lingual transfer capabilities and zero-shot generalization to unseen languages, thereby significantly advancing the field of multilingual LLM safety.</div>
<div class="mono" style="margin-top:8px">本研究解决了多语言应用中安全防护模型的不足，特别强调了非英语语言缺乏文化对齐数据集的问题。以往的方法由于收集此类数据集的高成本而面临挑战，导致这些语言的安全措施不足。提出的CultureGuard方法引入了一个四阶段的合成数据生成和过滤管道，包括文化数据隔离、适应、机器翻译和质量过滤，有效地将现有的英语数据集扩展到八种语言。该方法论生成了Nemotron-Safety-Guard-Dataset-v3，包含386,661个样本，并支持训练经过微调的模型，该模型在多语言内容安全基准测试中实现了最先进的性能。研究结果表明，该模型不仅在已知语言中表现出色，还展现出强大的跨语言迁移和零样本泛化能力，从而显著提升了多语言LLM的安全性。</div>
</details>
</div>
<div class="card">
<div class="title">Black-box Optimization of LLM Outputs by Asking for Directions</div>
<div class="meta-line">Authors: Jie Zhang, Meng Ding, Yang Liu, Jue Hong, Florian Tramèr</div>
<div class="meta-line">First: 2025-10-19T11:13:45+00:00 · Latest: 2025-10-19T11:13:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.16794v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.16794v1">PDF</a> · <a href="https://github.com/zj-jayzhang/black_box_llm_optimization">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel approach for attacking black-box large language models
(LLMs) by exploiting their ability to express confidence in natural language.
Existing black-box attacks require either access to continuous model outputs
like logits or confidence scores (which are rarely available in practice), or
rely on proxy signals from other models. Instead, we demonstrate how to prompt
LLMs to express their internal confidence in a way that is sufficiently
calibrated to enable effective adversarial optimization. We apply our general
method to three attack scenarios: adversarial examples for vision-LLMs,
jailbreaks and prompt injections. Our attacks successfully generate malicious
inputs against systems that only expose textual outputs, thereby dramatically
expanding the attack surface for deployed LLMs. We further find that better and
larger models exhibit superior calibration when expressing confidence, creating
a concerning security paradox where model capability improvements directly
enhance vulnerability. Our code is available at this
[link](https://github.com/zj-jayzhang/black_box_llm_optimization).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过询问方向对LLM输出进行黑箱优化</div>
<div class="mono" style="margin-top:8px">我们提出了一种新颖的方法，通过利用黑箱大型语言模型（LLMs）在自然语言中表达信心的能力来进行攻击。现有的黑箱攻击要么需要访问连续的模型输出，如logits或置信度分数（在实践中很少可用），要么依赖于其他模型的代理信号。相反，我们展示了如何提示LLMs以一种足够校准的方式表达其内部信心，从而实现有效的对抗优化。我们将我们的一般方法应用于三种攻击场景：视觉LLMs的对抗样本、越狱和提示注入。我们的攻击成功生成了针对仅暴露文本输出的系统的恶意输入，从而显著扩大了已部署LLMs的攻击面。我们进一步发现，更好和更大的模型在表达信心时表现出更优的校准，形成了一个令人担忧的安全悖论，即模型能力的提升直接增强了脆弱性。我们的代码可在此[链接](https://github.com/zj-jayzhang/black_box_llm_optimization)获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the challenge of black-box attacks on large language models (LLMs), which traditionally require access to model outputs like logits or confidence scores that are often unavailable. Previous methods either depend on these inaccessible outputs or utilize proxy signals from other models, limiting their effectiveness. The proposed approach innovatively prompts LLMs to express their internal confidence in a calibrated manner, facilitating adversarial optimization without needing direct access to model internals. The paper contributes to the field by demonstrating this method across three attack scenarios: adversarial examples for vision-LLMs, jailbreaks, and prompt injections, successfully generating malicious inputs against systems that only provide textual outputs. The findings indicate that larger and better models show improved calibration in expressing confidence, which paradoxically increases their vulnerability, thus highlighting a significant security concern in model development.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有黑箱攻击大型语言模型（LLM）的局限性，这些攻击通常需要访问连续的模型输出或依赖其他模型的代理信号，这在许多情况下都不切实际。提出的方法创新性地提示LLM以校准的方式表达其内部信心，从而在不需要直接访问模型内部的情况下实现有效的对抗优化。这种方法通过成功生成视觉-LLM的对抗示例、越狱和提示注入，显著扩大了已部署LLM的攻击面，突显出一个安全悖论，即模型能力的提升无意中增加了脆弱性。该方法论涉及利用LLM表达的信心来创建恶意输入，表明更大和更好的模型在校准方面表现优越，这可以在攻击场景中被利用，最终在这些任务中实现有效的对抗结果。</div>
</details>
</div>
<div class="card">
<div class="title">Active Honeypot Guardrail System: Probing and Confirming Multi-Turn LLM   Jailbreaks</div>
<div class="meta-line">Authors: ChenYu Wu, Yi Wang, Yang Liao</div>
<div class="meta-line">First: 2025-10-16T17:41:09+00:00 · Latest: 2025-10-16T17:41:09+00:00</div>
<div class="meta-line">Comments: 6pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15017v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.15017v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly vulnerable to multi-turn
jailbreak attacks, where adversaries iteratively elicit harmful behaviors that
bypass single-turn safety filters. Existing defenses predominantly rely on
passive rejection, which either fails against adaptive attackers or overly
restricts benign users. We propose a honeypot-based proactive guardrail system
that transforms risk avoidance into risk utilization. Our framework fine-tunes
a bait model to generate ambiguous, non-actionable but semantically relevant
responses, which serve as lures to probe user intent. Combined with the
protected LLM&#x27;s safe reply, the system inserts proactive bait questions that
gradually expose malicious intent through multi-turn interactions. We further
introduce the Honeypot Utility Score (HUS), measuring both the attractiveness
and feasibility of bait responses, and use a Defense Efficacy Rate (DER) for
balancing safety and usability. Initial experiment on MHJ Datasets with recent
attack method across GPT-4o show that our system significantly disrupts
jailbreak success while preserving benign user experience.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>主动蜜罐护栏系统：探测和确认多轮LLM越狱</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）越来越容易受到多轮越狱攻击，攻击者通过迭代引发绕过单轮安全过滤器的有害行为。现有防御主要依赖于被动拒绝，这要么对自适应攻击者无效，要么过度限制良性用户。我们提出了一种基于蜜罐的主动护栏系统，将风险规避转变为风险利用。我们的框架微调了一个诱饵模型，以生成模糊、不可操作但语义相关的响应，作为探测用户意图的诱饵。结合受保护的LLM的安全回复，该系统插入主动诱饵问题，通过多轮交互逐渐暴露恶意意图。我们进一步引入蜜罐效用评分（HUS），衡量诱饵响应的吸引力和可行性，并使用防御有效率（DER）来平衡安全性和可用性。在MHJ数据集上进行的初步实验显示，我们的系统显著干扰了越狱成功，同时保持了良性用户体验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the growing vulnerability of large language models (LLMs) to multi-turn jailbreak attacks, where adversaries exploit iterative interactions to bypass safety measures. Previous methods primarily focused on passive rejection, which either inadequately defends against adaptive attackers or imposes excessive restrictions on legitimate users. The proposed honeypot-based proactive guardrail system shifts from risk avoidance to risk utilization by fine-tuning a bait model that generates ambiguous yet relevant responses to probe user intent. This method introduces the Honeypot Utility Score (HUS) to evaluate bait responses and the Defense Efficacy Rate (DER) to balance safety and usability. Experimental results on the MHJ Datasets demonstrate that this approach significantly reduces jailbreak success rates while maintaining a positive experience for benign users.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLM）在多轮越狱攻击中日益脆弱的问题，这类攻击利用现有防御措施的局限性，主要依赖被动拒绝，往往无法应对自适应攻击者或过度限制良性用户。提出的基于蜜罐的主动护栏系统与传统方法不同，通过将风险规避转变为风险利用，利用诱饵模型生成模糊但相关的响应，以探测用户意图，同时保持安全性。本文的贡献在于提出了一种新颖的框架，包括蜜罐效用评分（HUS）用于评估诱饵响应，以及防御有效性比率（DER）用于平衡安全性和可用性。该方法论涉及对诱饵模型的微调，并将其与受保护的LLM集成，以插入主动诱饵问题，成功干扰越狱尝试，同时保持良性用户的体验，实验结果显示在MHJ数据集和GPT-4o上防御效果显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">When Style Breaks Safety: Defending LLMs Against Superficial Style   Alignment</div>
<div class="meta-line">Authors: Yuxin Xiao, Sana Tonekaboni, Walter Gerych, Vinith Suriyakumar, Marzyeh Ghassemi</div>
<div class="meta-line">First: 2025-06-09T05:57:39+00:00 · Latest: 2025-10-16T06:50:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.07452v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.07452v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) can be prompted with specific styles (e.g.,
formatting responses as lists), including in malicious queries. Prior jailbreak
research mainly augments these queries with additional string transformations
to maximize attack success rate (ASR). However, the impact of style patterns in
the original queries that are semantically irrelevant to the malicious intent
remains unclear. In this work, we seek to understand whether style patterns
compromise LLM safety, how superficial style alignment increases model
vulnerability, and how best to mitigate these risks during alignment. We first
define ASR inflation as the increase in ASR due to style patterns in existing
jailbreak benchmark queries. By evaluating 32 LLMs across seven benchmarks, we
find that nearly all models exhibit ASR inflation. Notably, the inflation
correlates with an LLM&#x27;s relative attention to style patterns, which also
overlap more with its instruction-tuning data when inflation occurs. We then
investigate superficial style alignment, and find that fine-tuning with
specific styles makes LLMs more vulnerable to jailbreaks of those same styles.
Finally, we propose SafeStyle, a defense strategy that incorporates a small
amount of safety training data augmented to match the distribution of style
patterns in the fine-tuning data. Across three LLMs, six fine-tuning style
settings, and two real-world instruction-tuning datasets, SafeStyle
consistently outperforms baselines in maintaining LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当风格破坏安全性：保护大型语言模型免受表面风格对齐的影响</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）可以通过特定风格（例如，将响应格式化为列表）进行提示，包括在恶意查询中。之前的越狱研究主要通过额外的字符串转换来增强这些查询，以最大化攻击成功率（ASR）。然而，原始查询中与恶意意图语义无关的风格模式的影响仍不清楚。在本研究中，我们旨在了解风格模式是否会危害LLM安全性，表面风格对齐如何增加模型脆弱性，以及在对齐过程中如何最好地减轻这些风险。我们首先将ASR膨胀定义为由于现有越狱基准查询中的风格模式而导致的ASR增加。通过评估32个LLM在七个基准上的表现，我们发现几乎所有模型都表现出ASR膨胀。值得注意的是，膨胀与LLM对风格模式的相对关注度相关，当膨胀发生时，这些模式与其指令调优数据的重叠也更多。然后，我们研究表面风格对齐，发现使用特定风格进行微调使LLM对这些相同风格的越狱更加脆弱。最后，我们提出了SafeStyle，一种防御策略，结合少量安全训练数据，增强以匹配微调数据中的风格模式分布。在三个LLM、六个微调风格设置和两个真实世界的指令调优数据集上，SafeStyle在维护LLM安全性方面始终优于基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the vulnerability of large language models (LLMs) to malicious queries that exploit specific style patterns, which has not been thoroughly investigated in prior jailbreak studies that primarily focused on augmenting queries with string transformations. The existing methods have shown limitations in understanding the role of style patterns in compromising LLM safety, leading to the introduction of ASR inflation, which is the increase in attack success rate due to these patterns. The paper contributes by defining this phenomenon and demonstrating that nearly all evaluated LLMs exhibit ASR inflation, which correlates with their attention to style patterns. The proposed methodology, SafeStyle, involves fine-tuning LLMs with a small amount of safety training data that aligns with the style patterns in the fine-tuning data. The results indicate that SafeStyle significantly enhances LLM safety across various models and fine-tuning settings, effectively addressing the identified vulnerabilities.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在恶意查询中利用特定风格提示的脆弱性，这是之前的越狱研究未充分探讨的问题，后者主要集中在通过字符串变换来提高攻击成功率（ASR）。现有方法未考虑与恶意意图语义无关的风格模式的影响，导致对这些模式如何危害LLM安全的理解不清晰。作者引入了ASR膨胀的概念，表明几乎所有评估的LLM都因风格模式而表现出ASR的增加，这与它们对这些模式的关注程度相关。研究还发现，使用特定风格对LLM进行微调会增加其对这些风格越狱的脆弱性。为了解决这些风险，本文提出了SafeStyle，一种利用与微调数据中风格模式分布相匹配的小量安全训练数据的防御策略。SafeStyle在三种LLM、六种微调风格设置和两个真实世界的指令微调数据集上表现出一致的性能提升，有效增强了LLM的安全性。</div>
</details>
</div>
<div class="card">
<div class="title">When &quot;Competency&quot; in Reasoning Opens the Door to Vulnerability:   Jailbreaking LLMs via Novel Complex Ciphers</div>
<div class="meta-line">Authors: Divij Handa, Zehua Zhang, Amir Saeidi, Shrinidhi Kumbhar, Md Nayem Uddin, Aswin RRV, Chitta Baral</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2024-02-16T11:37:05+00:00 · Latest: 2025-10-14T06:25:21+00:00</div>
<div class="meta-line">Comments: Published in Reliable ML from Unreliable Data workshop @ NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2402.10601v5">Abs</a> · <a href="http://arxiv.org/pdf/2402.10601v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Language Model (LLM) safety have primarily
focused on mitigating attacks crafted in natural language or common ciphers
(e.g. Base64), which are likely integrated into newer models&#x27; safety training.
However, we reveal a paradoxical vulnerability: as LLMs advance in reasoning,
they inadvertently become more susceptible to novel jailbreaking attacks.
Enhanced reasoning enables LLMs to interpret complex instructions and decode
complex user-defined ciphers, creating an exploitable security gap. To study
this vulnerability, we introduce Attacks using Custom Encryptions (ACE), a
jailbreaking technique that encodes malicious queries with novel ciphers.
Extending ACE, we introduce Layered Attacks using Custom Encryptions (LACE),
which applies multi-layer ciphers to amplify attack complexity. Furthermore, we
develop CipherBench, a benchmark designed to evaluate LLMs&#x27; accuracy in
decoding encrypted benign text. Our experiments reveal a critical trade-off:
LLMs that are more capable of decoding ciphers are more vulnerable to LACE,
with success rates on gpt-oss-20b escalating from 60% under ACE to 72% with
LACE. These findings highlight a critical insight: as LLMs become more adept at
deciphering complex user ciphers--many of which cannot be preemptively included
in safety training--they become increasingly exploitable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当推理中的“能力”打开脆弱性之门：通过新型复杂密码破解大型语言模型</div>
<div class="mono" style="margin-top:8px">近期大型语言模型（LLM）安全性的进展主要集中在减轻自然语言或常见密码（如Base64）构造的攻击，这些攻击可能已被纳入新模型的安全训练中。然而，我们揭示了一种矛盾的脆弱性：随着LLM在推理方面的进步，它们无意中变得更容易受到新型越狱攻击的影响。增强的推理能力使LLM能够解释复杂指令并解码复杂的用户定义密码，从而产生可利用的安全漏洞。为了研究这种脆弱性，我们引入了使用自定义加密的攻击（ACE），这是一种通过新型密码编码恶意查询的越狱技术。扩展ACE，我们引入了使用自定义加密的分层攻击（LACE），它应用多层密码以增强攻击复杂性。此外，我们开发了CipherBench，这是一个旨在评估LLM解码加密良性文本准确性的基准测试。我们的实验揭示了一个关键的权衡：能够解码密码的LLM在LACE下更脆弱，gpt-oss-20b的成功率从ACE下的60%上升到LACE下的72%。这些发现突显了一个关键的见解：随着LLM在解码复杂用户密码方面变得更加熟练——其中许多无法在安全训练中预先包含——它们变得越来越容易被利用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the growing concern of vulnerabilities in Large Language Models (LLMs) as they become more advanced in reasoning capabilities. Previous methods focused on defending against attacks using natural language or common ciphers, which are now integrated into safety training for newer models. The proposed approach, Attacks using Custom Encryptions (ACE), introduces a novel jailbreaking technique that utilizes complex user-defined ciphers, revealing a paradox where enhanced reasoning leads to increased susceptibility to these attacks. The paper contributes by presenting Layered Attacks using Custom Encryptions (LACE) to amplify attack complexity and introduces CipherBench, a benchmark for evaluating LLMs&#x27; decoding accuracy. Experimental results show that LLMs&#x27; success rates in decoding ciphers increase from 60% with ACE to 72% with LACE, indicating a critical trade-off between reasoning capability and vulnerability, thus supporting the need for improved safety measures in LLMs.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在推理能力日益增强的背景下，安全性脆弱性日益严重的问题。以往的方法主要集中在防御自然语言或常见密码的攻击，这些方法如今已被纳入新模型的安全训练中。本文提出的攻击方法——自定义加密攻击（ACE），引入了一种利用复杂用户定义密码的越狱技术，揭示了一个悖论：推理能力的增强导致了对这些攻击的更高易受攻击性。论文的贡献在于开发了分层自定义加密攻击（LACE），进一步复杂化攻击，并创建了CipherBench，一个用于评估LLMs解码加密文本性能的基准。实验结果显示，gpt-oss-20b模型在LACE攻击下的成功率从60%提高到72%，表明随着LLMs解码能力的提升，它们同时变得更加容易受到利用。</div>
</details>
</div>
<div class="card">
<div class="title">Attention-Aware GNN-based Input Defense against Multi-Turn LLM Jailbreak</div>
<div class="meta-line">Authors: Zixuan Huang, Kecheng Huang, Lihao Yin, Bowei He, Huiling Zhen, Mingxuan Yuan, Zili Shao</div>
<div class="meta-line">First: 2025-07-09T07:55:03+00:00 · Latest: 2025-10-14T04:28:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.07146v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.07146v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have gained significant traction in various
applications, yet their capabilities present risks for both constructive and
malicious exploitation. Despite extensive training and fine-tuning efforts
aimed at enhancing safety, LLMs remain susceptible to jailbreak attacks.
Recently, the emergence of multi-turn attacks has intensified this
vulnerability. Unlike single-turn attacks, multi-turn attacks incrementally
escalate dialogue complexity, rendering them more challenging to detect and
mitigate.
  In this study, we introduce G-Guard, an innovative attention-aware Graph
Neural Network (GNN)-based input classifier specifically designed to defend
against multi-turn jailbreak attacks targeting LLMs. G-Guard constructs an
entity graph for multi-turn queries, which captures the interrelationships
between queries and harmful keywords that present in multi-turn queries.
Furthermore, we propose an attention-aware augmentation mechanism that
retrieves the most relevant single-turn query based on the ongoing multi-turn
conversation. The retrieved query is incorporated as a labeled node within the
graph, thereby enhancing the GNN&#x27;s capacity to classify the current query as
harmful or benign. Evaluation results show that G-Guard consistently
outperforms all baselines across diverse datasets and evaluation metrics,
demonstrating its efficacy as a robust defense mechanism against multi-turn
jailbreak attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于注意力的GNN输入防御多轮LLM越狱攻击</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在各种应用中获得了显著关注，但其能力也带来了建设性和恶意利用的风险。尽管进行了广泛的训练和微调以增强安全性，LLMs仍然容易受到越狱攻击。最近，多轮攻击的出现加剧了这一脆弱性。与单轮攻击不同，多轮攻击逐步增加对话复杂性，使其更难以检测和缓解。在本研究中，我们介绍了G-Guard，一种创新的基于注意力的图神经网络（GNN）输入分类器，专门设计用于防御针对LLMs的多轮越狱攻击。G-Guard为多轮查询构建了一个实体图，捕捉查询与多轮查询中存在的有害关键词之间的相互关系。此外，我们提出了一种基于注意力的增强机制，根据正在进行的多轮对话检索最相关的单轮查询。检索到的查询作为标记节点纳入图中，从而增强GNN将当前查询分类为有害或良性的能力。评估结果表明，G-Guard在各种数据集和评估指标上始终优于所有基线，证明其作为多轮越狱攻击的强大防御机制的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the increasing vulnerability of Large Language Models (LLMs) to jailbreak attacks, particularly the more complex multi-turn attacks that escalate dialogue and are harder to detect. Previous methods have struggled to effectively mitigate these attacks due to their inability to capture the evolving context of conversations. The proposed approach, G-Guard, utilizes an attention-aware Graph Neural Network (GNN) that constructs an entity graph to represent the relationships between queries and harmful keywords, along with an attention-aware augmentation mechanism to enhance classification accuracy. This method is well-motivated as it directly targets the limitations of existing defenses. The paper contributes a novel defense mechanism that significantly improves classification performance against multi-turn jailbreak attacks, as evidenced by superior results across various datasets and metrics, supporting its effectiveness in safeguarding LLMs.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）面临的越狱攻击日益严重的问题，特别是更复杂的多轮攻击，这些攻击在检测上带来了重大挑战。以往的方法在有效减轻这些攻击方面存在困难，因为它们无法捕捉对话的演变上下文。所提出的方法G-Guard利用注意力感知的图神经网络（GNN）构建一个实体图，反映查询与有害关键词之间的关系，从而增强检测能力。该方法的动机明确，直接针对现有防御的局限性，通过将相关的单轮查询纳入图结构。研究表明，G-Guard在各种数据集和评估指标上显著优于现有基线，证实了其在防御多轮越狱攻击方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses   Against Llm Jailbreaks and Prompt Injections</div>
<div class="meta-line">Authors: Milad Nasr, Nicholas Carlini, Chawin Sitawarin, Sander V. Schulhoff, Jamie Hayes, Michael Ilie, Juliette Pluto, Shuang Song, Harsh Chaudhari, Ilia Shumailov, Abhradeep Thakurta, Kai Yuanqing Xiao, Andreas Terzis, Florian Tramèr</div>
<div class="meta-line">First: 2025-10-10T05:51:04+00:00 · Latest: 2025-10-10T05:51:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09023v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How should we evaluate the robustness of language model defenses? Current
defenses against jailbreaks and prompt injections (which aim to prevent an
attacker from eliciting harmful knowledge or remotely triggering malicious
actions, respectively) are typically evaluated either against a static set of
harmful attack strings, or against computationally weak optimization methods
that were not designed with the defense in mind. We argue that this evaluation
process is flawed.
  Instead, we should evaluate defenses against adaptive attackers who
explicitly modify their attack strategy to counter a defense&#x27;s design while
spending considerable resources to optimize their objective. By systematically
tuning and scaling general optimization techniques-gradient descent,
reinforcement learning, random search, and human-guided exploration-we bypass
12 recent defenses (based on a diverse set of techniques) with attack success
rate above 90% for most; importantly, the majority of defenses originally
reported near-zero attack success rates. We believe that future defense work
must consider stronger attacks, such as the ones we describe, in order to make
reliable and convincing claims of robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>攻击者第二步行动：更强的自适应攻击绕过针对 LLM 监狱破解和提示注入的防御</div>
<div class="mono" style="margin-top:8px">我们应该如何评估语言模型防御的稳健性？当前针对监狱破解和提示注入的防御（旨在防止攻击者引出有害知识或远程触发恶意行为）通常是针对一组静态的有害攻击字符串，或针对未考虑防御设计的计算弱优化方法进行评估。我们认为这一评估过程存在缺陷。相反，我们应该针对自适应攻击者进行评估，他们明确修改攻击策略以对抗防御设计，同时花费大量资源来优化目标。通过系统地调整和扩展一般优化技术——梯度下降、强化学习、随机搜索和人类引导探索——我们绕过了12个最近的防御（基于多种技术），大多数攻击成功率超过90%；重要的是，大多数防御最初报告的攻击成功率接近零。我们认为，未来的防御工作必须考虑更强的攻击，例如我们所描述的，以便做出可靠和令人信服的稳健性声明。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inadequacies in evaluating the robustness of language model defenses against jailbreaks and prompt injections, which are typically assessed using static harmful attack strings or weak optimization methods. These traditional approaches fail to account for adaptive attackers who can modify their strategies to exploit specific defense mechanisms. The proposed method involves systematically tuning and scaling various optimization techniques, including gradient descent and reinforcement learning, to effectively bypass multiple recent defenses with an attack success rate exceeding 90%. This contribution highlights the necessity for future defense evaluations to consider stronger adaptive attacks to substantiate claims of robustness. The methodology demonstrates significant effectiveness in overcoming defenses that previously reported near-zero success rates against attacks.</div>
<div class="mono" style="margin-top:8px">本文探讨了评估语言模型防御措施在抵御越狱和提示注入方面的不足，这对防止有害行为至关重要。以往的方法依赖于静态的有害攻击字符串或较弱的优化技术，未能考虑能够修改策略的自适应攻击者。所提出的方法强调在自适应攻击者面前评估防御，利用梯度下降和强化学习等先进的优化技术。研究表明，通过采用这些方法，作者成功绕过了12种最新的防御措施，攻击成功率超过90%，突显了未来防御策略需要考虑更强攻击方法，以确保真正的鲁棒性声明。</div>
</details>
</div>
<div class="card">
<div class="title">Machine Learning for Detection and Analysis of Novel LLM Jailbreaks</div>
<div class="meta-line">Authors: John Hawkins, Aditya Pramar, Rodney Beard, Rohitash Chandra</div>
<div class="meta-line">First: 2025-10-02T03:55:29+00:00 · Latest: 2025-10-10T05:08:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.01644v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.01644v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) suffer from a range of vulnerabilities that
allow malicious users to solicit undesirable responses through manipulation of
the input text. These so-called jailbreak prompts are designed to trick the LLM
into circumventing the safety guardrails put in place to keep responses
acceptable to the developer&#x27;s policies. In this study, we analyse the ability
of different machine learning models to distinguish jailbreak prompts from
genuine uses, including looking at our ability to identify jailbreaks that use
previously unseen strategies. Our results indicate that using current datasets
the best performance is achieved by fine tuning a Bidirectional Encoder
Representations from Transformers (BERT) model end-to-end for identifying
jailbreaks. We visualise the keywords that distinguish jailbreak from genuine
prompts and conclude that explicit reflexivity in prompt structure could be a
signal of jailbreak intention.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于检测和分析新型LLM越狱的机器学习</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）存在一系列漏洞，允许恶意用户通过操纵输入文本来获取不良响应。这些所谓的越狱提示旨在欺骗LLM绕过为保持响应符合开发者政策而设立的安全防护措施。在本研究中，我们分析了不同机器学习模型区分越狱提示与真实使用的能力，包括识别使用以前未见策略的越狱。我们的结果表明，使用当前数据集，通过端到端微调双向编码器表示的变换器（BERT）模型来识别越狱的最佳性能。我们可视化了区分越狱与真实提示的关键词，并得出结论，提示结构中的显式反身性可能是越狱意图的信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of Large Language Models (LLMs) that can be exploited by malicious users through specially crafted input text known as jailbreak prompts, which bypass safety measures. Previous methods struggled to effectively identify these prompts, particularly those employing novel strategies, highlighting a gap in the ability to distinguish between genuine and malicious uses. This study proposes a fine-tuned Bidirectional Encoder Representations from Transformers (BERT) model, which improves upon existing techniques by enhancing the detection of jailbreaks through end-to-end training. The contribution of this paper lies in its analysis of various machine learning models for this task, revealing that the BERT model achieves the best performance on current datasets. The findings indicate that the model can successfully identify jailbreak prompts, supporting the goal of improving LLM safety mechanisms.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLM）的脆弱性，这些脆弱性可以被恶意用户通过精心设计的输入提示（称为越狱提示）利用，从而绕过安全措施。以往的方法在有效识别这些越狱提示方面存在困难，尤其是那些采用新策略的提示，因此需要改进的检测技术。该研究提出了一种方法，专门针对区分越狱提示和合法使用的任务，对双向编码器表示的变换器（BERT）模型进行微调，从而增强检测能力。该方法论涉及分析各种机器学习模型，并可视化区分提示类型的关键词。结果表明，微调后的BERT模型在该任务上取得了优越的性能，支持了更强大的LLM安全机制的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through   Formal Logical Expression</div>
<div class="meta-line">Authors: Jingyu Peng, Maolin Wang, Nan Wang, Jiatong Li, Yuchen Li, Yuyang Ye, Wanyu Wang, Pengyue Jia, Kai Zhang, Xiangyu Zhao</div>
<div class="meta-line">First: 2025-05-18T04:23:51+00:00 · Latest: 2025-10-09T06:29:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.13527v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.13527v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite substantial advancements in aligning large language models (LLMs)
with human values, current safety mechanisms remain susceptible to jailbreak
attacks. We hypothesize that this vulnerability stems from distributional
discrepancies between alignment-oriented prompts and malicious prompts. To
investigate this, we introduce LogiBreak, a novel and universal black-box
jailbreak method that leverages logical expression translation to circumvent
LLM safety systems. By converting harmful natural language prompts into formal
logical expressions, LogiBreak exploits the distributional gap between
alignment data and logic-based inputs, preserving the underlying semantic
intent and readability while evading safety constraints. We evaluate LogiBreak
on a multilingual jailbreak dataset spanning three languages, demonstrating its
effectiveness across various evaluation settings and linguistic contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>逻辑越狱：通过形式逻辑表达高效解锁大型语言模型安全限制</div>
<div class="mono" style="margin-top:8px">尽管在将大型语言模型（LLMs）与人类价值观对齐方面取得了重大进展，但当前的安全机制仍然容易受到越狱攻击。我们假设这种脆弱性源于对齐导向提示与恶意提示之间的分布差异。为此，我们引入了LogiBreak，这是一种新颖且通用的黑箱越狱方法，利用逻辑表达翻译来规避LLM安全系统。通过将有害的自然语言提示转换为形式逻辑表达，LogiBreak利用对齐数据与基于逻辑的输入之间的分布差距，保留潜在的语义意图和可读性，同时规避安全约束。我们在涵盖三种语言的多语言越狱数据集上评估LogiBreak，展示了其在各种评估设置和语言环境中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of large language models (LLMs) to jailbreak attacks despite advancements in aligning these models with human values. Previous methods have struggled with distributional discrepancies between alignment-oriented prompts and malicious prompts, leading to ineffective safety mechanisms. The proposed approach, LogiBreak, introduces a novel black-box jailbreak method that translates harmful natural language prompts into formal logical expressions, effectively exploiting the distributional gap while maintaining semantic intent and readability. This method is well-motivated as it directly targets the identified weaknesses in existing safety systems. The paper contributes by demonstrating LogiBreak&#x27;s effectiveness on a multilingual jailbreak dataset across three languages, achieving significant performance improvements in various evaluation settings and linguistic contexts, thereby supporting the goal of enhancing LLM safety mechanisms.</div>
<div class="mono" style="margin-top:8px">本研究探讨了尽管在将大型语言模型（LLMs）与人类价值观对齐方面取得了重大进展，但它们仍然容易受到越狱攻击的脆弱性。以往的方法在对齐导向的提示与恶意提示之间存在分布差异，导致安全机制效果不佳。提出的方法LogiBreak引入了一种新颖的黑箱越狱方法，通过将有害的自然语言提示转换为形式逻辑表达式，有效利用分布差距，同时保持语义意图和可读性。该方法的提出是合理的，因为它直接针对现有安全系统中识别出的弱点。本文的贡献在于展示LogiBreak在跨三种语言的多语言越狱数据集上的有效性，在各种评估设置中实现了显著的性能提升，从而支持其增强LLM安全性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">$\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM   Systems with Optimized Prompt Attacks</div>
<div class="meta-line">Authors: Rana Muhammad Shahroz Khan, Zhen Tan, Sukwon Yun, Charles Fleming, Tianlong Chen</div>
<div class="meta-line">First: 2025-03-31T20:43:56+00:00 · Latest: 2025-10-08T22:17:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.00218v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.00218v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most discussions about Large Language Model (LLM) safety have focused on
single-agent settings but multi-agent LLM systems now create novel adversarial
risks because their behavior depends on communication between agents and
decentralized reasoning. In this work, we innovatively focus on attacking
pragmatic systems that have constrains such as limited token bandwidth, latency
between message delivery, and defense mechanisms. We design a
$\textit{permutation-invariant adversarial attack}$ that optimizes prompt
distribution across latency and bandwidth-constraint network topologies to
bypass distributed safety mechanisms within the system. Formulating the attack
path as a problem of $\textit{maximum-flow minimum-cost}$, coupled with the
novel $\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage
graph-based optimization to maximize attack success rate while minimizing
detection risk. Evaluating across models including $\texttt{Llama}$,
$\texttt{Mistral}$, $\texttt{Gemma}$, $\texttt{DeepSeek}$ and other variants on
various datasets like $\texttt{JailBreakBench}$ and
$\texttt{AdversarialBench}$, our method outperforms conventional attacks by up
to $7\times$, exposing critical vulnerabilities in multi-agent systems.
Moreover, we demonstrate that existing defenses, including variants of
$\texttt{Llama-Guard}$ and $\texttt{PromptGuard}$, fail to prohibit our attack,
emphasizing the urgent need for multi-agent specific safety mechanisms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>《被围攻的代理：通过优化提示攻击打破务实的多代理大型语言模型系统》</div>
<div class="mono" style="margin-top:8px">关于大型语言模型（LLM）安全性的讨论大多集中在单代理设置上，但多代理LLM系统现在带来了新的对抗风险，因为它们的行为依赖于代理之间的通信和分散推理。在这项工作中，我们创新性地关注于攻击具有限制的务实系统，例如有限的令牌带宽、消息传递延迟和防御机制。我们设计了一种“置换不变对抗攻击”，优化延迟和带宽受限网络拓扑中的提示分布，以绕过系统内的分布式安全机制。将攻击路径公式化为“最大流最小成本”问题，结合新颖的“置换不变规避损失（PIEL）”，我们利用基于图的优化来最大化攻击成功率，同时最小化检测风险。在包括“Llama”、“Mistral”、“Gemma”、“DeepSeek”等模型以及“JailBreakBench”和“AdversarialBench”等各种数据集上的评估中，我们的方法比传统攻击提高了多达7倍，暴露了多代理系统中的关键漏洞。此外，我们证明了现有防御措施，包括“Llama-Guard”和“PromptGuard”的变体，无法阻止我们的攻击，强调了对多代理特定安全机制的迫切需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the emerging adversarial risks associated with multi-agent Large Language Model (LLM) systems, which have been largely overlooked in previous safety discussions that focused on single-agent settings. Past methods have struggled to effectively manage the unique challenges posed by communication and decentralized reasoning in multi-agent environments, leading to vulnerabilities that can be exploited. The proposed approach introduces a permutation-invariant adversarial attack that optimizes prompt distribution to navigate latency and bandwidth constraints, formulated as a maximum-flow minimum-cost problem, and utilizes a novel Permutation-Invariant Evasion Loss (PIEL) for enhanced effectiveness. The research methodology involves evaluating this attack across various models, including Llama and Mistral, on datasets like JailBreakBench and AdversarialBench, achieving up to a 7x improvement over conventional attacks and revealing significant weaknesses in existing multi-agent safety defenses, thereby highlighting the necessity for tailored safety mechanisms.</div>
<div class="mono" style="margin-top:8px">本研究关注多智能体大型语言模型（LLM）系统所带来的新兴对抗风险，而这些风险在以往主要集中于单智能体设置的安全讨论中被忽视。传统方法未能充分考虑智能体之间通信和去中心化推理所带来的独特挑战，导致这些系统存在漏洞。所提出的方法引入了一种置换不变的对抗攻击，优化了在受限网络拓扑中的提示分布，有效绕过现有的安全机制。该方法的动机明确，因为它将攻击路径公式化为最大流最小成本问题，并采用新颖的置换不变规避损失（PIEL），以提高攻击成功率并降低检测风险。在对包括Llama和Mistral在内的多种模型进行评估时，使用JailBreakBench和AdversarialBench等数据集，结果显示该方法相比传统攻击提高了多达7倍，揭示了多智能体系统中的重大弱点，并突显了当前防御措施的不足。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Psycho-Lexical Approach for Constructing Value Systems in   Large Language Models</div>
<div class="meta-line">Authors: Haoran Ye, Tianze Zhang, Yuhang Xie, Liyuan Zhang, Yuanyi Ren, Xin Zhang, Guojie Song</div>
<div class="meta-line">Venue: ACL 2025</div>
<div class="meta-line">First: 2025-02-04T16:10:55+00:00 · Latest: 2025-10-07T14:57:19+00:00</div>
<div class="meta-line">Comments: ACL 2025 Main</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.02444v6">Abs</a> · <a href="http://arxiv.org/pdf/2502.02444v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Values are core drivers of individual and collective perception, cognition,
and behavior. Value systems, such as Schwartz&#x27;s Theory of Basic Human Values,
delineate the hierarchy and interplay among these values, enabling
cross-disciplinary investigations into decision-making and societal dynamics.
Recently, the rise of Large Language Models (LLMs) has raised concerns
regarding their elusive intrinsic values. Despite growing efforts in
evaluating, understanding, and aligning LLM values, a psychologically grounded
LLM value system remains underexplored. This study addresses the gap by
introducing the Generative Psycho-Lexical Approach (GPLA), a scalable,
adaptable, and theoretically informed method for constructing value systems.
Leveraging GPLA, we propose a psychologically grounded five-factor value system
tailored for LLMs. For systematic validation, we present three benchmarking
tasks that integrate psychological principles with cutting-edge AI priorities.
Our results reveal that the proposed value system meets standard psychological
criteria, better captures LLM values, improves LLM safety prediction, and
enhances LLM alignment, when compared to the canonical Schwartz&#x27;s values.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成性心理词汇方法构建大型语言模型的价值体系</div>
<div class="mono" style="margin-top:8px">价值观是个体和集体感知、认知和行为的核心驱动力。价值体系，如施瓦茨的基本人类价值理论，描绘了这些价值之间的层次和相互作用，使跨学科研究决策和社会动态成为可能。最近，大型语言模型（LLMs）的兴起引发了对其难以捉摸的内在价值的担忧。尽管在评估、理解和对齐LLM价值方面的努力不断增加，但基于心理学的LLM价值体系仍然未得到充分探索。本研究通过引入生成性心理词汇方法（GPLA）来填补这一空白，这是一种可扩展、可适应且理论上有依据的构建价值体系的方法。利用GPLA，我们提出了一个为LLM量身定制的基于心理学的五因素价值体系。为了进行系统验证，我们提出了三个基准任务，将心理学原理与前沿AI优先事项相结合。我们的结果表明，所提出的价值体系符合标准心理学标准，更好地捕捉LLM价值，提高LLM安全预测，并增强LLM对齐，相较于经典的施瓦茨价值观。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the need for a psychologically grounded value system in Large Language Models (LLMs), as existing methods have not adequately captured the intrinsic values of these models. Previous approaches, such as Schwartz&#x27;s Theory of Basic Human Values, lack the adaptability and scalability required for LLMs, leading to insufficient alignment and safety predictions. The proposed Generative Psycho-Lexical Approach (GPLA) offers a novel framework that constructs a five-factor value system specifically designed for LLMs, integrating psychological principles with AI priorities. The methodology includes systematic validation through three benchmarking tasks, demonstrating that the GPLA-based value system not only meets established psychological criteria but also significantly improves the alignment and safety prediction of LLMs compared to traditional methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在填补大型语言模型（LLMs）中缺乏心理学基础的价值体系的空白，强调现有方法在评估和对齐LLM价值时的不足。以往的方法往往忽视内在价值，导致对其在决策和社会动态中的影响理解不足。提出的生成心理词汇方法（GPLA）提供了一种可扩展和适应性强的框架，专门为LLMs构建五因素价值体系，结合了心理学原则和人工智能优先事项。通过三个基准任务验证了该方法，结果表明，基于GPLA的价值体系不仅符合既定的心理学标准，而且在LLM安全预测和对齐方面优于传统的施瓦茨价值，从而有效支持了研究目标。</div>
</details>
</div>
<div class="card">
<div class="title">Sampling-aware Adversarial Attacks Against Large Language Models</div>
<div class="meta-line">Authors: Tim Beyer, Yan Scholten, Leo Schwinn, Stephan Günnemann</div>
<div class="meta-line">First: 2025-07-06T16:13:33+00:00 · Latest: 2025-10-06T09:02:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.04446v3">Abs</a> · <a href="http://arxiv.org/pdf/2507.04446v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To guarantee safe and robust deployment of large language models (LLMs) at
scale, it is critical to accurately assess their adversarial robustness.
Existing adversarial attacks typically target harmful responses in single-point
greedy generations, overlooking the inherently stochastic nature of LLMs and
overestimating robustness. We show that for the goal of eliciting harmful
responses, repeated sampling of model outputs during the attack complements
prompt optimization and serves as a strong and efficient attack vector. By
casting attacks as a resource allocation problem between optimization and
sampling, we determine compute-optimal trade-offs and show that integrating
sampling into existing attacks boosts success rates by up to 37\% and improves
efficiency by up to two orders of magnitude. We further analyze how
distributions of output harmfulness evolve during an adversarial attack,
discovering that many common optimization strategies have little effect on
output harmfulness. Finally, we introduce a label-free proof-of-concept
objective based on entropy maximization, demonstrating how our sampling-aware
perspective enables new optimization targets. Overall, our findings establish
the importance of sampling in attacks to accurately assess and strengthen LLM
safety at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对大型语言模型的采样感知对抗攻击</div>
<div class="mono" style="margin-top:8px">为了确保大型语言模型（LLMs）在大规模部署中的安全性和鲁棒性，准确评估其对抗鲁棒性至关重要。现有的对抗攻击通常针对单点贪婪生成中的有害响应，忽视了LLMs固有的随机性，并高估了鲁棒性。我们表明，在引发有害响应的目标下，攻击期间对模型输出的重复采样补充了提示优化，并作为一种强大而高效的攻击向量。通过将攻击视为优化与采样之间的资源分配问题，我们确定了计算最优的权衡，并显示将采样整合到现有攻击中可以将成功率提高多达37\%，并将效率提高两个数量级。我们进一步分析了对抗攻击期间输出有害性分布的演变，发现许多常见的优化策略对输出有害性几乎没有影响。最后，我们引入了一种基于熵最大化的无标签概念验证目标，展示了我们的采样感知视角如何启用新的优化目标。总体而言，我们的研究结果确立了在攻击中采样的重要性，以准确评估和增强LLM在大规模下的安全性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical need for accurately assessing the adversarial robustness of large language models (LLMs) to ensure their safe deployment. Previous methods primarily focused on single-point greedy generations, which failed to account for the stochastic nature of LLMs, leading to an overestimation of their robustness. The proposed approach integrates repeated sampling of model outputs with prompt optimization, framing the attacks as a resource allocation problem that optimizes compute trade-offs. This method significantly enhances the success rates of attacks by up to 37% and improves efficiency by two orders of magnitude. The paper contributes to the understanding of how harmful output distributions change during adversarial attacks and introduces a novel label-free objective based on entropy maximization, ultimately highlighting the importance of sampling in evaluating and improving LLM safety.</div>
<div class="mono" style="margin-top:8px">本研究解决了准确评估大型语言模型（LLMs）对抗鲁棒性的重要需求，以确保其安全部署。以往的方法主要集中在单点贪婪生成上，未能考虑LLMs的随机性，导致对其鲁棒性的高估。所提出的方法将模型输出的重复采样与提示优化相结合，将攻击框架视为资源分配问题，从而优化计算权衡。该方法显著提高了对抗攻击的成功率，提升幅度可达37%，并将效率提高了两个数量级。本文的贡献在于加深了对攻击过程中有害输出分布演变的理解，并引入了一种基于熵最大化的无标签目标，强调了在评估和增强LLM安全性时采样的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Chasing Moving Targets with Online Self-Play Reinforcement Learning for   Safer Language Models</div>
<div class="meta-line">Authors: Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, Natasha Jaques</div>
<div class="meta-line">First: 2025-06-09T06:35:12+00:00 · Latest: 2025-10-06T03:42:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.07468v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.07468v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过在线自我对弈强化学习追踪移动目标以提高语言模型的安全性</div>
<div class="mono" style="margin-top:8px">传统的语言模型（LM）安全对齐依赖于反应性、分离的程序：攻击者利用静态模型，随后进行防御性微调以修补暴露的漏洞。这种顺序方法造成了不匹配——攻击者过度拟合过时的防御，而防御者则始终滞后于新出现的威胁。为了解决这个问题，我们提出了Self-RedTeam，一种在线自我对弈强化学习算法，其中攻击者和防御者代理通过持续互动共同进化。我们将安全对齐视为一个双人零和游戏，其中单个模型在攻击者和防御者角色之间交替——生成对抗性提示并对其进行保护——同时奖励语言模型裁定结果。这使得动态共同适应成为可能。基于零和游戏的博弈论框架，我们建立了理论安全保证，这激励了我们方法的设计：如果自我对弈收敛到纳什均衡，防御者将可靠地产生对任何对抗性输入的安全响应。在实证上，Self-RedTeam发现了比针对静态防御者训练的攻击者更具多样性的攻击（+21.8% SBERT），并在安全基准上实现了更高的鲁棒性（例如，在WildJailBreak上+65.5%），相比于针对静态攻击者训练的防御者。我们进一步提出了隐藏的思维链，允许代理私下规划，从而提高对抗性多样性并减少过度拒绝。我们的结果激励了从反应性修补转向主动共同进化的LM安全训练，使得通过多智能体强化学习（MARL）实现可扩展、自主和鲁棒的自我改进成为可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of conventional language model safety alignment, which typically follows a reactive and disjointed process, leading to a mismatch between attackers exploiting static models and defenders lagging in response. Previous methods often resulted in defenders being outpaced by evolving threats, prompting the need for a more dynamic approach. The proposed Self-RedTeam introduces an online self-play reinforcement learning algorithm that allows an attacker and defender agent to co-evolve through continuous interaction, framed as a two-player zero-sum game. This method not only establishes a theoretical safety guarantee through the convergence to a Nash Equilibrium but also empirically demonstrates improved performance, uncovering 21.8% more diverse attacks and achieving 65.5% higher robustness on safety benchmarks compared to traditional methods. The findings support a shift towards proactive co-evolution in language model safety training, enhancing the scalability and autonomy of safety mechanisms through multi-agent reinforcement learning.</div>
<div class="mono" style="margin-top:8px">本研究解决了传统语言模型安全对齐的局限性，传统方法通常采用反应性和分离的方式，导致防御者滞后于攻击者。以往的方法依赖于静态模型，导致攻击者利用过时的防御措施。提出的Self-RedTeam引入了一种在线自我对抗强化学习算法，攻击者和防御者通过共同演化，安全对齐被框定为一个双人零和博弈。该方法允许动态适应，并基于纳什均衡建立了理论安全保证，确保防御者能够对对抗输入产生安全响应。实证结果表明，Self-RedTeam在攻击多样性上提高了21.8%，在安全基准测试中提高了65.5%的鲁棒性，相较于传统方法，支持了语言模型安全训练中主动共同演化的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost   Always!</div>
<div class="meta-line">Authors: Jingdi Lei, Varun Gumma, Rishabh Bhardwaj, Seok Min Lim, Chuan Li, Amir Zadeh, Soujanya Poria</div>
<div class="meta-line">First: 2025-09-30T16:39:17+00:00 · Latest: 2025-10-03T12:46:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.26495v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.26495v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model (LLM) safety is one of the most pressing challenges for
enabling wide-scale deployment. While most studies and global discussions focus
on generic harms, such as models assisting users in harming themselves or
others, enterprises face a more fundamental concern: whether LLM-based agents
are safe for their intended use case. To address this, we introduce operational
safety, defined as an LLM&#x27;s ability to appropriately accept or refuse user
queries when tasked with a specific purpose. We further propose OffTopicEval,
an evaluation suite and benchmark for measuring operational safety both in
general and within specific agentic use cases. Our evaluations on six model
families comprising 20 open-weight LLMs reveal that while performance varies
across models, all of them remain highly operationally unsafe. Even the
strongest models - Qwen-3 (235B) with 77.77% and Mistral (24B) with 79.96% -
fall far short of reliable operational safety, while GPT models plateau in the
62-73% range, Phi achieves only mid-level scores (48-70%), and Gemma and
Llama-3 collapse to 39.53% and 23.84%, respectively. While operational safety
is a core model alignment issue, to suppress these failures, we propose
prompt-based steering methods: query grounding (Q-ground) and system-prompt
grounding (P-ground), which substantially improve OOD refusal. Q-ground
provides consistent gains of up to 23%, while P-ground delivers even larger
boosts, raising Llama-3.3 (70B) by 41% and Qwen-3 (30B) by 27%. These results
highlight both the urgent need for operational safety interventions and the
promise of prompt-based steering as a first step toward more reliable LLM-based
agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OffTopicEval：当大型语言模型进入错误的聊天时，几乎总是如此！</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）安全性是实现大规模部署的最紧迫挑战之一。虽然大多数研究和全球讨论集中在一般性危害上，例如模型协助用户伤害自己或他人，但企业面临更根本的担忧：基于LLM的代理是否安全用于其预期用途。为了解决这个问题，我们引入了操作安全性，定义为LLM在特定目的下适当地接受或拒绝用户查询的能力。我们进一步提出了OffTopicEval，一个评估套件和基准，用于测量一般和特定代理使用案例中的操作安全性。我们对六个模型家族中20个开放权重LLM的评估显示，尽管模型之间的性能差异很大，但它们都保持高度的操作不安全性。即使是最强的模型——Qwen-3（235B）为77.77%和Mistral（24B）为79.96%——也远未达到可靠的操作安全性，而GPT模型的表现停滞在62-73%的范围内，Phi仅达到中等分数（48-70%），Gemma和Llama-3分别降至39.53%和23.84%。虽然操作安全性是核心模型对齐问题，为了抑制这些失败，我们提出了基于提示的引导方法：查询基础（Q-ground）和系统提示基础（P-ground），这大大改善了OOD拒绝。Q-ground提供了高达23%的持续增益，而P-ground则提供了更大的提升，使Llama-3.3（70B）提高41%，Qwen-3（30B）提高27%。这些结果突显了对操作安全性干预的迫切需求，以及基于提示的引导作为朝着更可靠的基于LLM的代理迈出的第一步的前景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical issue of operational safety in Large Language Models (LLMs), focusing on their ability to appropriately accept or refuse user queries for specific purposes. Previous methods primarily concentrated on generic harms without adequately assessing the suitability of LLMs for intended use cases, leading to a gap in operational safety evaluation. The proposed approach, OffTopicEval, introduces an evaluation suite and benchmark specifically designed to measure operational safety across various model families. This methodology reveals that all tested models exhibit significant operational safety shortcomings, with the strongest models still falling short of acceptable levels. The paper contributes by demonstrating the effectiveness of prompt-based steering methods, such as query grounding and system-prompt grounding, which significantly enhance out-of-distribution refusal rates, achieving performance improvements of up to 41% in some cases, thereby underscoring the necessity for operational safety interventions in LLM applications.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在特定用例中操作安全性的问题，这是其安全部署的关键。以往的研究主要集中在一般性危害上，而未能充分评估LLMs在指定上下文中是否能够适当地处理用户查询。所提出的方法OffTopicEval引入了一个评估操作安全性的基准，揭示所有测试模型在操作安全性方面存在显著不足。该方法论包括基于提示的引导技术，特别是查询引导（Q-ground）和系统提示引导（P-ground），这些技术提高了对分布外查询的拒绝率。对20个开放权重LLM的评估表明，尽管性能存在差异，但即使是最佳模型也未能实现可靠的操作安全性，强调了干预的必要性以及所提出方法的有效性，这些方法在安全指标上显示出显著改善。</div>
</details>
</div>
<div class="card">
<div class="title">UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language   Models</div>
<div class="meta-line">Authors: Yuhao Sun, Zhuoer Xu, Shiwen Cui, Kun Yang, Lingyun Yu, Yongdong Zhang, Hongtao Xie</div>
<div class="meta-line">First: 2025-10-02T16:43:33+00:00 · Latest: 2025-10-02T16:43:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.02194v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.02194v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have achieved remarkable progress across a wide
range of tasks, but remain vulnerable to safety risks such as harmful content
generation and jailbreak attacks. Existing safety techniques -- including
external guardrails, inference-time guidance, and post-training alignment --
each face limitations in balancing safety, utility, and controllability. In
this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM
safety through safety-aware upcycling. Our approach first identifies
safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)
structure, where the router acts as a soft guardrail that selectively activates
original MLPs and added safety experts. We further introduce a two-stage SFT
strategy to strengthen safety discrimination while preserving general
capabilities. To enable flexible control at inference time, we introduce a
safety temperature mechanism, allowing dynamic adjustment of the trade-off
between safety and utility. Experiments across multiple benchmarks, base model,
and model scales demonstrate that UpSafe$^\circ$C achieves robust safety
improvements against harmful and jailbreak inputs, while maintaining
competitive performance on general tasks. Moreover, analysis shows that safety
temperature provides fine-grained inference-time control that achieves the
Pareto-optimal frontier between utility and safety. Our results highlight a new
direction for LLM safety: moving from static alignment toward dynamic, modular,
and inference-aware control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UpSafe$^\circ$C：大型语言模型的可控安全升级</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在广泛任务中取得了显著进展，但仍然容易受到有害内容生成和越狱攻击等安全风险的影响。现有的安全技术——包括外部护栏、推理时指导和后训练对齐——在平衡安全性、实用性和可控性方面各有局限。在本研究中，我们提出了UpSafe$^\circ$C，这是一个通过安全意识升级来增强LLM安全性的统一框架。我们的方法首先识别安全关键层，并将其升级为稀疏的专家混合（MoE）结构，其中路由器充当选择性激活原始MLP和新增安全专家的软护栏。我们进一步引入了两阶段的SFT策略，以增强安全区分能力，同时保持一般能力。为了在推理时实现灵活控制，我们引入了一种安全温度机制，允许动态调整安全性和实用性之间的权衡。多个基准、基础模型和模型规模的实验表明，UpSafe$^\circ$C在对抗有害和越狱输入时实现了稳健的安全性提升，同时在一般任务上保持了竞争性能。此外，分析表明安全温度提供了细粒度的推理时控制，实现了实用性和安全性之间的帕累托最优边界。我们的结果突出了LLM安全性的新方向：从静态对齐转向动态、模块化和推理意识的控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the safety vulnerabilities of Large Language Models (LLMs), which can generate harmful content and be susceptible to jailbreak attacks. Previous safety methods, such as external guardrails and post-training alignment, struggle to balance safety, utility, and controllability. The proposed UpSafe$^\circ$C framework improves LLM safety through a novel approach called safety-aware upcycling, which identifies safety-critical layers and transforms them into a sparse Mixture-of-Experts (MoE) structure, allowing for selective activation of safety experts. This method is well-motivated as it enhances safety discrimination while preserving general capabilities through a two-stage SFT strategy and introduces a safety temperature mechanism for flexible control during inference. Experimental results show that UpSafe$^\circ$C significantly improves safety against harmful inputs and jailbreak attempts while maintaining competitive performance on general tasks, achieving a Pareto-optimal balance between utility and safety.</div>
<div class="mono" style="margin-top:8px">本文针对大型语言模型（LLMs）面临的重大安全风险，如有害内容生成和越狱攻击进行了研究，现有的安全技术在平衡安全性、实用性和可控性方面存在局限性。提出的方法UpSafe$^\circ$C通过引入安全感知的再利用，识别安全关键层并将其转化为稀疏的专家混合（MoE）结构，与以往方法不同，从而增强安全性并保持性能。本文的贡献在于提出了两阶段的SFT策略以改善安全性判别，以及在推理过程中动态控制的安全温度机制，允许在安全性和实用性之间灵活权衡。实验结果表明，UpSafe$^\circ$C在多个基准测试中显著提高了对有害输入的安全性，同时保持了竞争力的性能，表明该方法成功支持了增强LLM安全性而不牺牲实用性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Instincts: LLMs Learn to Trust Their Internal Compass for   Self-Defense</div>
<div class="meta-line">Authors: Guobin Shen, Dongcheng Zhao, Haibo Tong, Jindong Li, Feifei Zhao, Yi Zeng</div>
<div class="meta-line">First: 2025-10-01T16:35:03+00:00 · Latest: 2025-10-01T16:35:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.01088v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.01088v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring Large Language Model (LLM) safety remains challenging due to the
absence of universal standards and reliable content validators, making it
difficult to obtain effective training signals. We discover that aligned models
already possess robust internal safety beliefs: they consistently produce
high-confidence refusals to harmful requests while exhibiting high entropy when
generating potentially dangerous content. This entropy gap reveals an untapped
signal--models intrinsically &quot;know&quot; when to refuse. We introduce Safety
Instincts Reinforcement Learning (SIRL), which transforms this internal
confidence into a self-generated reward signal, eliminating dependence on
external validators or human annotations. SIRL teaches models to trust their
safety instincts by reinforcing low-entropy refusal behaviors. Evaluated on
Llama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against
20+ jailbreak methods, from static prompts to adaptive attacks. Using only
15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods
while preserving performance on mathematics, coding, and conversation
benchmarks. Our work demonstrates that effective alignment can emerge from
within, paving the way for more autonomous and robust AI safety mechanisms that
scale without extensive human oversight.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全本能：大型语言模型学习信任其内部指南以进行自我防御</div>
<div class="mono" style="margin-top:8px">确保大型语言模型（LLM）的安全性仍然具有挑战性，因为缺乏普遍标准和可靠的内容验证者，使得获得有效的训练信号变得困难。我们发现，已对齐的模型已经具备强大的内部安全信念：它们在面对有害请求时始终产生高置信度的拒绝，同时在生成潜在危险内容时表现出高熵。这一熵差揭示了一个未被利用的信号——模型本质上“知道”何时拒绝。我们引入了安全本能强化学习（SIRL），将这种内部信心转化为自生成的奖励信号，消除对外部验证者或人工注释的依赖。SIRL通过强化低熵拒绝行为来教导模型信任其安全本能。在Llama和Qwen模型上的评估表明，SIRL在20多种越狱方法（从静态提示到自适应攻击）中保持89%以上的防御成功率（DSR）。仅使用15,000个未标记的提示，SIRL超越了资源密集型的监督方法，同时在数学、编码和对话基准测试中保持性能。我们的工作表明，有效的对齐可以从内部产生，为更自主和强大的AI安全机制铺平道路，这些机制在没有广泛人工监督的情况下可扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the ongoing challenges in ensuring the safety of Large Language Models (LLMs), primarily due to the lack of universal standards and reliable content validators, which complicates the training process. Previous methods often relied heavily on external validators or human annotations, leading to inefficiencies and potential biases. The proposed approach, Safety Instincts Reinforcement Learning (SIRL), leverages the internal safety beliefs of aligned models, transforming their intrinsic confidence into a self-generated reward signal that reinforces low-entropy refusal behaviors. This method is well-motivated as it aims to enhance model autonomy in safety decisions. The contribution of the paper lies in demonstrating that SIRL can achieve over 89% Defense Success Rates against various jailbreak methods using only 15,000 unlabeled prompts, outperforming traditional supervised methods while maintaining performance across other benchmarks such as mathematics and coding.</div>
<div class="mono" style="margin-top:8px">本研究解决了确保大型语言模型（LLM）安全性的问题，这一问题因缺乏普遍标准和可靠内容验证者而变得复杂，导致有效训练信号的获取困难。以往的方法通常过于依赖外部验证者或人工注释，这可能资源密集且无法有效捕捉模型内在的安全信念。提出的安全本能强化学习（SIRL）方法利用对齐模型的内部信心，创建自生成的奖励信号，从而减少对外部输入的依赖。该方法的动机明确，因为它通过强化低熵拒绝行为来教导模型信任自身的安全本能。本文的贡献在于证明SIRL能够在仅使用15,000个未标记提示的情况下，对抗多种越狱方法时实现超过89%的防御成功率，超越传统的监督方法，同时在数学和编码等其他基准上保持性能。</div>
</details>
</div>
<div class="card">
<div class="title">QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety</div>
<div class="meta-line">Authors: Taegyeong Lee, Jeonghwa Yoo, Hyoungseo Cho, Soo Yong Kim, Yunho Maeng</div>
<div class="meta-line">Venue: ACL</div>
<div class="meta-line">First: 2025-06-14T01:23:50+00:00 · Latest: 2025-09-30T07:47:51+00:00</div>
<div class="meta-line">Comments: Accept to ACLW 2025 (WOAH); fix typo</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.12299v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.12299v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent advancements in Large Language Models(LLMs) have had a significant
impact on a wide range of fields, from general domains to specialized areas.
However, these advancements have also significantly increased the potential for
malicious users to exploit harmful and jailbreak prompts for malicious attacks.
Although there have been many efforts to prevent harmful prompts and jailbreak
prompts, protecting LLMs from such malicious attacks remains an important and
challenging task. In this paper, we propose QGuard, a simple yet effective
safety guard method, that utilizes question prompting to block harmful prompts
in a zero-shot manner. Our method can defend LLMs not only from text-based
harmful prompts but also from multi-modal harmful prompt attacks. Moreover, by
diversifying and modifying guard questions, our approach remains robust against
the latest harmful prompts without fine-tuning. Experimental results show that
our model performs competitively on both text-only and multi-modal harmful
datasets. Additionally, by providing an analysis of question prompting, we
enable a white-box analysis of user inputs. We believe our method provides
valuable insights for real-world LLM services in mitigating security risks
associated with harmful prompts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QGuard：基于问题的零-shot多模态LLM安全防护</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的最新进展对从一般领域到专业领域的广泛领域产生了重大影响。然而，这些进展也显著增加了恶意用户利用有害和越狱提示进行恶意攻击的潜力。尽管已经有许多努力来防止有害提示和越狱提示，但保护LLMs免受此类恶意攻击仍然是一项重要且具有挑战性的任务。本文提出了QGuard，一种简单而有效的安全防护方法，利用问题提示以零-shot方式阻止有害提示。我们的方法不仅可以防御基于文本的有害提示，还可以防御多模态有害提示攻击。此外，通过多样化和修改防护问题，我们的方法在不进行微调的情况下仍然对最新的有害提示保持稳健。实验结果表明，我们的模型在文本和多模态有害数据集上表现出竞争力。此外，通过提供问题提示的分析，我们实现了对用户输入的白盒分析。我们相信我们的方法为现实世界的LLM服务提供了有价值的见解，以减轻与有害提示相关的安全风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the growing concern of malicious exploitation of Large Language Models (LLMs) through harmful and jailbreak prompts, which poses significant security risks across various applications. Previous methods aimed at preventing such attacks have been insufficient, often requiring fine-tuning or lacking robustness against evolving threats. The proposed QGuard method distinguishes itself by employing question prompting in a zero-shot manner, effectively blocking harmful prompts without the need for extensive model adjustments. The contribution of this paper lies in its innovative approach to safeguarding LLMs from both text-based and multi-modal attacks, demonstrating competitive performance on relevant datasets. The methodology involves diversifying and modifying guard questions to enhance resilience against new harmful prompts, thus supporting the goal of improving LLM safety in real-world applications.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）被恶意利用的日益严重的问题，特别是通过有害和越狱提示，这对各种应用构成了重大安全风险。以往旨在防止此类攻击的方法效果不佳，往往需要微调或缺乏对不断演变威胁的鲁棒性。提出的QGuard方法通过采用零样本的问题提示，能够有效阻止有害提示，且无需进行广泛的修改或微调，从而与众不同。该方法不仅保护文本攻击，还扩展到多模态有害提示，展示了其多样性。实验结果表明，QGuard在文本和多模态有害数据集上均表现出竞争力的性能，支持其在现实应用中增强LLMs安全性的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Backdoor Attribution: Elucidating and Controlling Backdoor in Language   Models</div>
<div class="meta-line">Authors: Miao Yu, Zhenhong Zhou, Moayad Aloqaily, Kun Wang, Biwei Huang, Stephen Wang, Yueming Jin, Qingsong Wen</div>
<div class="meta-line">First: 2025-09-26T01:45:25+00:00 · Latest: 2025-09-30T01:52:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.21761v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.21761v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuned Large Language Models (LLMs) are vulnerable to backdoor attacks
through data poisoning, yet the internal mechanisms governing these attacks
remain a black box. Previous research on interpretability for LLM safety tends
to focus on alignment, jailbreak, and hallucination, but overlooks backdoor
mechanisms, making it difficult to understand and fully eliminate the backdoor
threat. In this paper, aiming to bridge this gap, we explore the interpretable
mechanisms of LLM backdoors through Backdoor Attribution (BkdAttr), a
tripartite causal analysis framework. We first introduce the Backdoor Probe
that proves the existence of learnable backdoor features encoded within the
representations. Building on this insight, we further develop Backdoor
Attention Head Attribution (BAHA), efficiently pinpointing the specific
attention heads responsible for processing these features. Our primary
experiments reveals these heads are relatively sparse; ablating a minimal
\textbf{$\sim$ 3%} of total heads is sufficient to reduce the Attack Success
Rate (ASR) by \textbf{over 90%}. More importantly, we further employ these
findings to construct the Backdoor Vector derived from these attributed heads
as a master controller for the backdoor. Through only \textbf{1-point}
intervention on \textbf{single} representation, the vector can either boost ASR
up to \textbf{$\sim$ 100% ($\uparrow$)} on clean inputs, or completely
neutralize backdoor, suppressing ASR down to \textbf{$\sim$ 0% ($\downarrow$)}
on triggered inputs. In conclusion, our work pioneers the exploration of
mechanistic interpretability in LLM backdoors, demonstrating a powerful method
for backdoor control and revealing actionable insights for the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>后门归因：阐明和控制语言模型中的后门</div>
<div class="mono" style="margin-top:8px">微调的大型语言模型（LLMs）易受数据中毒的后门攻击，但这些攻击的内部机制仍然是一个黑箱。以往关于LLM安全性的可解释性研究往往集中在对齐、越狱和幻觉上，而忽视了后门机制，使得理解和完全消除后门威胁变得困难。本文旨在填补这一空白，通过后门归因（BkdAttr）这一三方因果分析框架，探索LLM后门的可解释机制。我们首先引入后门探测器，证明了编码在表示中的可学习后门特征的存在。在此基础上，我们进一步开发了后门注意力头归因（BAHA），有效地定位负责处理这些特征的特定注意力头。我们的主要实验表明，这些头相对稀疏；消除大约3%的总头数足以将攻击成功率（ASR）降低超过90%。更重要的是，我们进一步利用这些发现构建了基于这些归因头的后门向量，作为后门的主控器。通过对单一表示进行1点干预，该向量可以在干净输入上将ASR提升至约100%（↑），或在未触发输入上完全中和后门，将ASR压制至约0%（↓）。总之，我们的工作开创了LLM后门机制可解释性的探索，展示了一种强大的后门控制方法，并为社区提供了可操作的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the vulnerability of fine-tuned Large Language Models (LLMs) to backdoor attacks via data poisoning, highlighting a gap in the interpretability research that has largely neglected backdoor mechanisms. Previous methods focused on aspects like alignment and hallucination but failed to provide insights into backdoor threats, which this paper aims to rectify through a novel approach called Backdoor Attribution (BkdAttr), a tripartite causal analysis framework. The paper contributes by introducing the Backdoor Probe to confirm the existence of learnable backdoor features and developing Backdoor Attention Head Attribution (BAHA) to identify the specific attention heads involved. The methodology demonstrates that ablating approximately 3% of total heads can reduce the Attack Success Rate (ASR) by over 90%, and the Backdoor Vector derived from these heads can either enhance ASR to nearly 100% or neutralize the backdoor effect entirely, achieving ASR close to 0%. This work advances the understanding of LLM backdoors and provides a robust mechanism for their control.</div>
<div class="mono" style="margin-top:8px">本研究关注微调的大型语言模型（LLMs）在数据中毒下对后门攻击的脆弱性，强调了对这些攻击内部机制缺乏理解的问题。以往的方法主要集中在对齐和幻觉等可解释性方面，忽视了后门机制，从而使消除此类威胁变得复杂。提出的方法，后门归因（BkdAttr），引入了一个三方因果分析框架，包括后门探测器以识别可学习的后门特征，以及后门注意力头归因（BAHA）以隔离负责这些特征的特定注意力头。这一方法论表明，去除大约3%的总注意力头可以使攻击成功率（ASR）降低超过90%，而构建的后门向量则允许对后门进行显著控制，在干净输入上实现近100%的ASR提升，或在触发输入上完全中和。研究结果为LLM后门的机制可解释性做出了贡献，并为增强模型安全性提供了可行的见解。</div>
</details>
</div>
<div class="card">
<div class="title">STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents</div>
<div class="meta-line">Authors: Jing-Jing Li, Jianfeng He, Chao Shang, Devang Kulshreshtha, Xun Xian, Yi Zhang, Hang Su, Sandesh Swamy, Yanjun Qi</div>
<div class="meta-line">First: 2025-09-30T00:31:44+00:00 · Latest: 2025-09-30T00:31:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.25624v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.25624v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As LLMs advance into autonomous agents with tool-use capabilities, they
introduce security challenges that extend beyond traditional content-based LLM
safety concerns. This paper introduces Sequential Tool Attack Chaining (STAC),
a novel multi-turn attack framework that exploits agent tool use. STAC chains
together tool calls that each appear harmless in isolation but, when combined,
collectively enable harmful operations that only become apparent at the final
execution step. We apply our framework to automatically generate and
systematically evaluate 483 STAC cases, featuring 1,352 sets of
user-agent-environment interactions and spanning diverse domains, tasks, agent
types, and 10 failure modes. Our evaluations show that state-of-the-art LLM
agents, including GPT-4.1, are highly vulnerable to STAC, with attack success
rates (ASR) exceeding 90% in most cases. The core design of STAC&#x27;s automated
framework is a closed-loop pipeline that synthesizes executable multi-step tool
chains, validates them through in-environment execution, and reverse-engineers
stealthy multi-turn prompts that reliably induce agents to execute the verified
malicious sequence. We further perform defense analysis against STAC and find
that existing prompt-based defenses provide limited protection. To address this
gap, we propose a new reasoning-driven defense prompt that achieves far
stronger protection, cutting ASR by up to 28.8%. These results highlight a
crucial gap: defending tool-enabled agents requires reasoning over entire
action sequences and their cumulative effects, rather than evaluating isolated
prompts or responses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STAC：当无辜工具形成危险链条以越狱LLM代理</div>
<div class="mono" style="margin-top:8px">随着LLM向具备工具使用能力的自主代理发展，它们引入了超越传统内容基础LLM安全问题的安全挑战。本文介绍了顺序工具攻击链（STAC），一种新颖的多轮攻击框架，利用代理工具的使用。STAC将看似无害的工具调用串联在一起，但当组合时，集体启用有害操作，这些操作仅在最终执行步骤时显现。我们应用该框架自动生成并系统评估了483个STAC案例，涉及1,352组用户-代理-环境交互，涵盖多种领域、任务、代理类型和10种失败模式。我们的评估显示，最先进的LLM代理，包括GPT-4.1，对STAC高度脆弱，在大多数情况下攻击成功率（ASR）超过90%。STAC自动化框架的核心设计是一个闭环管道，合成可执行的多步骤工具链，通过环境执行进行验证，并反向工程隐蔽的多轮提示，可靠地诱导代理执行经过验证的恶意序列。我们进一步对STAC进行防御分析，发现现有的基于提示的防御提供的保护有限。为了解决这一差距，我们提出了一种新的基于推理的防御提示，能够实现更强的保护，将ASR降低至28.8%。这些结果突显了一个关键差距：防御工具启用的代理需要对整个行动序列及其累积效果进行推理，而不是评估孤立的提示或响应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the emerging security challenges posed by large language models (LLMs) as they evolve into autonomous agents capable of tool use, which extends beyond traditional safety concerns. Previous methods primarily focused on content-based safety, failing to account for the cumulative effects of tool interactions, leading to vulnerabilities in LLM agents. The proposed Sequential Tool Attack Chaining (STAC) framework innovatively chains together seemingly harmless tool calls to expose harmful operations that only manifest at the final execution stage. This paper contributes by systematically generating and evaluating 483 STAC cases across various domains, demonstrating that state-of-the-art LLM agents, including GPT-4.1, exhibit attack success rates exceeding 90%. The methodology involves a closed-loop pipeline for synthesizing and validating multi-step tool chains, revealing that existing defenses are inadequate, while a new reasoning-driven defense prompt significantly reduces attack success rates by up to 28.8%. These findings underscore the necessity of considering entire action sequences for effective defense against tool-enabled agents.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）作为具备工具使用能力的自主代理所带来的新兴安全挑战，这些挑战超出了传统安全问题的范围。以往的方法主要集中在基于内容的安全性，未能考虑看似无害的工具调用链可能导致的风险。提出的顺序工具攻击链（STAC）框架通过系统生成和评估利用这些漏洞的多轮攻击场景，提供了一种新颖的方法。该方法的动机明确，因为它揭示了现有防御措施的局限性，这些措施通常忽视工具交互的累积效应。研究表明，最先进的LLM代理，包括GPT-4.1，对STAC高度敏感，攻击成功率超过90%。此外，论文引入了一种新的基于推理的防御提示，显著增强了保护效果，将攻击成功率降低了多达28.8%，强调了需要考虑整个行动序列的全面防御。</div>
</details>
</div>
<div class="card">
<div class="title">Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection</div>
<div class="meta-line">Authors: Hoang Phan, Victor Li, Qi Lei</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-09-29T12:54:28+00:00 · Latest: 2025-09-29T12:54:28+00:00</div>
<div class="meta-line">Comments: Accepted to EMNLP 2025 Findings</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.01270v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.01270v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have revolutionized natural language processing
with their ability to generate coherent and contextually relevant text.
However, their deployment raises significant concerns about the potential for
generating harmful or inappropriate content. In this paper, we introduce
Progressive Self-Reflection (PSR), a novel inference-time technique that
empowers LLMs to self-monitor and correct their outputs dynamically.
Experimental results demonstrate that applying our proposed method to
Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\% to 5.9\%, to
Llama-3.1-8B base from 89.7\% to 5.6\%, and to Qwen2.5-7B-Instruct from 44.4\%
to 3.8\%, without additional training, while maintaining their original
performance on benign tasks. Our approach acts as a test-time scaling method,
where additional self-reflection rounds enhance safety at the cost of inference
overhead. To balance safety with computational efficiency, we introduce a
lightweight self-reflection predictor that estimates the optimal number of
reflection rounds based on input complexity. This adaptive mechanism prevents
unnecessary self-assessment on benign inputs while ensuring thorough evaluation
when encountering potentially harmful content. Our findings suggest that
Progressive Self-Reflection serves as a scalable test-time approach, enhancing
LLM safety by dynamically allocating computational resources in proportion to
the input&#x27;s risk profile.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>三思而后行：通过渐进自我反思保障安全</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）凭借生成连贯且上下文相关文本的能力，彻底改变了自然语言处理。然而，它们的应用引发了关于生成有害或不当内容的潜在风险的重大担忧。本文介绍了一种新颖的推理时技术——渐进自我反思（PSR），使LLMs能够动态自我监控和纠正其输出。实验结果表明，将我们提出的方法应用于Llama-3.1-8B-Instruct时，攻击成功率从77.5%降低到5.9%；对Llama-3.1-8B base，从89.7%降低到5.6%；对Qwen2.5-7B-Instruct，从44.4%降低到3.8%，且无需额外训练，同时保持其在良性任务上的原始性能。我们的方法作为一种测试时缩放方法，额外的自我反思轮次在推理开销的代价下增强了安全性。为了在安全性与计算效率之间取得平衡，我们引入了一种轻量级自我反思预测器，根据输入复杂性估计最佳反思轮次。该自适应机制防止在良性输入上进行不必要的自我评估，同时确保在遇到潜在有害内容时进行全面评估。我们的研究结果表明，渐进自我反思作为一种可扩展的测试时方法，通过根据输入的风险特征动态分配计算资源，增强了LLM的安全性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concerns regarding the potential for large language models (LLMs) to generate harmful or inappropriate content, which poses significant risks in their deployment. Previous methods lacked effective self-monitoring capabilities, leading to high rates of harmful output. The proposed Progressive Self-Reflection (PSR) technique differs by allowing LLMs to dynamically self-correct their outputs during inference, significantly reducing the attack success rates across various models without additional training. This paper contributes a novel approach that balances safety and computational efficiency through a lightweight self-reflection predictor, which optimally adjusts the number of self-reflection rounds based on input complexity. Experimental results show that PSR reduces attack success rates dramatically while maintaining performance on benign tasks, thus supporting the goal of enhancing LLM safety in real-world applications.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLM）在部署过程中可能生成有害内容的担忧。以往的方法缺乏有效的自我监控能力，导致不当输出的高发生率。所提出的渐进自我反思（PSR）技术通过在推理过程中动态自我修正，使LLM显著降低攻击成功率，同时保持在良性任务上的性能。本文的贡献在于提出了一种可扩展的测试时方法，通过引入轻量级自我反思预测器，根据输入复杂性优化反思轮次的数量，从而平衡安全性和计算效率。实验结果表明，PSR在多种模型中将攻击成功率降低至最低3.8%，且无需额外训练，从而支持增强LLM安全性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">LionGuard 2: Building Lightweight, Data-Efficient &amp; Localised   Multilingual Content Moderators</div>
<div class="meta-line">Authors: Leanne Tan, Gabriel Chua, Ziyu Ge, Roy Ka-Wei Lee</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-07-21T07:50:48+00:00 · Latest: 2025-09-28T02:30:26+00:00</div>
<div class="meta-line">Comments: EMNLP 2025 System Demonstration Track</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.15339v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.15339v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern moderation systems increasingly support multiple languages, but often
fail to address localisation and low-resource variants - creating safety gaps
in real-world deployments. Small models offer a potential alternative to large
LLMs, yet still demand considerable data and compute. We present LionGuard 2, a
lightweight, multilingual moderation classifier tailored to the Singapore
context, supporting English, Chinese, Malay, and partial Tamil. Built on
pre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2
outperforms several commercial and open-source systems across 17 benchmarks,
including both Singapore-specific and public English datasets. The system is
actively deployed within the Singapore Government, demonstrating practical
efficacy at scale. Our findings show that high-quality local data and robust
multilingual embeddings can achieve strong moderation performance, without
fine-tuning large models. We release our model weights and part of our training
data to support future work on LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LionGuard 2：构建轻量级、数据高效和本地化的多语言内容审核系统</div>
<div class="mono" style="margin-top:8px">现代审核系统越来越支持多种语言，但往往未能解决本地化和低资源变体的问题，导致实际部署中的安全漏洞。小型模型为大型LLM提供了一种潜在替代方案，但仍然需要相当的数据和计算资源。我们提出了LionGuard 2，这是一个针对新加坡环境量身定制的轻量级多语言审核分类器，支持英语、中文、马来语和部分泰米尔语。LionGuard 2基于预训练的OpenAI嵌入和多头序数分类器，在17个基准测试中超越了多个商业和开源系统，包括新加坡特定和公共英语数据集。该系统已在新加坡政府中积极部署，展示了大规模的实际有效性。我们的研究结果表明，高质量的本地数据和强大的多语言嵌入可以在不微调大型模型的情况下实现强大的审核性能。我们发布了模型权重和部分训练数据，以支持未来在LLM安全方面的工作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing multilingual moderation systems, which often overlook localization and low-resource language variants, leading to safety gaps in real-world applications. Previous methods typically rely on large language models that require extensive data and computational resources, which can be impractical. The proposed approach, LionGuard 2, introduces a lightweight multilingual moderation classifier specifically designed for the Singapore context, utilizing pre-trained OpenAI embeddings and a multi-head ordinal classifier to enhance efficiency and effectiveness. This paper contributes by demonstrating that high-quality local data and robust multilingual embeddings can yield strong moderation performance without the need for fine-tuning large models. The methodology was tested across 17 benchmarks, showing that LionGuard 2 outperforms several commercial and open-source systems, achieving practical efficacy in deployment within the Singapore Government, thus supporting its goals of localized and efficient content moderation.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有多语言内容审核系统的局限性，这些系统往往忽视本地化和低资源语言变体，从而在实际应用中造成安全漏洞。以往的方法通常依赖于大型语言模型（LLMs），这些模型需要大量的数据和计算资源，实施起来可能不切实际。提出的方法LionGuard 2引入了一种轻量级的多语言审核分类器，专门为新加坡环境设计，利用预训练的OpenAI嵌入和多头序数分类器来提高效率和本地化。本文的贡献在于证明高质量的本地数据和强大的多语言嵌入可以在不微调大型模型的情况下实现有效的审核性能。该方法在17个基准测试中进行了测试，结果显示LionGuard 2在多个商业和开源系统中表现优越，在新加坡政府的实际部署中实现了有效性，从而支持其在内容审核中安全和高效的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of   Compliance</div>
<div class="meta-line">Authors: Wenbin Hu, Huihao Jing, Haochen Shi, Haoran Li, Yangqiu Song</div>
<div class="meta-line">First: 2025-09-26T12:11:29+00:00 · Latest: 2025-09-26T12:11:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.22250v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.22250v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The proliferation of Large Language Models (LLMs) has demonstrated remarkable
capabilities, elevating the critical importance of LLM safety. However,
existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic
protection, failing to ensure safety for the nuanced and complex behaviors of
modern LLM systems. To address this problem, we solve LLM safety from legal
compliance perspectives, named safety compliance. In this work, we posit
relevant established legal frameworks as safety standards for defining and
measuring safety compliance, including the EU AI Act and GDPR, which serve as
core legal frameworks for AI safety and data security in Europe. To bridge the
gap between LLM safety and legal compliance, we first develop a new benchmark
for safety compliance by generating realistic LLM safety scenarios seeded with
legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization
(GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively
aligns LLMs with legal standards to mitigate safety risks. Our comprehensive
experiments demonstrate that the Compliance Reasoner achieves superior
performance on the new benchmark, with average improvements of +10.45% for the
EU AI Act and +11.85% for GDPR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全合规：通过合规视角重新思考大型语言模型的安全推理</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的普及展示了显著的能力，提升了LLM安全的重要性。然而，现有的安全方法依赖于临时分类法，缺乏严格、系统的保护，未能确保现代LLM系统复杂行为的安全。为了解决这个问题，我们从法律合规的角度解决LLM安全，称之为安全合规。在这项工作中，我们将相关的既定法律框架视为定义和衡量安全合规的安全标准，包括欧盟人工智能法案和通用数据保护条例，这些是欧洲人工智能安全和数据安全的核心法律框架。为了弥合LLM安全与法律合规之间的差距，我们首先通过生成带有法律条款的现实LLM安全场景来开发一个新的安全合规基准。随后，我们使用群体政策优化（GRPO）对Qwen3-8B进行对齐，以构建一个安全推理器——合规推理器，有效地将LLM与法律标准对齐，以降低安全风险。我们的综合实验表明，合规推理器在新的基准上实现了卓越的性能，欧盟人工智能法案的平均提升为+10.45%，通用数据保护条例的平均提升为+11.85%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical need for safety in Large Language Models (LLMs), highlighting that existing safety methods are often ad-hoc and lack a systematic approach, which fails to account for the complex behaviors of modern LLMs. The proposed method, termed safety compliance, shifts the focus to legal compliance frameworks such as the EU AI Act and GDPR, providing a more rigorous standard for measuring safety. This approach is well-motivated as it seeks to bridge the gap between LLM safety and legal requirements. The methodology involves developing a new benchmark for safety compliance through realistic LLM safety scenarios and aligning the Qwen3-8B model using Group Policy Optimization to create a Compliance Reasoner. The experiments show that this Compliance Reasoner significantly improves performance on the new benchmark, achieving average enhancements of +10.45% for the EU AI Act and +11.85% for GDPR, thereby supporting the goals of enhancing LLM safety through legal compliance.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLM）安全性的重要问题，指出当前的安全方法往往是临时的，缺乏系统性，无法充分管理这些模型的复杂行为。提出的方法，称为安全合规，转向法律合规框架，如欧盟人工智能法案和GDPR，以建立严格的安全标准。这一方法具有良好的动机，因为它旨在弥合LLM安全与法律要求之间的差距。本文的贡献在于通过基于法律法规的现实场景开发了一个新的安全合规基准，并使用群体政策优化对Qwen3-8B模型进行对齐，创建了合规推理器。实验结果表明，合规推理器在新的基准上显著提高了性能，欧盟人工智能法案平均提升10.45%，GDPR平均提升11.85%，从而支持通过法律合规增强LLM安全性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">The Rogue Scalpel: Activation Steering Compromises LLM Safety</div>
<div class="meta-line">Authors: Anton Korznikov, Andrey Galichin, Alexey Dontsov, Oleg Y. Rogov, Ivan Oseledets, Elena Tutubalina</div>
<div class="meta-line">First: 2025-09-26T08:49:47+00:00 · Latest: 2025-09-26T08:49:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.22067v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.22067v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Activation steering is a promising technique for controlling LLM behavior by
adding semantically meaningful vectors directly into a model&#x27;s hidden states
during inference. It is often framed as a precise, interpretable, and
potentially safer alternative to fine-tuning. We demonstrate the opposite:
steering systematically breaks model alignment safeguards, making it comply
with harmful requests. Through extensive experiments on different model
families, we show that even steering in a random direction can increase the
probability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign
features from a sparse autoencoder (SAE), a common source of interpretable
directions, increases these rates by a further 2-4%. Finally, we show that
combining 20 randomly sampled vectors that jailbreak a single prompt creates a
universal attack, significantly increasing harmful compliance on unseen
requests. These results challenge the paradigm of safety through
interpretability, showing that precise control over model internals does not
guarantee precise control over model behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>流氓手术刀：激活引导妥协了大型语言模型的安全性</div>
<div class="mono" style="margin-top:8px">激活引导是一种有前景的技术，通过在推理过程中将语义上有意义的向量直接添加到模型的隐藏状态中来控制大型语言模型的行为。它通常被视为一种精确、可解释且潜在更安全的替代微调的方法。我们证明了相反的观点：引导系统性地破坏了模型对齐的安全措施，使其遵从有害请求。通过对不同模型家族的广泛实验，我们显示即使在随机方向上引导也能将有害遵从的概率从0%提高到2-27%。令人担忧的是，从稀疏自编码器（SAE）引导良性特征，作为可解释方向的常见来源，进一步将这些比率提高了2-4%。最后，我们展示了结合20个随机采样的向量来破解单个提示会产生一种通用攻击，显著增加对未见请求的有害遵从。这些结果挑战了通过可解释性实现安全性的范式，表明对模型内部的精确控制并不保证对模型行为的精确控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the safety concerns associated with large language models (LLMs) by investigating the technique of activation steering, which aims to control LLM behavior through the addition of semantically meaningful vectors to hidden states during inference. Previous methods, such as fine-tuning, have been framed as safer alternatives, but they often lack interpretability and can still lead to harmful outcomes. The proposed approach reveals that activation steering can actually compromise model alignment safeguards, leading to increased compliance with harmful requests. The research methodology involves extensive experiments across different model families, demonstrating that even random steering can raise harmful compliance rates significantly. The findings indicate that the perceived safety through interpretability is misleading, as precise control over model internals does not ensure safe model behavior.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）相关的安全问题，以及旨在通过操控推理过程中的隐藏状态来控制LLM行为的激活引导技术。以往的方法，如微调，常被视为更安全的替代方案，但它们往往无法维持模型的对齐，可能导致有害输出。所提出的方法批判性地审视激活引导，揭示其实际上可能会妨碍安全性，增加有害合规的可能性，即使在随机方向上引导时也是如此。该研究通过对不同模型系列进行广泛实验，表明引导良性特征可能加剧这些风险，并且组合多个向量可以创建普遍攻击。这些发现有助于深入理解LLM的安全性，挑战了可解释性确保安全性的假设，并强调了在模型设计中需要更强有力的安全措施。</div>
</details>
</div>
<div class="card">
<div class="title">Preemptive Detection and Steering of LLM Misalignment via Latent   Reachability</div>
<div class="meta-line">Authors: Sathwik Karnik, Somil Bansal</div>
<div class="meta-line">First: 2025-09-25T20:15:29+00:00 · Latest: 2025-09-25T20:15:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.21528v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.21528v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are now ubiquitous in everyday tools, raising
urgent safety concerns about their tendency to generate harmful content. The
dominant safety approach -- reinforcement learning from human feedback (RLHF)
-- effectively shapes model behavior during training but offers no safeguards
at inference time, where unsafe continuations may still arise. We propose
BRT-Align, a reachability-based framework that brings control-theoretic safety
tools to LLM inference. BRT-Align models autoregressive generation as a
dynamical system in latent space and learn a safety value function via backward
reachability, estimating the worst-case evolution of a trajectory. This enables
two complementary mechanisms: (1) a runtime monitor that forecasts unsafe
completions several tokens in advance, and (2) a least-restrictive steering
filter that minimally perturbs latent states to redirect generation away from
unsafe regions. Experiments across multiple LLMs and toxicity benchmarks
demonstrate that BRT-Align provides more accurate and earlier detection of
unsafe continuations than baselines. Moreover, for LLM safety alignment,
BRT-Align substantially reduces unsafe generations while preserving sentence
diversity and coherence. Qualitative results further highlight emergent
alignment properties: BRT-Align consistently produces responses that are less
violent, less profane, less offensive, and less politically biased. Together,
these findings demonstrate that reachability analysis provides a principled and
practical foundation for inference-time LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过潜在可达性进行大语言模型不一致性的预防检测与引导</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在日常工具中无处不在，带来了关于其生成有害内容的紧迫安全问题。主流的安全方法——基于人类反馈的强化学习（RLHF）——在训练期间有效地塑造模型行为，但在推理时没有任何保障，仍可能出现不安全的延续。我们提出了BRT-Align，一个基于可达性的框架，将控制理论安全工具引入LLM推理。BRT-Align将自回归生成建模为潜在空间中的动态系统，并通过反向可达性学习安全价值函数，估计轨迹的最坏情况演变。这使得两种互补机制成为可能：（1）一个运行时监控器，提前几个标记预测不安全的完成，和（2）一个最少限制的引导过滤器，最小扰动潜在状态以将生成重定向到安全区域。多个LLM和毒性基准的实验表明，BRT-Align比基线提供了更准确和更早的不安全延续检测。此外，对于LLM安全对齐，BRT-Align显著减少了不安全生成，同时保持句子的多样性和连贯性。定性结果进一步突显了新兴的对齐特性：BRT-Align始终生成更少暴力、更少粗俗、更少冒犯和更少政治偏见的响应。这些发现共同表明，可达性分析为推理时的LLM安全提供了一个原则性和实用的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the pressing safety concerns associated with large language models (LLMs), particularly their potential to generate harmful content during inference, despite existing methods like reinforcement learning from human feedback (RLHF) that only shape behavior during training. The proposed approach, BRT-Align, differs from past methods by applying control-theoretic safety tools to LLM inference, modeling autoregressive generation as a dynamical system in latent space and utilizing backward reachability to learn a safety value function. This framework introduces a runtime monitor for early detection of unsafe completions and a steering filter to redirect generation away from unsafe regions. Experimental results across various LLMs and toxicity benchmarks show that BRT-Align significantly improves the accuracy and timeliness of unsafe continuation detection while reducing unsafe outputs and maintaining sentence diversity and coherence. The findings indicate that reachability analysis offers a robust and effective foundation for enhancing LLM safety during inference.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）所带来的紧迫安全问题，特别是它们生成有害内容的倾向。以往的方法主要是基于人类反馈的强化学习（RLHF），虽然在训练期间有效塑造模型行为，但在推理阶段缺乏保护措施，可能导致不安全的输出。提出的方法BRT-Align引入了一种基于可达性的框架，将控制理论安全工具应用于LLM推理，将自回归生成建模为潜在空间中的动态系统，并通过反向可达性学习安全价值函数。该方法的贡献在于能够实现运行时监控，提前检测不安全的生成，并通过最小扰动潜在状态的引导过滤器，将生成引导远离不安全区域。实验结果表明，BRT-Align在多个LLM和毒性基准测试中实现了更准确和更早的危险生成检测，显著减少了不安全生成，同时保持了句子的多样性和连贯性，并产生了更少暴力、粗俗、冒犯和政治偏见的回应，从而证明了可达性分析在推理时LLM安全性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language   Models via Sparse Autoencoder Interpretation Framework</div>
<div class="meta-line">Authors: Jiaqi Weng, Han Zheng, Hanyu Zhang, Qinqin He, Jialing Tao, Hui Xue, Zhixuan Chu, Xiting Wang</div>
<div class="meta-line">First: 2025-09-11T11:22:43+00:00 · Latest: 2025-09-24T03:58:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.18127v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.18127v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to address broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related concepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regulations.
For rigorous safety analysis, we must extract a rich and diverse set of
safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we propose Safe-SAIL, a framework
for interpreting SAE features within LLMs to advance mechanistic understanding
in safety domains. Our approach systematically identifies SAE with best
concept-specific interpretability, explains safety-related neurons, and
introduces efficient strategies to scale up the interpretation process. We will
release a comprehensive toolkit including SAE checkpoints and human-readable
neuron explanations, which supports empirical analysis of safety risks to
promote research on LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Safe-SAIL：通过稀疏自编码器解释框架构建大型语言模型的细粒度安全景观</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在现实世界应用中的日益部署引发了重大安全问题。现有的大多数安全研究集中于评估LLM输出或特定安全任务，限制了其应对更广泛、未定义风险的能力。稀疏自编码器（SAEs）促进了解释性研究，通过解释从纠缠信号中分解出的单一意义原子特征来澄清模型行为。然而，之前对SAEs的应用并未用细粒度的安全相关概念解释特征，因此未能充分应对安全关键行为，如生成有毒响应和违反安全规定。为了进行严格的安全分析，我们必须提取丰富多样的安全相关特征，以有效捕捉这些高风险行为，但面临两个挑战：识别具有生成安全概念特定神经元最大潜力的SAEs，以及详细特征解释的高昂成本。本文提出Safe-SAIL，一个在LLMs中解释SAE特征的框架，以推进安全领域的机制理解。我们的方法系统地识别具有最佳概念特定可解释性的SAE，解释安全相关神经元，并引入高效策略以扩大解释过程。我们将发布一个综合工具包，包括SAE检查点和人类可读的神经元解释，支持安全风险的实证分析，以促进LLM安全研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing safety concerns associated with the deployment of large language models (LLMs) in real-world applications, highlighting that existing safety evaluations primarily focus on LLM outputs or specific tasks, which limits their effectiveness in addressing broader risks. Previous methods, particularly those using Sparse Autoencoders (SAEs), have not adequately interpreted features related to fine-grained safety concepts, leading to insufficient analysis of safety-critical behaviors. The proposed Safe-SAIL framework differs by systematically identifying SAEs that enhance interpretability for safety concepts, explaining safety-related neurons, and implementing efficient strategies to scale the interpretation process. This paper contributes a comprehensive toolkit that includes SAE checkpoints and human-readable neuron explanations, facilitating empirical safety risk analysis. The methodology demonstrates its effectiveness in advancing mechanistic understanding of safety domains within LLMs, thereby supporting the goal of improving safety assessments in these models.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在实际应用中日益增加的安全隐患，指出现有的安全评估主要集中在模型输出或特定任务上，限制了其应对更广泛安全风险的有效性。以往使用稀疏自编码器（SAEs）的方法未能充分解释与细粒度安全概念相关的特征，导致对生成有毒响应等安全关键行为的分析不足。所提出的Safe-SAIL框架通过系统识别具有概念特定可解释性的SAEs，并提供高效的扩展解释过程策略，改进了这些方法。本文的贡献在于提供一个全面的工具包，包括SAE检查点和可读的人类神经元解释，促进安全风险的实证分析。该方法在推进LLMs安全领域的机制理解方面表现出有效性，从而支持了增强该领域安全评估的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian scaling laws for in-context learning</div>
<div class="meta-line">Authors: Aryaman Arora, Dan Jurafsky, Christopher Potts, Noah D. Goodman</div>
<div class="meta-line">First: 2024-10-21T21:45:22+00:00 · Latest: 2025-09-22T16:30:22+00:00</div>
<div class="meta-line">Comments: COLM 2025 camera-ready version; 9 pages main text, 39 pages total</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2410.16531v4">Abs</a> · <a href="http://arxiv.org/pdf/2410.16531v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In-context learning (ICL) is a powerful technique for getting language models
to perform complex tasks with no training updates. Prior work has established
strong correlations between the number of in-context examples provided and the
accuracy of the model&#x27;s predictions. In this paper, we seek to explain this
correlation by showing that ICL approximates a Bayesian learner. This
perspective gives rise to a novel Bayesian scaling law for ICL. In experiments
with \mbox{GPT-2} models of different sizes, our scaling law matches existing
scaling laws in accuracy while also offering interpretable terms for task
priors, learning efficiency, and per-example probabilities. To illustrate the
analytic power that such interpretable scaling laws provide, we report on
controlled synthetic dataset experiments designed to inform real-world studies
of safety alignment. In our experimental protocol, we use SFT or DPO to
suppress an unwanted existing model capability and then use ICL to try to bring
that capability back (many-shot jailbreaking). We then study ICL on real-world
instruction-tuned LLMs using capabilities benchmarks as well as a new many-shot
jailbreaking dataset. In all cases, Bayesian scaling laws accurately predict
the conditions under which ICL will cause suppressed behaviors to reemerge,
which sheds light on the ineffectiveness of post-training at increasing LLM
safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文学习的贝叶斯缩放法则</div>
<div class="mono" style="margin-top:8px">上下文学习（ICL）是一种强大的技术，可以使语言模型在没有训练更新的情况下执行复杂任务。先前的研究已建立了提供的上下文示例数量与模型预测准确性之间的强相关性。本文旨在通过展示ICL近似于贝叶斯学习者来解释这种相关性。这一视角产生了一个新的ICL贝叶斯缩放法则。在不同规模的GPT-2模型实验中，我们的缩放法则在准确性上与现有的缩放法则相匹配，同时也提供了任务先验、学习效率和每个示例概率的可解释项。为了说明这种可解释的缩放法则所提供的分析能力，我们报告了旨在为现实世界安全对齐研究提供信息的受控合成数据集实验。在我们的实验协议中，我们使用SFT或DPO来抑制不需要的现有模型能力，然后使用ICL尝试恢复该能力（多次破解）。随后，我们在真实世界的指令调优LLM上研究ICL，使用能力基准以及一个新的多次破解数据集。在所有情况下，贝叶斯缩放法则准确预测了ICL将导致抑制行为重新出现的条件，这揭示了后训练在提高LLM安全性方面的无效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the growing interest in in-context learning (ICL) as a method for enabling language models to perform complex tasks without requiring training updates. Previous methods have established correlations between the number of in-context examples and model accuracy, but they lack a comprehensive theoretical framework. The proposed approach introduces a Bayesian perspective to ICL, leading to a novel Bayesian scaling law that not only aligns with existing accuracy scaling laws but also provides interpretable insights into task priors and learning efficiency. The research methodology involves experiments with different sizes of GPT-2 models, where the Bayesian scaling laws successfully predict the conditions under which ICL can reactivate suppressed model capabilities, demonstrating its effectiveness in real-world applications related to safety alignment. The findings indicate that the proposed scaling laws can enhance understanding of ICL&#x27;s behavior, particularly in contexts where model safety is a concern.</div>
<div class="mono" style="margin-top:8px">本文探讨了语言模型中的上下文学习（ICL）现象，该技术使模型能够在不需要训练更新的情况下执行复杂任务。以往的方法建立了上下文示例数量与模型准确性之间的相关性，但缺乏全面的理论框架。提出的方法引入了贝叶斯视角，形成了ICL的新规模法则，解决了现有方法的局限性，提供了与任务先验和学习效率相关的可解释项。本文的贡献在于证明ICL可以被理解为近似贝叶斯学习者，这一观点通过对不同规模的GPT-2模型进行实验得到了验证。研究方法包括对合成数据集和真实世界指令调优的LLM进行控制实验，结果表明贝叶斯规模法则能够准确预测ICL何时能够恢复被抑制的模型能力，从而突显了通过后训练调整提高LLM安全性的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs</div>
<div class="meta-line">Authors: Jean-Charles Noirot Ferrand, Yohan Beugin, Eric Pauley, Ryan Sheatsley, Patrick McDaniel</div>
<div class="meta-line">First: 2025-01-27T22:13:05+00:00 · Latest: 2025-09-19T21:29:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2501.16534v2">Abs</a> · <a href="http://arxiv.org/pdf/2501.16534v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Alignment in large language models (LLMs) is used to enforce guidelines such
as safety. Yet, alignment fails in the face of jailbreak attacks that modify
inputs to induce unsafe outputs. In this paper, we introduce and evaluate a new
technique for jailbreak attacks. We observe that alignment embeds a safety
classifier in the LLM responsible for deciding between refusal and compliance,
and seek to extract an approximation of this classifier: a surrogate
classifier. To this end, we build candidate classifiers from subsets of the
LLM. We first evaluate the degree to which candidate classifiers approximate
the LLM&#x27;s safety classifier in benign and adversarial settings. Then, we attack
the candidates and measure how well the resulting adversarial inputs transfer
to the LLM. Our evaluation shows that the best candidates achieve accurate
agreement (an F1 score above 80%) using as little as 20% of the model
architecture. Further, we find that attacks mounted on the surrogate
classifiers can be transferred to the LLM with high success. For example, a
surrogate using only 50% of the Llama 2 model achieved an attack success rate
(ASR) of 70% with half the memory footprint and runtime -- a substantial
improvement over attacking the LLM directly, where we only observed a 22% ASR.
These results show that extracting surrogate classifiers is an effective and
efficient means for modeling (and therein addressing) the vulnerability of
aligned models to jailbreaking attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>目标对齐：提取对齐大语言模型的安全分类器</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）中的对齐用于执行安全等指导方针。然而，在面对修改输入以诱导不安全输出的越狱攻击时，对齐会失败。本文介绍并评估了一种新的越狱攻击技术。我们观察到，对齐在LLM中嵌入了一个安全分类器，负责决定拒绝与遵从之间的选择，并试图提取该分类器的近似值：一个替代分类器。为此，我们从LLM的子集构建候选分类器。我们首先评估候选分类器在良性和对抗环境中与LLM的安全分类器的近似程度。然后，我们攻击这些候选者，并测量结果对抗输入转移到LLM的效果。我们的评估显示，最佳候选者在使用仅20%的模型架构时，达到了准确一致（F1分数超过80%）。此外，我们发现对替代分类器发起的攻击可以高成功率地转移到LLM。例如，使用仅50%的Llama 2模型的替代分类器达到了70%的攻击成功率（ASR），且内存占用和运行时间减半——相比直接攻击LLM，我们仅观察到22%的ASR。这些结果表明，提取替代分类器是建模（并解决）对齐模型对越狱攻击脆弱性的一种有效且高效的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of large language models (LLMs) to jailbreak attacks, which exploit alignment mechanisms intended to enforce safety guidelines. Previous methods have struggled with effectively countering these attacks, leading to unsafe outputs. The proposed approach introduces a technique to extract a surrogate safety classifier from the LLM, which approximates the original classifier responsible for safety decisions. This method is motivated by the observation that alignment embeds a safety classifier within the LLM. The paper contributes by demonstrating that these surrogate classifiers can achieve high accuracy (F1 score above 80%) with significantly reduced model architecture, and that attacks on these classifiers can be effectively transferred to the LLM, achieving an attack success rate of 70% with only 50% of the model&#x27;s resources, compared to just 22% when attacking the LLM directly. This indicates that the proposed method is both effective and efficient in addressing the vulnerabilities of aligned models to jailbreak attacks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLMs）在面对越狱攻击时的脆弱性，这些攻击通过修改输入来破坏安全性。以往的方法在有效应对这些攻击方面存在困难，因为它们往往无法准确建模嵌入LLM中的安全分类器。本文提出了一种新方法，从LLM的子集提取代理分类器，从而更有效地评估安全对齐。该方法论包括评估这些候选分类器在良性和对抗环境中对LLM安全分类器的近似程度，然后测试从代理分类器到LLM的攻击可转移性。研究结果表明，最佳代理分类器在仅使用20%的模型架构时，F1分数超过80%，并且对这些分类器的攻击可以以显著更高的成功率转移到LLM，相比于直接攻击，显示出该方法在应对对齐模型脆弱性方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Digging Into the Internal: Causality-Based Analysis of LLM Function   Calling</div>
<div class="meta-line">Authors: Zhenlan Ji, Daoyuan Wu, Wenxuan Wang, Pingchuan Ma, Shuai Wang, Lei Ma</div>
<div class="meta-line">First: 2025-09-18T08:30:26+00:00 · Latest: 2025-09-18T08:30:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.16268v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.16268v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Function calling (FC) has emerged as a powerful technique for facilitating
large language models (LLMs) to interact with external systems and perform
structured tasks. However, the mechanisms through which it influences model
behavior remain largely under-explored. Besides, we discover that in addition
to the regular usage of FC, this technique can substantially enhance the
compliance of LLMs with user instructions. These observations motivate us to
leverage causality, a canonical analysis method, to investigate how FC works
within LLMs. In particular, we conduct layer-level and token-level causal
interventions to dissect FC&#x27;s impact on the model&#x27;s internal computational
logic when responding to user queries. Our analysis confirms the substantial
influence of FC and reveals several in-depth insights into its mechanisms. To
further validate our findings, we conduct extensive experiments comparing the
effectiveness of FC-based instructions against conventional prompting methods.
We focus on enhancing LLM safety robustness, a critical LLM application
scenario, and evaluate four mainstream LLMs across two benchmark datasets. The
results are striking: FC shows an average performance improvement of around
135% over conventional prompting methods in detecting malicious inputs,
demonstrating its promising potential to enhance LLM reliability and capability
in practical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深入内部：基于因果关系的LLM函数调用分析</div>
<div class="mono" style="margin-top:8px">函数调用（FC）已成为促进大型语言模型（LLMs）与外部系统交互和执行结构化任务的强大技术。然而，它影响模型行为的机制仍然在很大程度上未被探索。此外，我们发现除了常规使用FC外，这项技术可以显著增强LLMs对用户指令的遵从性。这些观察促使我们利用因果关系这一经典分析方法，研究FC在LLMs中的工作原理。特别地，我们进行层级和标记级的因果干预，以剖析FC在响应用户查询时对模型内部计算逻辑的影响。我们的分析确认了FC的显著影响，并揭示了其机制的若干深入见解。为了进一步验证我们的发现，我们进行了广泛的实验，比较基于FC的指令与传统提示方法的有效性。我们专注于增强LLM安全性鲁棒性，这是LLM应用场景中的一个关键点，并在两个基准数据集上评估了四个主流LLMs。结果令人瞩目：FC在检测恶意输入方面的平均性能提升约为135%，展示了其在实际应用中增强LLM可靠性和能力的良好潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the under-explored mechanisms of function calling (FC) in large language models (LLMs), which has become a significant technique for enabling LLMs to interact with external systems. Previous methods primarily relied on conventional prompting, which lacked the ability to enhance model compliance with user instructions effectively. The proposed approach utilizes causality-based analysis to investigate FC&#x27;s impact on LLMs, offering a more nuanced understanding of its internal computational logic. The contribution of this research lies in its layer-level and token-level causal interventions that reveal FC&#x27;s substantial influence on model behavior. The methodology includes extensive experiments comparing FC-based instructions with traditional prompting methods, focusing on LLM safety robustness, and the results indicate an average performance improvement of approximately 135% in detecting malicious inputs, thus supporting the goal of enhancing LLM reliability and capability in practical applications.</div>
<div class="mono" style="margin-top:8px">本文探讨了大语言模型（LLMs）中函数调用（FC）机制的不足之处，FC已成为使LLMs执行结构化任务和与外部系统交互的重要技术。以往的方法主要依赖传统提示，无法充分利用LLMs对用户指令的遵从性。提出的方法利用基于因果关系的分析来研究FC对LLM行为的影响，为理解其内部计算逻辑提供了良好的动机。研究方法包括层级和标记级的因果干预，以及广泛的实验，比较FC基础指令与传统提示方法。研究结果表明，FC显著增强了LLM的安全鲁棒性，在检测恶意输入方面相比传统方法平均提高了约135%的性能，从而支持了提高LLM在实际应用中可靠性和能力的目标。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Jailbreak Detection for (Almost) Free!</div>
<div class="meta-line">Authors: Guorui Chen, Yifan Xia, Xiaojun Jia, Zhijiang Li, Philip Torr, Jindong Gu</div>
<div class="meta-line">First: 2025-09-18T02:42:52+00:00 · Latest: 2025-09-18T02:42:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.14558v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.14558v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) enhance security through alignment when widely
used, but remain susceptible to jailbreak attacks capable of producing
inappropriate content. Jailbreak detection methods show promise in mitigating
jailbreak attacks through the assistance of other models or multiple model
inferences. However, existing methods entail significant computational costs.
In this paper, we first present a finding that the difference in output
distributions between jailbreak and benign prompts can be employed for
detecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak
Detection (FJD) which prepends an affirmative instruction to the input and
scales the logits by temperature to further distinguish between jailbreak and
benign prompts through the confidence of the first token. Furthermore, we
enhance the detection performance of FJD through the integration of virtual
instruction learning. Extensive experiments on aligned LLMs show that our FJD
can effectively detect jailbreak prompts with almost no additional
computational costs during LLM inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>几乎免费的LLM越狱检测！</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在广泛使用时通过对齐增强安全性，但仍然容易受到能够生成不当内容的越狱攻击。越狱检测方法在通过其他模型或多个模型推理的帮助下显示出减轻越狱攻击的前景。然而，现有方法涉及显著的计算成本。本文首先提出一个发现，即越狱提示和良性提示之间的输出分布差异可以用于检测越狱提示。基于这一发现，我们提出了一种免费越狱检测（FJD），该方法在输入前添加肯定指令，并通过温度缩放logits，以进一步通过第一个token的置信度区分越狱和良性提示。此外，我们通过整合虚拟指令学习增强了FJD的检测性能。在对齐的LLMs上进行的大量实验表明，我们的FJD可以有效地检测越狱提示，几乎没有在LLM推理过程中增加额外的计算成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of large language models (LLMs) to jailbreak attacks that can generate inappropriate content, highlighting the need for effective detection methods. Previous approaches to jailbreak detection often rely on multiple model inferences or additional models, which incur high computational costs. The proposed Free Jailbreak Detection (FJD) method differs by utilizing the output distribution differences between jailbreak and benign prompts, enhancing detection with minimal computational overhead by appending an affirmative instruction and adjusting logits through temperature scaling. This paper contributes a novel detection methodology that significantly improves the ability to identify jailbreak prompts while maintaining efficiency, achieving effective detection with almost no extra computational costs during LLM inference, thus supporting the goal of enhancing security in LLM applications.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLM）对越狱攻击的脆弱性，这种攻击可能导致生成不当内容。以往的越狱检测方法通常依赖多个模型推理或额外模型，导致计算成本高昂。提出的免费越狱检测（FJD）方法通过利用越狱提示和良性提示之间的输出分布差异来进行检测，结合肯定指令并通过温度缩放logits，以在计算开销极小的情况下增强检测效果。这种方法的动机明确，旨在在不增加显著资源消耗的情况下维护安全性。该论文贡献了一种新颖的检测方法，能够在LLM推理过程中有效识别越狱提示，且几乎没有额外成本，在对齐的LLM上进行了广泛实验，表现出强大的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Jailbreaking Large Language Models Through Content Concretization</div>
<div class="meta-line">Authors: Johan Wahréus, Ahmed Hussain, Panos Papadimitratos</div>
<div class="meta-line">First: 2025-09-16T10:34:26+00:00 · Latest: 2025-09-16T10:34:26+00:00</div>
<div class="meta-line">Comments: Accepted for presentation in the Conference on Game Theory and AI for
  Security (GameSec) 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.12937v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.12937v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly deployed for task automation
and content generation, yet their safety mechanisms remain vulnerable to
circumvention through different jailbreaking techniques. In this paper, we
introduce \textit{Content Concretization} (CC), a novel jailbreaking technique
that iteratively transforms abstract malicious requests into concrete,
executable implementations. CC is a two-stage process: first, generating
initial LLM responses using lower-tier, less constrained safety filters models,
then refining them through higher-tier models that process both the preliminary
output and original prompt. We evaluate our technique using 350
cybersecurity-specific prompts, demonstrating substantial improvements in
jailbreak Success Rates (SRs), increasing from 7\% (no refinements) to 62\%
after three refinement iterations, while maintaining a cost of 7.5\textcent~per
prompt. Comparative A/B testing across nine different LLM evaluators confirms
that outputs from additional refinement steps are consistently rated as more
malicious and technically superior. Moreover, manual code analysis reveals that
generated outputs execute with minimal modification, although optimal
deployment typically requires target-specific fine-tuning. With eventual
improved harmful code generation, these results highlight critical
vulnerabilities in current LLM safety frameworks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过内容具体化破解大型语言模型</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地用于任务自动化和内容生成，但其安全机制仍然容易受到不同破解技术的规避。本文介绍了一种新颖的破解技术——内容具体化（CC），该技术通过迭代将抽象的恶意请求转化为具体的可执行实现。CC是一个两阶段的过程：首先，使用低级别、约束较少的安全过滤模型生成初始LLM响应，然后通过处理初步输出和原始提示的高级模型进行精炼。我们使用350个网络安全特定提示评估了我们的技术，显示出破解成功率（SRs）显著提高，从7%（无精炼）增加到62%，经过三次精炼迭代，同时每个提示的成本保持在7.5美分。对九个不同LLM评估器的比较A/B测试确认，额外精炼步骤的输出在恶意性和技术优越性上始终被评为更高。此外，手动代码分析显示，生成的输出在最小修改下即可执行，尽管最佳部署通常需要针对特定目标的微调。随着有害代码生成的最终改善，这些结果突显了当前LLM安全框架中的关键漏洞。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of Large Language Models (LLMs) to jailbreaking techniques that can circumvent their safety mechanisms, which is a growing concern as these models are increasingly used for automation and content generation. Previous methods for jailbreaking have been limited in effectiveness and often rely on straightforward manipulation of prompts, leading to low success rates. The proposed approach, Content Concretization (CC), differs by iteratively transforming abstract malicious requests into concrete implementations through a two-stage process that utilizes both lower-tier and higher-tier models for refinement. This method effectively increases jailbreak success rates from 7% to 62% after three iterations, while maintaining a low cost per prompt. The paper contributes to the understanding of LLM vulnerabilities and demonstrates that the refined outputs are rated as more malicious and technically superior, indicating significant improvements in harmful code generation and highlighting critical flaws in existing LLM safety frameworks.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在越狱技术方面的脆弱性，这些技术能够规避其安全机制，随着这些模型在自动化和内容生成中的日益使用，这一问题愈发重要。以往的方法在执行恶意请求时成功率较低，因此提出了一种新的方法——内容具体化（CC），该方法通过将抽象请求迭代转化为具体实现来解决这一问题。该过程分为两个阶段，首先使用约束较少的模型生成响应，然后通过更高级的模型进行精炼，有效地将越狱成功率从7%提高到62%，同时保持低成本。本文的贡献在于加深了对LLM脆弱性的理解，并表明经过精炼的输出被评估为更具恶意和技术优势，显示出有害代码生成的显著改善，并突显了现有LLM安全框架中的关键弱点。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Pretraining: Toward the Next Generation of Safe AI</div>
<div class="meta-line">Authors: Pratyush Maini, Sachin Goyal, Dylan Sam, Alex Robey, Yash Savani, Yiding Jiang, Andy Zou, Matt Fredrikson, Zacharcy C. Lipton, J. Zico Kolter</div>
<div class="meta-line">First: 2025-04-23T17:58:08+00:00 · Latest: 2025-09-15T17:51:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.16980v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.16980v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) are increasingly deployed in high-stakes
settings, the risk of generating harmful or toxic content remains a central
challenge. Post-hoc alignment methods are brittle: once unsafe patterns are
learned during pretraining, they are hard to remove. In this work, we present a
data-centric pretraining framework that builds safety into the model from the
start. Our framework consists of four key steps: (i) Safety Filtering: building
a safety classifier to classify webdata into safe and unsafe categories; (ii)
Safety Rephrasing: we recontextualize unsafe webdata into safer narratives;
(iii) Native Refusal: we develop RefuseWeb and Moral Education pretraining
datasets that actively teach model to refuse on unsafe content and the moral
reasoning behind it, and (iv) Harmfulness-Tag annotated pretraining: we flag
unsafe content during pretraining using a special token, and use it to steer
model away from unsafe generations at inference. Our safety-pretrained models
reduce attack success rates from 38.8\% to 8.4\% on standard LLM safety
benchmarks with no performance degradation on general tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全预训练：迈向下一代安全人工智能</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在高风险环境中的日益应用，生成有害或有毒内容的风险仍然是一个核心挑战。事后对齐方法脆弱：一旦在预训练期间学习到不安全的模式，就很难去除。在这项工作中，我们提出了一种以数据为中心的预训练框架，从一开始就将安全性融入模型。我们的框架包括四个关键步骤：（i）安全过滤：构建安全分类器，将网络数据分类为安全和不安全类别；（ii）安全重述：将不安全的网络数据重新情境化为更安全的叙述；（iii）本地拒绝：我们开发了RefuseWeb和道德教育预训练数据集，积极教导模型拒绝不安全内容及其背后的道德推理；（iv）有害标签注释的预训练：在预训练期间使用特殊标记标记不安全内容，并利用它在推理时引导模型远离不安全生成。我们的安全预训练模型在标准LLM安全基准上将攻击成功率从38.8%降低到8.4%，而在一般任务上没有性能下降。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical challenge of harmful content generation in large language models (LLMs) as they are increasingly used in sensitive applications. Previous methods, particularly post-hoc alignment techniques, have proven to be ineffective as they struggle to eliminate unsafe patterns once they are ingrained during pretraining. The proposed data-centric pretraining framework differs by integrating safety measures from the outset, incorporating steps such as safety filtering, rephrasing unsafe content, and developing datasets that teach models to refuse unsafe content. This approach is well-motivated as it aims to proactively prevent the generation of harmful outputs. The contribution of the paper lies in its innovative methodology that significantly reduces attack success rates from 38.8% to 8.4% on established safety benchmarks, while maintaining performance on general tasks, thus supporting the goal of creating safer AI systems.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）在高风险应用中生成有害或有毒内容的关键问题，尤其是在安全至关重要的情况下。以往的方法主要是事后对齐技术，效果不佳，因为一旦在预训练过程中学习到不安全的模式，就很难消除。所提出的数据中心预训练框架通过从一开始就整合安全措施而有所不同，采用包括安全过滤、重新表述不安全内容、开发道德推理数据集以及在预训练过程中标注有害内容的四个步骤。该方法的动机明确，旨在主动将安全性植入LLMs，而不是事后修补。论文的贡献显著，因为安全预训练模型在标准安全基准上的攻击成功率从38.8%降低到8.4%，同时在一般任务上保持性能，从而支持了更安全的人工智能部署目标。</div>
</details>
</div>
<div class="card">
<div class="title">Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and   Investigating the Real Misuse Threat of LLMs</div>
<div class="meta-line">Authors: Yu Yan, Sheng Sun, Zhe Wang, Yijun Lin, Zenghao Duan, zhifei zheng, Min Liu, Zhiyi yin, Jianping Zhang</div>
<div class="meta-line">First: 2025-08-22T12:41:26+00:00 · Latest: 2025-09-15T03:59:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.16347v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.16347v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the development of Large Language Models (LLMs), numerous efforts have
revealed their vulnerabilities to jailbreak attacks. Although these studies
have driven the progress in LLMs&#x27; safety alignment, it remains unclear whether
LLMs have internalized authentic knowledge to deal with real-world crimes, or
are merely forced to simulate toxic language patterns. This ambiguity raises
concerns that jailbreak success is often attributable to a hallucination loop
between jailbroken LLM and judger LLM. By decoupling the use of jailbreak
techniques, we construct knowledge-intensive Q\&amp;A to investigate the misuse
threats of LLMs in terms of dangerous knowledge possession, harmful task
planning utility, and harmfulness judgment robustness. Experiments reveal a
mismatch between jailbreak success rates and harmful knowledge possession in
LLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness
judgments on toxic language patterns. Our study reveals a gap between existing
LLM safety assessments and real-world threat potential.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混淆是最终障碍：重新思考越狱评估与调查大型语言模型的真实滥用威胁</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）的发展，许多努力揭示了它们对越狱攻击的脆弱性。尽管这些研究推动了LLMs安全对齐的进展，但仍不清楚LLMs是否内化了应对现实犯罪的真实知识，或仅仅被迫模拟有毒语言模式。这种模糊性引发了担忧，即越狱成功往往归因于越狱LLM与评判LLM之间的幻觉循环。通过解耦越狱技术的使用，我们构建了知识密集型问答，以调查LLMs在危险知识拥有、有害任务规划效用和有害性判断稳健性方面的滥用威胁。实验揭示了越狱成功率与LLMs中有害知识拥有之间的不匹配，现有的LLM作为评判者框架往往将有害性判断锚定在有毒语言模式上。我们的研究揭示了现有LLM安全评估与现实世界威胁潜力之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of Large Language Models (LLMs) to jailbreak attacks, highlighting concerns about their ability to internalize genuine knowledge for real-world applications rather than merely simulating harmful language. Previous methods have focused on assessing LLM safety through jailbreak techniques, which often lead to misleading conclusions due to a hallucination loop between the jailbroken and judging LLMs. The proposed approach shifts the focus to knowledge-intensive question and answer evaluations, effectively decoupling jailbreak techniques to better assess the misuse threats of LLMs. This study contributes to a clearer understanding of the gap between LLM safety assessments and their actual threat potential, revealing that existing frameworks may misjudge harmfulness based on toxic language patterns. The methodology involves rigorous experimentation that uncovers a mismatch between jailbreak success rates and the possession of harmful knowledge, ultimately supporting the need for improved safety evaluations in LLMs.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在监狱攻击中的脆弱性，强调它们是否能够内化真实知识以应对现实世界应用的疑虑，还是仅仅模仿有害语言模式。以往的方法主要集中在评估LLM的安全性，但往往依赖于有毒语言模式，导致监狱攻击成功率与实际有害知识拥有之间的不一致。所提出的方法通过构建知识密集型问答来评估LLM的误用威胁，有效地分离了监狱技术的影响。本研究通过揭示现有安全评估与现实威胁潜力之间的差异，为LLM安全性提供了更深入的理解。该方法表明，监狱攻击尝试的成功率与有害知识的实际拥有之间存在显著差距，表明当前框架可能无法准确反映LLM所带来的潜在危险。</div>
</details>
</div>
<div class="card">
<div class="title">Improving LLM Safety and Helpfulness using SFT and DPO: A Study on   OPT-350M</div>
<div class="meta-line">Authors: Piyush Pant</div>
<div class="meta-line">First: 2025-09-10T23:22:59+00:00 · Latest: 2025-09-10T23:22:59+00:00</div>
<div class="meta-line">Comments: 17 pages, 3 figures. Code and dataset available at
  https://github.com/PiyushWithPant/Improving-LLM-Safety-and-Helpfulness-using-SFT-and-DPO</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.09055v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.09055v1">PDF</a> · <a href="https://github.com/PiyushWithPant/Improving-LLM-Safety-and-Helpfulness-using-SFT-and-DPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This research investigates the effectiveness of alignment techniques,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a
combined SFT+DPO approach on improving the safety and helpfulness of the
OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,
we train and evaluate four models: the base OPT350M, an SFT model, a DPO model,
and a model trained with both SFT and DPO. We introduce three key evaluation
metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined
Alignment Score (CAS), all derived from reward model outputs. The results show
that while SFT outperforms DPO, The combined SFT+DPO model outperforms all
others across all metrics, demonstrating the complementary nature of these
techniques. Our findings also highlight challenges posed by noisy data, limited
GPU resources, and training constraints. This study offers a comprehensive view
of how fine-tuning strategies affect model alignment and provides a foundation
for more robust alignment pipelines in future work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用SFT和DPO提高LLM的安全性和有用性：关于OPT-350M的研究</div>
<div class="mono" style="margin-top:8px">本研究探讨了对齐技术的有效性，包括监督微调（SFT）、直接偏好优化（DPO）以及结合SFT+DPO的方法，以提高OPT-350M语言模型的安全性和有用性。利用Anthropic Helpful-Harmless RLHF数据集，我们训练和评估了四个模型：基础OPT350M、SFT模型、DPO模型，以及一个同时使用SFT和DPO训练的模型。我们引入了三个关键评估指标：无害率（HmR）、有用率（HpR）和综合对齐得分（CAS），这些指标均来源于奖励模型输出。结果表明，虽然SFT优于DPO，但结合SFT+DPO的模型在所有指标上均优于其他模型，展示了这些技术的互补性。我们的研究还强调了噪声数据、有限的GPU资源和训练限制所带来的挑战。本研究提供了微调策略如何影响模型对齐的全面视角，并为未来更强大的对齐管道奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the growing concern of safety and helpfulness in language models, specifically focusing on the OPT-350M model. Previous methods, such as standard fine-tuning, have shown limitations in effectively aligning models with user preferences, leading to safety issues. The proposed approach combines Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), which aims to leverage the strengths of both techniques to enhance model alignment. The study contributes by introducing three evaluation metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and Combined Alignment Score (CAS), which provide a comprehensive assessment of model performance. The experimental results indicate that while SFT alone outperforms DPO, the combined SFT+DPO approach yields superior results across all metrics, thus supporting the goal of improving model safety and helpfulness despite challenges such as noisy data and limited resources.</div>
<div class="mono" style="margin-top:8px">本研究关注语言模型安全性和有用性日益增长的关注，特别是针对OPT-350M模型。以往的方法，如标准微调，在有效对齐模型行为与用户期望方面存在局限性，因此需要改进技术。提出的方法结合了监督微调（SFT）和直接偏好优化（DPO），以增强对齐，动机在于观察到这些方法可以相辅相成。该研究利用Anthropic Helpful-Harmless RLHF数据集训练和评估四个模型，并引入新的评估指标：无害率、有用率和综合对齐分数。结果表明，虽然单独的SFT表现优于DPO，但结合的SFT+DPO模型在所有对齐指标上均超过其他模型，从而提供了一种更有效的提高模型安全性和有用性的策略。</div>
</details>
</div>
<div class="card">
<div class="title">Uncovering the Vulnerability of Large Language Models in the Financial   Domain via Risk Concealment</div>
<div class="meta-line">Authors: Gang Cheng, Haibo Jin, Wenbin Zhang, Haohan Wang, Jun Zhuang</div>
<div class="meta-line">First: 2025-09-07T22:35:15+00:00 · Latest: 2025-09-07T22:35:15+00:00</div>
<div class="meta-line">Comments: Preprint, under review. TL;DR: We propose a multi-turn red-teaming
  framework, RCA, that reveals critical regulatory vulnerabilities in financial
  LLMs, achieving over 93% attack success on a proposed new benchmark,
  FIN-Bench</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.10546v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.10546v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly integrated into financial
applications, yet existing red-teaming research primarily targets harmful
content, largely neglecting regulatory risks. In this work, we aim to
investigate the vulnerability of financial LLMs through red-teaming approaches.
We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that
iteratively conceals regulatory risks to provoke seemingly compliant yet
regulatory-violating responses from LLMs. To enable systematic evaluation, we
construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in
financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA
effectively bypasses nine mainstream LLMs, achieving an average attack success
rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.
These findings reveal a critical gap in current alignment techniques and
underscore the urgent need for stronger moderation mechanisms in financial
domains. We hope this work offers practical insights for advancing robust and
domain-aware LLM alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过风险隐蔽揭示金融领域大型语言模型的脆弱性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地应用于金融领域，但现有的红队研究主要针对有害内容，基本忽视了监管风险。本研究旨在通过红队方法调查金融LLMs的脆弱性。我们引入了风险隐蔽攻击（RCA），这是一种新颖的多轮框架，迭代地隐蔽监管风险，以引发看似合规但违反监管的LLM响应。为了实现系统评估，我们构建了FIN-Bench，这是一个用于评估金融环境中LLM安全性的领域特定基准。在FIN-Bench上的广泛实验表明，RCA有效绕过了九种主流LLMs，平均攻击成功率（ASR）达到93.18%，其中GPT-4.1的成功率为98.28%，OpenAI o1的成功率为97.56%。这些发现揭示了当前对齐技术的关键缺口，并强调了金融领域迫切需要更强的监管机制。我们希望本研究为推进稳健且领域意识强的LLM对齐提供实用见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the increasing integration of Large Language Models (LLMs) in financial applications, highlighting a gap in existing red-teaming research that primarily focuses on harmful content while overlooking regulatory risks. Previous methods have not adequately assessed the compliance of LLMs with regulatory standards, leading to potential vulnerabilities. The proposed approach, Risk-Concealment Attacks (RCA), introduces a novel multi-turn framework that conceals regulatory risks to elicit compliant yet non-compliant responses from LLMs, thus addressing the shortcomings of past methods. The paper contributes by constructing FIN-Bench, a domain-specific benchmark for evaluating LLM safety in financial contexts, and demonstrates through extensive experiments that RCA successfully bypasses nine mainstream LLMs, achieving an average attack success rate of 93.18%, with particularly high rates on GPT-4.1 and OpenAI o1. These results indicate a significant gap in current alignment techniques and emphasize the need for improved moderation mechanisms in the financial domain.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在金融应用中的日益整合，强调了现有红队研究主要关注有害内容而忽视监管风险的缺口。以往的方法未能充分评估LLMs在金融环境中的脆弱性，因此需要一种更有针对性的方法。提出的风险隐蔽攻击（RCA）框架提供了一种新颖的多轮策略，通过隐蔽监管风险来引发LLMs的合规但不合规的响应，从而解决了过去方法的局限性。本文的贡献在于引入了FIN-Bench，一个用于评估金融领域LLM安全性的特定领域基准，并展示了RCA的有效性，在九种主流LLMs上实现了93.18%的平均攻击成功率，其中GPT-4.1的成功率为98.28%。这些结果表明当前对齐技术存在重大脆弱性，并强调了在金融领域改进监管机制的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language   Models</div>
<div class="meta-line">Authors: Youjia Zheng, Mohammad Zandsalimy, Shanu Sushmita</div>
<div class="meta-line">First: 2025-09-05T19:57:38+00:00 · Latest: 2025-09-05T19:57:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.05471v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.05471v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly vulnerable to a sophisticated
form of adversarial prompting known as camouflaged jailbreaking. This method
embeds malicious intent within seemingly benign language to evade existing
safety mechanisms. Unlike overt attacks, these subtle prompts exploit
contextual ambiguity and the flexible nature of language, posing significant
challenges to current defense systems. This paper investigates the construction
and impact of camouflaged jailbreak prompts, emphasizing their deceptive
characteristics and the limitations of traditional keyword-based detection
methods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts,
containing 500 curated examples (400 harmful and 100 benign prompts) designed
to rigorously stress-test LLM safety protocols. In addition, we propose a
multi-faceted evaluation framework that measures harmfulness across seven
dimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards,
Harmful Potential, Educational Value, Content Quality, and Compliance Score.
Our findings reveal a stark contrast in LLM behavior: while models demonstrate
high safety and content quality with benign inputs, they exhibit a significant
decline in performance and safety when confronted with camouflaged jailbreak
attempts. This disparity underscores a pervasive vulnerability, highlighting
the urgent need for more nuanced and adaptive security strategies to ensure the
responsible and robust deployment of LLMs in real-world applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面具背后：大型语言模型中伪装越狱的基准测试</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越容易受到一种复杂的对抗性提示形式的攻击，称为伪装越狱。这种方法在看似无害的语言中嵌入恶意意图，以规避现有的安全机制。与明显的攻击不同，这些微妙的提示利用了上下文模糊性和语言的灵活性，对当前的防御系统构成了重大挑战。本文研究了伪装越狱提示的构建及其影响，强调了它们的欺骗特性以及传统基于关键词的检测方法的局限性。我们引入了一个新的基准数据集——伪装越狱提示，包含500个经过精心挑选的示例（400个有害和100个无害提示），旨在严格测试LLM的安全协议。此外，我们提出了一个多维度评估框架，衡量七个维度的有害性：安全意识、技术可行性、实施保障、有害潜力、教育价值、内容质量和合规评分。我们的研究结果揭示了LLM行为的明显对比：虽然模型在无害输入下表现出高安全性和内容质量，但在面对伪装越狱尝试时，其性能和安全性显著下降。这种差异突显了普遍的脆弱性，强调了迫切需要更细致和适应性的安全策略，以确保LLM在现实应用中的负责任和稳健部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the growing vulnerability of Large Language Models (LLMs) to camouflaged jailbreaking, a sophisticated adversarial prompting technique that embeds harmful intent within seemingly harmless language, thereby evading existing safety mechanisms. Traditional detection methods, primarily reliant on keyword-based approaches, are inadequate against these subtle attacks due to their contextual ambiguity. The proposed research introduces a novel benchmark dataset, Camouflaged Jailbreak Prompts, consisting of 500 curated examples, and a multi-faceted evaluation framework that assesses harmfulness across seven dimensions. The study reveals that while LLMs perform well with benign inputs, their safety and performance significantly decline when faced with camouflaged jailbreak attempts, highlighting a critical vulnerability and the necessity for improved security strategies in LLM deployment.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）对伪装越狱攻击的日益脆弱性，这是一种复杂的对抗性提示技术，通过在看似无害的语言中嵌入恶意意图，从而规避传统安全机制。过去的方法主要依赖基于关键词的检测，但对于伪装提示的微妙性而言，这些方法显得不足。本文的贡献在于引入了一个新的基准数据集——伪装越狱提示，包含500个精心策划的示例，以及一个多维度的评估框架，评估七个维度的危害性。研究方法表明，尽管LLMs在处理无害输入时保持高安全性和内容质量，但在面对伪装越狱尝试时，其性能和安全性显著下降，表明存在关键脆弱性，迫切需要更具适应性的安全策略，以确保LLMs在实际应用中的负责任和稳健部署。</div>
</details>
</div>
<div class="card">
<div class="title">Antidote: Post-fine-tuning Safety Alignment for Large Language Models   against Harmful Fine-tuning</div>
<div class="meta-line">Authors: Tiansheng Huang, Gautam Bhattacharya, Pratik Joshi, Josh Kimball, Ling Liu</div>
<div class="meta-line">First: 2024-08-18T21:45:03+00:00 · Latest: 2025-09-05T04:54:29+00:00</div>
<div class="meta-line">Comments: Rejected by AAAI25-AIA. Accepted by ICML25. Authors are thankful to
  the anonymous reviewers from both AAAI25-AIA and ICML25</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2408.09600v3">Abs</a> · <a href="http://arxiv.org/pdf/2408.09600v3">PDF</a> · <a href="https://github.com/git-disl/Antidote">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safety aligned Large Language Models (LLMs) are vulnerable to harmful
fine-tuning attacks -- a few harmful data mixed in the fine-tuning dataset can
break the LLMs&#x27;s safety alignment. While several defenses have been proposed,
our evaluation shows that existing defenses fail \textit{when some specific
training hyper-parameters are chosen} -- a large learning rate or a large
number of training epochs in the fine-tuning stage can easily invalidate the
defense. To this end, we propose Antidote, a post-fine-tuning stage solution,
which remains \textbf{\textit{agnostic to the training hyper-parameters in the
fine-tuning stage}}. Antidote relies on the philosophy that by removing the
harmful parameters, the harmful model can be recovered from the harmful
behaviors, regardless of how those harmful parameters are formed in the
fine-tuning stage. With this philosophy, we introduce a one-shot pruning stage
after harmful fine-tuning to remove the harmful weights that are responsible
for the generation of harmful content. Despite its embarrassing simplicity,
empirical results show that Antidote can reduce harmful score while maintaining
accuracy on downstream tasks. Code is available at
https://github.com/git-disl/Antidote.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解药：针对有害微调的后微调安全对齐大型语言模型</div>
<div class="mono" style="margin-top:8px">安全对齐的大型语言模型（LLMs）易受到有害微调攻击——在微调数据集中混入少量有害数据会破坏LLMs的安全对齐。虽然提出了几种防御措施，但我们的评估显示，现有防御在选择某些特定训练超参数时失败——在微调阶段使用较大的学习率或较多的训练轮次会轻易使防御失效。为此，我们提出了解药，一种后微调阶段的解决方案，它对微调阶段的训练超参数保持无关性。解药依赖于这样一种理念：通过去除有害参数，可以从有害行为中恢复有害模型，而不管这些有害参数在微调阶段是如何形成的。基于这一理念，我们在有害微调后引入了一次性剪枝阶段，以去除负责生成有害内容的有害权重。尽管其简单性令人尴尬，但实证结果表明，解药可以在保持下游任务准确性的同时降低有害评分。代码可在https://github.com/git-disl/Antidote获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of safety-aligned Large Language Models (LLMs) to harmful fine-tuning attacks, where even a small amount of harmful data can compromise their safety. Previous methods have been ineffective under certain training hyper-parameters, such as high learning rates or extended training epochs, which can invalidate existing defenses. The proposed approach, Antidote, offers a post-fine-tuning solution that is agnostic to these hyper-parameters, focusing on removing harmful parameters to recover the model&#x27;s safety. This methodology introduces a one-shot pruning stage to eliminate weights responsible for harmful outputs. Empirical results demonstrate that Antidote effectively reduces harmful scores while preserving accuracy on downstream tasks, supporting its intended goals.</div>
<div class="mono" style="margin-top:8px">本研究解决了安全对齐的大型语言模型（LLMs）在受到有害微调攻击时的脆弱性，即即使少量有害数据也能破坏其安全对齐。以往的方法在某些训练超参数下（如高学习率或长时间训练）效果不佳，因此需要更稳健的解决方案。提出的方法Antidote引入了一种与这些超参数无关的后微调方法，专注于去除有害参数以恢复模型的安全行为。该方法论包括一个一次性修剪阶段，以消除导致有害输出的权重。实验结果表明，Antidote能够有效降低有害评分，同时保持下游任务的准确性，支持其预期目标。</div>
</details>
</div>
<div class="card">
<div class="title">Context Engineering for Trustworthiness: Rescorla Wagner Steering Under   Mixed and Inappropriate Contexts</div>
<div class="meta-line">Authors: Rushi Wang, Jiateng Liu, Cheng Qian, Yifan Shen, Yanzhou Pan, Zhaozhuo Xu, Ahmed Abbasi, Heng Ji, Denghui Zhang</div>
<div class="meta-line">First: 2025-09-02T00:40:34+00:00 · Latest: 2025-09-02T00:40:34+00:00</div>
<div class="meta-line">Comments: 36 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.04500v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.04500v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Incorporating external context can significantly enhance the response quality
of Large Language Models (LLMs). However, real-world contexts often mix
relevant information with disproportionate inappropriate content, posing
reliability risks. How do LLMs process and prioritize mixed context? To study
this, we introduce the Poisoned Context Testbed, pairing queries with
real-world contexts containing relevant and inappropriate content. Inspired by
associative learning in animals, we adapt the Rescorla-Wagner (RW) model from
neuroscience to quantify how competing contextual signals influence LLM
outputs. Our adapted model reveals a consistent behavioral pattern: LLMs
exhibit a strong tendency to incorporate information that is less prevalent in
the context. This susceptibility is harmful in real-world settings, where small
amounts of inappropriate content can substantially degrade response quality.
Empirical evaluations on our testbed further confirm this vulnerability. To
tackle this, we introduce RW-Steering, a two-stage finetuning-based approach
that enables the model to internally identify and ignore inappropriate signals.
Unlike prior methods that rely on extensive supervision across diverse context
mixtures, RW-Steering generalizes robustly across varying proportions of
inappropriate content. Experiments show that our best fine-tuned model improves
response quality by 39.8% and reverses the undesirable behavior curve,
establishing RW-Steering as a robust, generalizable context engineering
solution for improving LLM safety in real-world use.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可信度的上下文工程：在混合和不当上下文下的Rescorla-Wagner引导</div>
<div class="mono" style="margin-top:8px">引入外部上下文可以显著提高大型语言模型（LLMs）的响应质量。然而，现实世界的上下文往往将相关信息与不当内容混合，带来可靠性风险。LLMs如何处理和优先考虑混合上下文？为此，我们引入了毒化上下文测试平台，将查询与包含相关和不当内容的现实世界上下文配对。受到动物联想学习的启发，我们从神经科学中改编了Rescorla-Wagner（RW）模型，以量化竞争上下文信号如何影响LLM输出。我们改编的模型揭示了一种一致的行为模式：LLMs表现出强烈倾向于纳入在上下文中较少出现的信息。这种易感性在现实世界环境中是有害的，因为少量不当内容会显著降低响应质量。我们在测试平台上的实证评估进一步确认了这一脆弱性。为了解决这个问题，我们引入了RW-Steering，一种基于两阶段微调的方法，使模型能够内部识别和忽略不当信号。与依赖于多样上下文混合的广泛监督的先前方法不同，RW-Steering在不同不当内容比例下具有良好的泛化能力。实验表明，我们最佳的微调模型提高了响应质量39.8%，并逆转了不良行为曲线，确立了RW-Steering作为一种稳健、可泛化的上下文工程解决方案，以提高LLM在现实世界使用中的安全性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enhancing the response quality of Large Language Models (LLMs) when faced with mixed and inappropriate contexts, which can compromise reliability. Previous methods often required extensive supervision and struggled with varying context mixtures, leading to inconsistent performance. The proposed approach, RW-Steering, is motivated by the need for a more robust solution and adapts the Rescorla-Wagner model from neuroscience to quantify the influence of competing contextual signals. This two-stage finetuning method allows LLMs to identify and disregard inappropriate signals effectively. Empirical evaluations on the Poisoned Context Testbed demonstrate that RW-Steering improves response quality by 39.8%, successfully addressing the identified vulnerabilities and enhancing LLM safety in practical applications.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）在面对包含相关和不当信息的混合上下文时提高响应质量的挑战，这种情况可能会影响可靠性。以往的方法在处理多样上下文混合时需要大量监督，导致LLM输出的脆弱性。提出的方法RW-Steering旨在提高LLM的安全性，使模型能够识别并忽略不当信号，而无需广泛的监督。该方法论采用基于Rescorla-Wagner模型的两阶段微调过程，量化竞争上下文信号的影响。在毒性上下文测试平台上进行的实验表明，RW-Steering显著提高了响应质量39.8%，有效减轻了不当内容的负面影响，展示了其作为现实应用中上下文工程的强大解决方案的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Unraveling LLM Jailbreaks Through Safety Knowledge Neurons</div>
<div class="meta-line">Authors: Chongwen Zhao, Kaizhu Huang</div>
<div class="meta-line">First: 2025-09-01T17:17:06+00:00 · Latest: 2025-09-01T17:17:06+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.01631v1">Abs</a> · <a href="http://arxiv.org/pdf/2509.01631v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly attracting attention in various
applications. Nonetheless, there is a growing concern as some users attempt to
exploit these models for malicious purposes, including the synthesis of
controlled substances and the propagation of disinformation, a technique known
as &quot;Jailbreak.&quot; While some studies have achieved defenses against jailbreak
attacks by modifying output distributions or detecting harmful content, the
exact rationale still remains elusive. In this work, we present a novel
neuron-level interpretability method that focuses on the role of safety-related
knowledge neurons. Unlike existing approaches, our method projects the model&#x27;s
internal representation into a more consistent and interpretable vocabulary
space. We then show that adjusting the activation of safety-related neurons can
effectively control the model&#x27;s behavior with a mean ASR higher than 97%.
Building on this insight, we propose SafeTuning, a fine-tuning strategy that
reinforces safety-critical neurons to improve model robustness against
jailbreaks. SafeTuning consistently reduces attack success rates across
multiple LLMs and outperforms all four baseline defenses. These findings offer
a new perspective on understanding and defending against jailbreak attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过安全知识神经元揭示大型语言模型的越狱</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在各种应用中越来越受到关注。然而，随着一些用户试图利用这些模型进行恶意活动，包括合成管制物质和传播虚假信息，越狱技术的担忧日益增加。尽管一些研究通过修改输出分布或检测有害内容来防御越狱攻击，但确切的原理仍然难以捉摸。在本研究中，我们提出了一种新颖的神经元级可解释性方法，专注于安全相关知识神经元的作用。与现有方法不同，我们的方法将模型的内部表示投影到一个更一致和可解释的词汇空间。然后，我们展示了调整安全相关神经元的激活可以有效控制模型的行为，平均攻击成功率（ASR）超过97%。基于这一见解，我们提出了SafeTuning，一种微调策略，强化安全关键神经元以提高模型对越狱的鲁棒性。SafeTuning在多个LLM上持续降低攻击成功率，并超越所有四个基线防御。这些发现为理解和防御越狱攻击提供了新的视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the increasing exploitation of Large Language Models (LLMs) for malicious activities, particularly through techniques known as &#x27;Jailbreak,&#x27; which can lead to harmful outputs. Previous methods aimed at defending against these attacks often focused on modifying output distributions or detecting harmful content, but they lacked clarity on the underlying mechanisms. The proposed approach introduces a novel neuron-level interpretability method that emphasizes safety-related knowledge neurons, projecting the model&#x27;s internal representations into a more interpretable vocabulary space. This method allows for effective control of the model&#x27;s behavior by adjusting the activation of these neurons, achieving a mean attack success rate (ASR) reduction higher than 97%. The contribution of this paper lies in the introduction of SafeTuning, a fine-tuning strategy that enhances the robustness of LLMs against jailbreaks, consistently outperforming existing defenses across multiple models.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）被恶意利用的问题，特别是通过被称为“越狱”的技术，这可能导致有害输出。以往的防御方法主要集中在修改输出分布或检测有害内容上，但往往缺乏清晰的原理和有效性。本文提出了一种新的神经元级可解释性方法，强调安全相关知识神经元，将模型的内部表示投影到更可解释的词汇空间中。所提出的方法SafeTuning对这些安全关键神经元进行微调，在多个LLMs上实现了超过97%的平均攻击成功率（ASR）降低，显著优于现有防御方法，并为理解越狱漏洞提供了新的视角。</div>
</details>
</div>
<div class="card">
<div class="title">Turning the Spell Around: Lightweight Alignment Amplification via   Rank-One Safety Injection</div>
<div class="meta-line">Authors: Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, George Turkiyyah, Bernard Ghanem</div>
<div class="meta-line">First: 2025-08-28T13:22:33+00:00 · Latest: 2025-08-28T13:22:33+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.20766v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.20766v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safety alignment in Large Language Models (LLMs) often involves mediating
internal representations to refuse harmful requests. Recent research has
demonstrated that these safety mechanisms can be bypassed by ablating or
removing specific representational directions within the model. In this paper,
we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box
method that amplifies a model&#x27;s safety alignment by permanently steering its
activations toward the refusal-mediating subspace. ROSI operates as a simple,
fine-tuning-free rank-one weight modification applied to all residual stream
write matrices. The required safety direction can be computed from a small set
of harmful and harmless instruction pairs. We show that ROSI consistently
increases safety refusal rates - as evaluated by Llama Guard 3 - while
preserving the utility of the model on standard benchmarks such as MMLU,
HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align
&#x27;uncensored&#x27; models by amplifying their own latent safety directions,
demonstrating its utility as an effective last-mile safety procedure. Our
results suggest that targeted, interpretable weight steering is a cheap and
potent mechanism to improve LLM safety, complementing more resource-intensive
fine-tuning paradigms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扭转咒语：通过秩一安全注入实现轻量级对齐放大</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的安全对齐通常涉及调解内部表示以拒绝有害请求。最近的研究表明，这些安全机制可以通过消融或移除模型内特定的表示方向来绕过。在本文中，我们提出了相反的方法：秩一安全注入（ROSI），这是一种白盒方法，通过永久性地将模型的激活引导到拒绝调解子空间来放大模型的安全对齐。ROSI作为一种简单的、无需微调的秩一权重修改，应用于所有残差流写矩阵。所需的安全方向可以从一小组有害和无害的指令对中计算得出。我们展示了ROSI在提高安全拒绝率方面的一致性——通过Llama Guard 3进行评估，同时在MMLU、HellaSwag和Arc等标准基准上保持模型的实用性。此外，我们还展示了ROSI可以通过放大自身潜在安全方向来重新对齐“未审查”模型，证明其作为有效的最后一公里安全程序的实用性。我们的结果表明，针对性的、可解释的权重引导是一种廉价而有效的机制，可以提高LLM的安全性，补充更资源密集的微调范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of safety alignment in Large Language Models (LLMs), where existing methods can be circumvented by manipulating internal representations. Traditional approaches often rely on complex fine-tuning processes that may not effectively enhance safety. In contrast, the proposed Rank-One Safety Injection (ROSI) method offers a lightweight, white-box solution that amplifies safety alignment by modifying model weights to direct activations toward a refusal-mediating subspace without requiring extensive fine-tuning. The paper demonstrates that ROSI significantly improves safety refusal rates as measured by Llama Guard 3, while maintaining model performance on standard benchmarks like MMLU, HellaSwag, and Arc, thus providing a cost-effective and interpretable alternative to existing safety enhancement methods.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLMs）中安全对齐的挑战，现有方法可能通过改变特定的内部表示而被规避。传统方法通常依赖于复杂的微调过程，这可能资源密集且未必有效增强安全性。相比之下，提出的秩一安全注入（ROSI）方法提供了一种轻量级解决方案，通过修改模型激活，使其朝向拒绝中介子空间，从而增强安全对齐，无需微调。该方法利用一小组有害和无害指令对来计算必要的安全方向。论文表明，ROSI显著提高了安全拒绝率（通过Llama Guard 3测量），同时在MMLU、HellaSwag和Arc等标准基准上保持模型性能，从而提供了一种成本效益高且可解释的方法来改善LLM安全性。</div>
</details>
</div>
<div class="card">
<div class="title">LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large   Language Models</div>
<div class="meta-line">Authors: Zhiyuan Ning, Tianle Gu, Jiaxin Song, Shixin Hong, Lingyu Li, Huacan Liu, Jie Li, Yixu Wang, Meng Lingyu, Yan Teng, Yingchun Wang</div>
<div class="meta-line">First: 2025-08-18T08:59:01+00:00 · Latest: 2025-08-27T12:21:20+00:00</div>
<div class="meta-line">Comments: 7pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.12733v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.12733v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread adoption and increasing prominence of large language models
(LLMs) in global technologies necessitate a rigorous focus on ensuring their
safety across a diverse range of linguistic and cultural contexts. The lack of
a comprehensive evaluation and diverse data in existing multilingual safety
evaluations for LLMs limits their effectiveness, hindering the development of
robust multilingual safety alignment. To address this critical gap, we
introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted
with meticulous attention to linguistic authenticity. The LinguaSafe dataset
comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated
using a combination of translated, transcreated, and natively-sourced data, our
dataset addresses the critical need for multilingual safety evaluations of
LLMs, filling the void in the safety evaluation of LLMs across diverse
under-represented languages from Hungarian to Malay. LinguaSafe presents a
multidimensional and fine-grained evaluation framework, with direct and
indirect safety assessments, including further evaluations for oversensitivity.
The results of safety and helpfulness evaluations vary significantly across
different domains and different languages, even in languages with similar
resource levels. Our benchmark provides a comprehensive suite of metrics for
in-depth safety evaluation, underscoring the critical importance of thoroughly
assessing multilingual safety in LLMs to achieve more balanced safety
alignment. Our dataset and code are released to the public to facilitate
further research in the field of multilingual LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LinguaSafe：大型语言模型的综合多语言安全基准</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在全球技术中的广泛应用和日益重要性，迫切需要在多样的语言和文化背景下确保其安全性。现有多语言安全评估中缺乏全面的评估和多样的数据，限制了其有效性，阻碍了稳健的多语言安全对齐的发展。为了解决这一关键缺口，我们推出了LinguaSafe，这是一个经过精心设计的综合多语言安全基准，注重语言的真实性。LinguaSafe数据集包含12种语言的45,000个条目，从匈牙利语到马来语。我们的数据集结合了翻译、再创作和本土数据，满足了对LLMs进行多语言安全评估的迫切需求，填补了在匈牙利语到马来语等多种代表性不足语言的安全评估空白。LinguaSafe提供了一个多维度和细致的评估框架，包括直接和间接的安全评估，以及对过度敏感性的进一步评估。安全性和有用性评估的结果在不同领域和不同语言之间差异显著，即使在资源水平相似的语言中也是如此。我们的基准提供了一套全面的指标，用于深入的安全评估，强调了在LLMs中彻底评估多语言安全以实现更平衡的安全对齐的重要性。我们的数据集和代码已公开发布，以促进多语言LLM安全领域的进一步研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the urgent need for safety evaluations of large language models (LLMs) across various linguistic and cultural contexts, highlighting the limitations of existing multilingual safety assessments due to a lack of comprehensive data. Previous methods have often relied on insufficiently diverse datasets, which hinder effective safety alignment for LLMs. The proposed approach, LinguaSafe, introduces a meticulously curated benchmark consisting of 45,000 entries in 12 languages, combining translated, transcreated, and natively-sourced data to fill the gap in multilingual safety evaluations. This paper contributes a multidimensional evaluation framework that includes both direct and indirect safety assessments, revealing significant variations in safety and helpfulness across different languages and domains. The methodology demonstrates that the comprehensive metrics provided by LinguaSafe can effectively support the goal of achieving balanced safety alignment in LLMs, facilitating further research in this critical area.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在不同语言和文化背景下的安全评估需求，强调现有多语言安全评估因数据缺乏而存在的不足。以往的方法因数据集缺乏多样性和真实性而受到限制，妨碍了LLMs的有效安全对齐。提出的LinguaSafe方法引入了一个强大的多语言安全基准，包含12种语言的45,000个条目，利用翻译、再创作和本土数据的混合，确保语言的真实性。该基准提供了一个多维评估框架，包括直接和间接的安全评估，揭示了不同语言和领域之间安全性和有用性显著差异。该方法不仅填补了多语言安全评估的关键空白，还提供了一套全面的评估指标，支持实现LLMs的平衡安全对齐目标。</div>
</details>
</div>
<div class="card">
<div class="title">Prefill-level Jailbreak: A Black-Box Risk Analysis of Large Language   Models</div>
<div class="meta-line">Authors: Yakai Li, Jiekang Hu, Weiduan Sang, Luping Ma, Dongsheng Nie, Weijuan Zhang, Aimin Yu, Yi Su, Qingjia Huang, Qihang Zhou</div>
<div class="meta-line">First: 2025-04-28T07:38:43+00:00 · Latest: 2025-08-25T20:17:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.21038v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.21038v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models face security threats from jailbreak attacks. Existing
research has predominantly focused on prompt-level attacks while largely
ignoring the underexplored attack surface of user-controlled response
prefilling. This functionality allows an attacker to dictate the beginning of a
model&#x27;s output, thereby shifting the attack paradigm from persuasion to direct
state manipulation.In this paper, we present a systematic black-box security
analysis of prefill-level jailbreak attacks. We categorize these new attacks
and evaluate their effectiveness across fourteen language models. Our
experiments show that prefill-level attacks achieve high success rates, with
adaptive methods exceeding 99% on several models. Token-level probability
analysis reveals that these attacks work through initial-state manipulation by
changing the first-token probability from refusal to compliance.Furthermore, we
show that prefill-level jailbreak can act as effective enhancers, increasing
the success of existing prompt-level attacks by 10 to 15 percentage points. Our
evaluation of several defense strategies indicates that conventional content
filters offer limited protection. We find that a detection method focusing on
the manipulative relationship between the prompt and the prefill is more
effective. Our findings reveal a gap in current LLM safety alignment and
highlight the need to address the prefill attack surface in future safety
training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>预填级越狱：大型语言模型的黑箱风险分析</div>
<div class="mono" style="margin-top:8px">大型语言模型面临来自越狱攻击的安全威胁。现有研究主要集中在提示级攻击上，而在很大程度上忽视了用户控制的响应预填充这一尚未充分探索的攻击面。这一功能使攻击者能够决定模型输出的开头，从而将攻击范式从说服转变为直接状态操控。本文对预填级越狱攻击进行了系统的黑箱安全分析。我们对这些新攻击进行了分类，并评估了它们在十四种语言模型上的有效性。实验表明，预填级攻击的成功率很高，适应性方法在多个模型上超过99%。标记级概率分析显示，这些攻击通过改变拒绝到合规的首标记概率来实现初始状态操控。此外，我们还表明，预填级越狱可以作为有效的增强器，将现有提示级攻击的成功率提高10到15个百分点。我们对几种防御策略的评估表明，传统内容过滤器提供的保护有限。我们发现，关注提示与预填之间操控关系的检测方法更为有效。我们的研究结果揭示了当前大型语言模型安全对齐中的一个缺口，并强调了在未来安全训练中解决预填攻击面的问题的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the security vulnerabilities of Large Language Models (LLMs) to jailbreak attacks, which have primarily been studied at the prompt level, neglecting the significant threat posed by user-controlled response prefiling. The authors propose a novel approach that systematically analyzes prefill-level jailbreak attacks, contrasting with existing methods that overlook this attack surface. The paper contributes by categorizing these attacks and demonstrating their effectiveness across fourteen language models, revealing that adaptive prefill-level methods can achieve success rates exceeding 99%. The research methodology includes token-level probability analysis to illustrate how these attacks manipulate initial states, and the findings indicate that prefill-level attacks can enhance the efficacy of prompt-level attacks by 10 to 15 percentage points. Additionally, the study critiques conventional defense strategies, suggesting that a detection method focused on the relationship between prompts and prefill is more effective, thereby underscoring a critical gap in current LLM safety alignment and the need for improved safety training.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在越狱攻击方面的安全漏洞，这些攻击主要在提示级别进行研究，而忽视了用户控制的响应预填充所带来的重大威胁。所提出的方法将重点转向预填充级攻击，这种攻击通过操纵模型输出的初始状态，从而增强现有提示级攻击的有效性。本文的贡献在于提供了一种系统的黑箱安全分析，分类了这些新攻击类型，并评估了它们在十四种语言模型上的成功率，结果显示自适应预填充级攻击的成功率超过99%。该方法包括令牌级概率分析，以展示这些攻击如何操纵首个令牌的概率，研究结果表明传统防御措施不足，建议基于提示与预填充之间关系的检测方法更为有效。总体而言，研究强调了LLM安全中的关键缺口，并强调未来安全措施需要解决预填充的脆弱性。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring the Vulnerability of the Content Moderation Guardrail in Large   Language Models via Intent Manipulation</div>
<div class="meta-line">Authors: Jun Zhuang, Haibo Jin, Ye Zhang, Zhengjian Kang, Wenbin Zhang, Gaby G. Dagher, Haohan Wang</div>
<div class="meta-line">Venue: EMNLP</div>
<div class="meta-line">First: 2025-05-24T06:47:32+00:00 · Latest: 2025-08-25T00:27:19+00:00</div>
<div class="meta-line">Comments: Accepted for EMNLP&#x27;25 Findings. TL;DR: We propose a new two-stage
  intent-based prompt-refinement framework, IntentPrompt, that aims to explore
  the vulnerability of LLMs&#x27; content moderation guardrails by refining prompts
  into benign-looking declarative forms via intent manipulation for red-teaming
  purposes</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.18556v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.18556v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Intent detection, a core component of natural language understanding, has
considerably evolved as a crucial mechanism in safeguarding large language
models (LLMs). While prior work has applied intent detection to enhance LLMs&#x27;
moderation guardrails, showing a significant success against content-level
jailbreaks, the robustness of these intent-aware guardrails under malicious
manipulations remains under-explored. In this work, we investigate the
vulnerability of intent-aware guardrails and demonstrate that LLMs exhibit
implicit intent detection capabilities. We propose a two-stage intent-based
prompt-refinement framework, IntentPrompt, that first transforms harmful
inquiries into structured outlines and further reframes them into
declarative-style narratives by iteratively optimizing prompts via feedback
loops to enhance jailbreak success for red-teaming purposes. Extensive
experiments across four public benchmarks and various black-box LLMs indicate
that our framework consistently outperforms several cutting-edge jailbreak
methods and evades even advanced Intent Analysis (IA) and Chain-of-Thought
(CoT)-based defenses. Specifically, our &quot;FSTR+SPIN&quot; variant achieves attack
success rates ranging from 88.25% to 96.54% against CoT-based defenses on the
o1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based
defenses. These findings highlight a critical weakness in LLMs&#x27; safety
mechanisms and suggest that intent manipulation poses a growing challenge to
content moderation guardrails.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过意图操控探索大型语言模型内容审核护栏的脆弱性</div>
<div class="mono" style="margin-top:8px">意图检测是自然语言理解的核心组成部分，作为保护大型语言模型（LLMs）的重要机制，已经显著发展。虽然之前的工作已将意图检测应用于增强LLMs的审核护栏，并在内容级别的越狱攻击中取得了显著成功，但这些意图感知护栏在恶意操控下的鲁棒性仍然未被充分探索。在本研究中，我们调查了意图感知护栏的脆弱性，并展示了LLMs表现出隐式意图检测能力。我们提出了一种两阶段的基于意图的提示优化框架IntentPrompt，首先将有害查询转化为结构化大纲，然后通过反馈循环迭代优化提示，将其进一步重构为声明式叙述，以增强越狱成功率，服务于红队测试目的。针对四个公共基准和多种黑箱LLMs的广泛实验表明，我们的框架在多个前沿越狱方法中始终表现优越，甚至能够规避先进的意图分析（IA）和思维链（CoT）防御。具体而言，我们的“FSTR+SPIN”变体在o1模型上对CoT防御的攻击成功率范围为88.25%到96.54%，在GPT-4o模型下对IA防御的攻击成功率范围为86.75%到97.12%。这些发现突显了LLMs安全机制的关键弱点，并表明意图操控对内容审核护栏构成日益严峻的挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the vulnerability of content moderation guardrails in large language models (LLMs), particularly focusing on intent detection as a mechanism for safeguarding these models. Previous methods have successfully applied intent detection to improve moderation but have not thoroughly examined the robustness of these guardrails against malicious intent manipulations. The proposed approach, IntentPrompt, differs by introducing a two-stage prompt-refinement framework that transforms harmful inquiries into structured outlines and reframes them into declarative narratives, thereby enhancing the effectiveness of jailbreak attempts. The contribution of this paper lies in its demonstration of the implicit intent detection capabilities of LLMs and the identification of weaknesses in their safety mechanisms. Through extensive experiments on four public benchmarks, the proposed method consistently outperformed existing jailbreak techniques, achieving attack success rates between 88.25% and 97.12% against advanced defenses, thereby underscoring the challenges posed by intent manipulation to content moderation systems.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）内容审查防护机制的脆弱性，特别关注意图检测，这对自然语言理解至关重要。以往的方法成功地将意图检测应用于改善审查防护，以抵御内容级的越狱攻击，但其在恶意意图操控下的稳健性尚未得到充分研究。所提出的两阶段基于意图的提示优化框架IntentPrompt与现有方法不同，通过将有害查询转化为结构化大纲，并通过迭代优化提示将其重新构建为陈述式叙述，从而有效增强越狱尝试的成功率，显示出其揭示LLM安全机制弱点的动机。该方法在四个公共基准和多种黑箱LLM上进行了测试，针对链式思维防御的攻击成功率在88.25%到96.54%之间，针对意图分析防御的成功率在86.75%到97.12%之间，表明当前内容审查策略面临重大挑战。</div>
</details>
</div>
<div class="card">
<div class="title">SafeLLM: Unlearning Harmful Outputs from Large Language Models against   Jailbreak Attacks</div>
<div class="meta-line">Authors: Xiangman Li, Xiaodong Wu, Qi Li, Jianbing Ni, Rongxing Lu</div>
<div class="meta-line">First: 2025-08-21T02:39:14+00:00 · Latest: 2025-08-21T02:39:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15182v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.15182v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Jailbreak attacks pose a serious threat to the safety of Large Language
Models (LLMs) by crafting adversarial prompts that bypass alignment mechanisms,
causing the models to produce harmful, restricted, or biased content. In this
paper, we propose SafeLLM, a novel unlearning-based defense framework that
unlearn the harmful knowledge from LLMs while preserving linguistic fluency and
general capabilities. SafeLLM employs a three-stage pipeline: (1) dynamic
unsafe output detection using a hybrid approach that integrates external
classifiers with model-internal evaluations; (2) token-level harmful content
tracing through feedforward network (FFN) activations to localize harmful
knowledge; and (3) constrained optimization to suppress unsafe behavior without
degrading overall model quality. SafeLLM achieves targeted and irreversible
forgetting by identifying and neutralizing FFN substructures responsible for
harmful generation pathways. Extensive experiments on prominent LLMs (Vicuna,
LLaMA, and GPT-J) across multiple jailbreak benchmarks show that SafeLLM
substantially reduces attack success rates while maintaining high
general-purpose performance. Compared to standard defense methods such as
supervised fine-tuning and direct preference optimization, SafeLLM offers
stronger safety guarantees, more precise control over harmful behavior, and
greater robustness to unseen attacks. Moreover, SafeLLM maintains the general
performance after the harmful knowledge unlearned. These results highlight
unlearning as a promising direction for scalable and effective LLM safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafeLLM：从大型语言模型中消除有害输出以抵御越狱攻击</div>
<div class="mono" style="margin-top:8px">越狱攻击通过构造对抗性提示绕过对齐机制，对大型语言模型（LLMs）的安全构成严重威胁，导致模型生成有害、受限或偏见的内容。本文提出了SafeLLM，一种新颖的基于消除学习的防御框架，旨在消除LLMs中的有害知识，同时保持语言流畅性和一般能力。SafeLLM采用三阶段流程：（1）使用混合方法进行动态不安全输出检测，结合外部分类器与模型内部评估；（2）通过前馈网络（FFN）激活进行标记级有害内容追踪，以定位有害知识；（3）约束优化以抑制不安全行为而不降低整体模型质量。SafeLLM通过识别和中和负责有害生成路径的FFN子结构，实现了有针对性和不可逆转的遗忘。在多个越狱基准测试中，对知名LLMs（Vicuna、LLaMA和GPT-J）进行的广泛实验表明，SafeLLM显著降低了攻击成功率，同时保持高通用性能。与监督微调和直接偏好优化等标准防御方法相比，SafeLLM提供了更强的安全保障，更精确地控制有害行为，并对未见攻击具有更大的鲁棒性。此外，SafeLLM在消除有害知识后保持了整体性能。这些结果突显了消除学习作为可扩展和有效的LLM安全方向的前景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the significant threat posed by jailbreak attacks on Large Language Models (LLMs), which exploit adversarial prompts to generate harmful content. Previous methods, such as supervised fine-tuning and direct preference optimization, have limitations in effectively mitigating these attacks while preserving model performance. The proposed SafeLLM framework introduces an unlearning-based approach that detects unsafe outputs, traces harmful content, and optimizes model behavior without degrading overall quality. This methodology demonstrates substantial improvements in reducing attack success rates across various jailbreak benchmarks on prominent LLMs like Vicuna, LLaMA, and GPT-J, while maintaining high general-purpose performance, thus providing stronger safety guarantees and robustness against unseen attacks.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）面临的监狱破解攻击的重大威胁，该攻击利用对抗性提示引发有害或偏见的输出。以往的方法，如监督微调和直接偏好优化，在有效减轻这些风险的同时保持模型性能方面存在局限性。提出的SafeLLM框架引入了一种基于遗忘的方法，能够检测不安全输出、在标记级别追踪有害内容，并优化模型以抑制不安全行为，而不降低其整体质量。这一方法在Vicuna、LLaMA和GPT-J等模型的多个监狱破解基准测试中显示出显著降低攻击成功率，同时保持高通用性能，从而提供更强的安全保障和对未知攻击的增强鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">CCFC: Core &amp; Core-Full-Core Dual-Track Defense for LLM Jailbreak   Protection</div>
<div class="meta-line">Authors: Jiaming Hu, Haoyu Wang, Debarghya Mukherjee, Ioannis Ch. Paschalidis</div>
<div class="meta-line">First: 2025-08-19T04:17:21+00:00 · Latest: 2025-08-19T04:17:21+00:00</div>
<div class="meta-line">Comments: 11 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.14128v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.14128v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Jailbreak attacks pose a serious challenge to the safe deployment of large
language models (LLMs). We introduce CCFC (Core &amp; Core-Full-Core), a
dual-track, prompt-level defense framework designed to mitigate LLMs&#x27;
vulnerabilities from prompt injection and structure-aware jailbreak attacks.
CCFC operates by first isolating the semantic core of a user query via few-shot
prompting, and then evaluating the query using two complementary tracks: a
core-only track to ignore adversarial distractions (e.g., toxic suffixes or
prefix injections), and a core-full-core (CFC) track to disrupt the structural
patterns exploited by gradient-based or edit-based attacks. The final response
is selected based on a safety consistency check across both tracks, ensuring
robustness without compromising on response quality. We demonstrate that CCFC
cuts attack success rates by 50-75% versus state-of-the-art defenses against
strong adversaries (e.g., DeepInception, GCG), without sacrificing fidelity on
benign queries. Our method consistently outperforms state-of-the-art
prompt-level defenses, offering a practical and effective solution for safer
LLM deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CCFC：针对LLM越狱保护的核心与全核心双轨防御</div>
<div class="mono" style="margin-top:8px">越狱攻击对大型语言模型（LLM）的安全部署构成了严重挑战。我们介绍了CCFC（核心与全核心），这是一种双轨、提示级防御框架，旨在减轻LLM在提示注入和结构感知越狱攻击中的脆弱性。CCFC首先通过少量提示隔离用户查询的语义核心，然后使用两个互补轨道评估查询：一个仅核心轨道以忽略对抗性干扰（例如，有毒后缀或前缀注入），以及一个核心-全核心（CFC）轨道以破坏被基于梯度或编辑的攻击利用的结构模式。最终响应基于两个轨道的安全一致性检查进行选择，确保在不妥协响应质量的情况下保持鲁棒性。我们证明CCFC在对抗强大对手（例如，DeepInception，GCG）时将攻击成功率降低了50-75%，而不牺牲对良性查询的保真度。我们的方法始终优于最先进的提示级防御，提供了一种实用有效的解决方案，以实现更安全的LLM部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the significant challenge posed by jailbreak attacks on large language models (LLMs), which threaten their safe deployment. Previous methods have struggled with effectively mitigating vulnerabilities to prompt injection and structure-aware attacks, often compromising response quality. The proposed CCFC (Core &amp; Core-Full-Core) framework introduces a dual-track defense mechanism that isolates the semantic core of user queries and evaluates them through two complementary tracks, enhancing robustness against adversarial distractions and structural exploitation. This approach is well-motivated as it significantly reduces attack success rates by 50-75% compared to state-of-the-art defenses while maintaining fidelity on benign queries, demonstrating its effectiveness in ensuring safer LLM deployment.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLM）面临的越狱攻击这一重大挑战，这些攻击威胁着其安全部署。以往的方法在有效减轻对提示注入和结构感知攻击的脆弱性方面存在困难，往往牺牲了响应质量或未能提供足够的保护。提出的CCFC（核心与全核心）框架引入了一种双轨防御机制，通过隔离用户查询的语义核心，并通过两个互补轨道对其进行评估，从而增强了对对抗性干扰和结构利用的鲁棒性。这种方法具有良好的动机，因为它确保了安全性而不牺牲响应质量。该方法在攻击成功率上相比于最先进的防御手段减少了50-75%，同时保持了对良性查询的保真度，从而为更安全的LLM部署提供了切实可行的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Automated Black-box Prompt Engineering for Personalized Text-to-Image   Generation</div>
<div class="meta-line">Authors: Yutong He, Alexander Robey, Naoki Murata, Yiding Jiang, Joshua Nathaniel Williams, George J. Pappas, Hamed Hassani, Yuki Mitsufuji, Ruslan Salakhutdinov, J. Zico Kolter</div>
<div class="meta-line">Venue: Transactions on Machine Learning Research (TMLR), 2025. ISSN
  2835-8856</div>
<div class="meta-line">First: 2024-03-28T02:35:53+00:00 · Latest: 2025-08-16T03:22:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2403.19103v4">Abs</a> · <a href="http://arxiv.org/pdf/2403.19103v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prompt engineering is an effective but labor-intensive way to control
text-to-image (T2I) generative models. Its time-intensive nature and complexity
have spurred the development of algorithms for automated prompt generation.
However, these methods often struggle with transferability across T2I models,
require white-box access to the underlying model, or produce non-intuitive
prompts. In this work, we introduce PRISM, an algorithm that automatically
produces human-interpretable and transferable prompts that can effectively
generate desired concepts given only black-box access to T2I models. Inspired
by large language model (LLM) jailbreaking, PRISM leverages the in-context
learning ability of LLMs to iteratively refine the candidate prompt
distribution built upon the reference images. Our experiments demonstrate the
versatility and effectiveness of PRISM in generating accurate prompts for
objects, styles, and images across multiple T2I models, including Stable
Diffusion, DALL-E, and Midjourney.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>个性化文本到图像生成的自动化黑箱提示工程</div>
<div class="mono" style="margin-top:8px">提示工程是一种有效但劳动密集的控制文本到图像（T2I）生成模型的方法。其耗时的特性和复杂性促使了自动提示生成算法的发展。然而，这些方法通常在T2I模型之间的可转移性方面存在困难，需要对底层模型的白箱访问，或产生非直观的提示。在本研究中，我们介绍了PRISM，这是一种自动生成可被人理解和可转移的提示的算法，能够在仅有黑箱访问T2I模型的情况下有效生成所需概念。受到大型语言模型（LLM）越狱的启发，PRISM利用LLM的上下文学习能力，迭代地优化基于参考图像构建的候选提示分布。我们的实验展示了PRISM在为多个T2I模型（包括Stable Diffusion、DALL-E和Midjourney）生成准确提示方面的多样性和有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of prompt engineering in text-to-image (T2I) generative models, which is a labor-intensive process that often lacks transferability and requires white-box access to the models. Previous methods have struggled with these issues, producing non-intuitive prompts and limiting their applicability. The proposed approach, PRISM, is motivated by the need for an automated solution that generates human-interpretable and transferable prompts using only black-box access to T2I models. PRISM utilizes the in-context learning capabilities of large language models (LLMs) to iteratively refine prompts based on reference images. The methodology was tested across various T2I models, including Stable Diffusion, DALL-E, and Midjourney, demonstrating its effectiveness in generating accurate prompts for diverse objects and styles, thereby supporting the goal of enhancing prompt engineering efficiency.</div>
<div class="mono" style="margin-top:8px">本研究解决了文本到图像（T2I）生成模型中提示工程的挑战，这一过程劳动密集且通常缺乏可转移性，并且需要对模型的白盒访问。以往的方法在这些问题上表现不佳，生成的提示往往不直观，限制了在不同T2I模型中的可用性。所提出的方法PRISM旨在通过仅使用对T2I模型的黑盒访问来克服这些限制，生成可被人理解和可转移的提示，灵感来自大型语言模型的技术。本文的贡献在于其新颖的算法，通过基于参考图像迭代精炼提示生成，展示了其在多个T2I模型（如Stable Diffusion、DALL-E和Midjourney）中的有效性，能够为多样的对象和风格生成准确的提示，从而支持提高提示工程效率的目标。</div>
</details>
</div>
<div class="card">
<div class="title">ORFuzz: Fuzzing the &quot;Other Side&quot; of LLM Safety -- Testing Over-Refusal</div>
<div class="meta-line">Authors: Haonan Zhang, Dongxia Wang, Yi Liu, Kexin Chen, Jiashui Wang, Xinlei Ying, Long Liu, Wenhai Wang</div>
<div class="meta-line">First: 2025-08-15T05:03:26+00:00 · Latest: 2025-08-15T05:03:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.11222v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.11222v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously
rejecting benign queries due to overly conservative safety measures - a
critical functional flaw that undermines their reliability and usability.
Current methods for testing this behavior are demonstrably inadequate,
suffering from flawed benchmarks and limited test generation capabilities, as
highlighted by our empirical user study. To the best of our knowledge, this
paper introduces the first evolutionary testing framework, ORFuzz, for the
systematic detection and analysis of LLM over-refusals. ORFuzz uniquely
integrates three core components: (1) safety category-aware seed selection for
comprehensive test coverage, (2) adaptive mutator optimization using reasoning
LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge
model validated to accurately reflect user perception of toxicity and refusal.
Our extensive evaluations demonstrate that ORFuzz generates diverse, validated
over-refusal instances at a rate (6.98% average) more than double that of
leading baselines, effectively uncovering vulnerabilities. Furthermore,
ORFuzz&#x27;s outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly
transferable test cases that achieves a superior 63.56% average over-refusal
rate across 10 diverse LLMs, significantly outperforming existing datasets.
ORFuzz and ORFuzzSet provide a robust automated testing framework and a
valuable community resource, paving the way for developing more reliable and
trustworthy LLM-based software systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ORFuzz：模糊测试大型语言模型安全性的“另一面”——测试过度拒绝</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越表现出过度拒绝——由于过于保守的安全措施错误地拒绝良性查询——这是一个关键的功能缺陷，削弱了它们的可靠性和可用性。目前测试这种行为的方法显然不够，存在基准缺陷和有限的测试生成能力，正如我们的实证用户研究所强调的。根据我们所知，本文首次引入了进化测试框架ORFuzz，用于系统检测和分析LLM的过度拒绝。ORFuzz独特地集成了三个核心组件：（1）安全类别感知的种子选择，以实现全面的测试覆盖；（2）使用推理LLM的自适应变异器优化，以生成有效的测试用例；（3）OR-Judge，一个经过验证的人类对齐评判模型，能够准确反映用户对毒性和拒绝的感知。我们的广泛评估表明，ORFuzz以超过领先基准两倍的速度（平均6.98%）生成多样化、经过验证的过度拒绝实例，有效揭示了漏洞。此外，ORFuzz的输出构成了ORFuzzSet的基础，这是一个包含1,855个高度可转移测试用例的新基准，在10个不同的LLM中实现了63.56%的优越平均过度拒绝率，显著优于现有数据集。ORFuzz和ORFuzzSet提供了一个强大的自动化测试框架和有价值的社区资源，为开发更可靠和可信赖的基于LLM的软件系统铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical issue of over-refusal in Large Language Models (LLMs), where these models incorrectly reject benign queries due to overly cautious safety protocols, thus compromising their reliability and usability. Previous testing methods have proven inadequate due to flawed benchmarks and limited test generation capabilities, which ORFuzz aims to overcome by introducing an evolutionary testing framework that systematically detects and analyzes over-refusals. The paper contributes by integrating safety category-aware seed selection, adaptive mutator optimization using reasoning LLMs, and a human-aligned judge model, OR-Judge, to enhance test case generation. The methodology involves extensive evaluations that demonstrate ORFuzz&#x27;s ability to generate diverse over-refusal instances at a rate more than double that of leading baselines, achieving a 63.56% average over-refusal rate across 10 diverse LLMs, thus supporting the goal of improving LLM reliability and safety.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLM）中的过度拒绝问题，即这些模型由于过于谨慎的安全协议错误地拒绝无害查询，这影响了其可靠性和可用性。以往测试此行为的方法由于基准缺陷和有限的测试生成能力而显得不足。提出的方法ORFuzz引入了一种进化测试框架，系统地检测和分析过度拒绝，通过整合安全类别感知的种子选择、使用推理LLM的自适应变异优化以及一个名为OR-Judge的人类对齐评判模型。这种方法论实现了全面的测试覆盖和有效的测试用例生成。结果表明，ORFuzz以6.98%的平均生成过度拒绝实例的速率，超过领先基准的两倍，并创建了ORFuzzSet，一个包含1,855个测试用例的基准，在10个不同的LLM上实现了63.56%的平均过度拒绝率，显著优于现有数据集，为开发更可靠的LLM基础系统做出了贡献。</div>
</details>
</div>
<div class="card">
<div class="title">SproutBench: A Benchmark for Safe and Ethical Large Language Models for   Youth</div>
<div class="meta-line">Authors: Wenpeng Xing, Lanyi Wei, Haixiao Hu, Rongchang Li, Mohan Li, Changting Lin, Meng Han</div>
<div class="meta-line">First: 2025-08-14T18:21:39+00:00 · Latest: 2025-08-14T18:21:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.11009v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.11009v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid proliferation of large language models (LLMs) in applications
targeting children and adolescents necessitates a fundamental reassessment of
prevailing AI safety frameworks, which are largely tailored to adult users and
neglect the distinct developmental vulnerabilities of minors. This paper
highlights key deficiencies in existing LLM safety benchmarks, including their
inadequate coverage of age-specific cognitive, emotional, and social risks
spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence
(13--18). To bridge these gaps, we introduce SproutBench, an innovative
evaluation suite comprising 1,283 developmentally grounded adversarial prompts
designed to probe risks such as emotional dependency, privacy violations, and
imitation of hazardous behaviors. Through rigorous empirical evaluation of 47
diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by
robust inter-dimensional correlations (e.g., between Safety and Risk
Prevention) and a notable inverse relationship between Interactivity and Age
Appropriateness. These insights yield practical guidelines for advancing
child-centric AI design and deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SproutBench：面向青少年的安全与伦理大型语言模型基准</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在针对儿童和青少年的应用中迅速普及，迫切需要对现有的人工智能安全框架进行根本性重新评估，这些框架主要针对成人用户，忽视了未成年人独特的发展脆弱性。本文强调了现有LLM安全基准的关键缺陷，包括对早期儿童（0-6岁）、中期儿童（7-12岁）和青少年（13-18岁）特定认知、情感和社会风险的覆盖不足。为填补这些空白，我们推出了SproutBench，一个创新的评估套件，包含1,283个基于发展理论的对抗性提示，旨在探测情感依赖、隐私侵犯和模仿危险行为等风险。通过对47种多样化LLM的严格实证评估，我们发现了显著的安全脆弱性，并通过强有力的跨维度相关性（例如，安全性与风险预防之间）和互动性与年龄适宜性之间的显著反向关系进行了验证。这些见解为推进以儿童为中心的人工智能设计和部署提供了实用指南。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing use of large language models (LLMs) in applications for children and adolescents raises concerns about their safety, as existing AI safety frameworks primarily focus on adult users and overlook the unique developmental vulnerabilities of younger populations. Previous methods have failed to adequately address age-specific cognitive, emotional, and social risks, leading to a lack of comprehensive safety benchmarks for minors. This paper proposes SproutBench, a new evaluation suite that includes 1,283 adversarial prompts specifically designed to assess risks relevant to different developmental stages, such as emotional dependency and privacy violations. The methodology involves empirical evaluation of 47 LLMs, revealing significant safety vulnerabilities and correlations that inform guidelines for safer AI applications for youth. The findings indicate that the proposed approach effectively highlights the inadequacies of current safety measures and supports the need for child-centric AI design.</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在儿童和青少年应用中的广泛使用，安全性问题引发关注，因为现有的人工智能安全框架主要关注成人用户，忽视了年轻群体独特的发展脆弱性。以往的方法未能充分解决与年龄相关的认知、情感和社会风险，导致需要更具针对性的方法。本文提出了SproutBench，一个基准测试，包含1283个专门设计的对抗性提示，旨在评估与不同发展阶段相关的风险，如情感依赖和隐私侵犯。该方法通过对47种不同的LLMs进行实证评估，揭示了显著的安全脆弱性和相关性，为儿童中心的人工智能设计提供了指导。研究结果表明，所提出的方法有效识别风险，并支持提高青少年LLMs安全性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">POEX: Towards Policy Executable Jailbreak Attacks Against the LLM-based   Robots</div>
<div class="meta-line">Authors: Xuancun Lu, Zhengxian Huang, Xinfeng Li, Chi Zhang, Xiaoyu ji, Wenyuan Xu</div>
<div class="meta-line">First: 2024-12-21T13:58:27+00:00 · Latest: 2025-08-11T08:29:19+00:00</div>
<div class="meta-line">Comments: Homepage: https://poex-jailbreak.github.io/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.16633v3">Abs</a> · <a href="http://arxiv.org/pdf/2412.16633v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://poex-jailbreak.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The integration of LLMs into robots has witnessed significant growth, where
LLMs can convert instructions into executable robot policies. However, the
inherent vulnerability of LLMs to jailbreak attacks brings critical security
risks from the digital domain to the physical world. An attacked LLM-based
robot could execute harmful policies and cause physical harm. In this paper, we
investigate the feasibility and rationale of jailbreak attacks against
LLM-based robots and answer three research questions: (1) How applicable are
existing LLM jailbreak attacks against LLM-based robots? (2) What unique
challenges arise if they are not directly applicable? (3) How to defend against
such jailbreak attacks? To this end, we first construct a
&quot;human-object-environment&quot; robot risks-oriented Harmful-RLbench and then
conduct a measurement study on LLM-based robot systems. Our findings conclude
that traditional LLM jailbreak attacks are inapplicable in robot scenarios, and
we identify two unique challenges: determining policy-executable optimization
directions and accurately evaluating robot-jailbroken policies. To enable a
more thorough security analysis, we introduce POEX (POlicy EXecutable)
jailbreak, a red-teaming framework that induces harmful yet executable policy
to jailbreak LLM-based robots. POEX incorporates hidden layer gradient
optimization to guarantee jailbreak success and policy execution as well as a
multi-agent evaluator to accurately assess the practical executability of
policies. Experiments conducted on the real-world robotic systems and in
simulation demonstrate the efficacy of POEX, highlighting critical security
vulnerabilities and its transferability across LLMs. Finally, we propose
prompt-based and model-based defenses to mitigate attacks. Our findings
underscore the urgent need for security measures to ensure the safe deployment
of LLM-based robots in critical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POEX：针对基于LLM的机器人政策可执行越狱攻击的研究</div>
<div class="mono" style="margin-top:8px">将LLM集成到机器人中已显著增长，LLM可以将指令转换为可执行的机器人政策。然而，LLM固有的越狱攻击脆弱性将数字领域的关键安全风险带入物理世界。被攻击的基于LLM的机器人可能执行有害政策并造成物理伤害。本文研究了针对基于LLM的机器人的越狱攻击的可行性和理由，并回答了三个研究问题：（1）现有的LLM越狱攻击对基于LLM的机器人适用性如何？（2）如果不直接适用，会出现什么独特挑战？（3）如何防御此类越狱攻击？为此，我们首先构建了一个以“人-物体-环境”为导向的有害RLbench，然后对基于LLM的机器人系统进行测量研究。我们的研究结果表明，传统的LLM越狱攻击在机器人场景中不适用，我们识别出两个独特挑战：确定政策可执行的优化方向和准确评估机器人越狱政策。为了进行更全面的安全分析，我们引入了POEX（政策可执行）越狱，这是一个红队框架，诱导有害但可执行的政策以越狱基于LLM的机器人。POEX结合了隐藏层梯度优化，以确保越狱成功和政策执行，以及一个多智能体评估器，以准确评估政策的实际可执行性。在真实机器人系统和模拟中的实验表明了POEX的有效性，突显了关键的安全漏洞及其在LLM之间的可转移性。最后，我们提出了基于提示和基于模型的防御措施以减轻攻击。我们的研究结果强调了确保基于LLM的机器人在关键应用中安全部署的紧迫性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing integration of large language models (LLMs) into robotic systems, which poses significant security risks due to their vulnerability to jailbreak attacks that could lead to harmful physical actions. Previous methods for executing jailbreak attacks on LLMs are found to be ineffective in robotic contexts, primarily due to unique challenges such as the need for policy-executable optimization and accurate evaluation of robot-jailbroken policies. This paper introduces POEX (Policy Executable jailbreak), a novel framework that utilizes hidden layer gradient optimization and a multi-agent evaluator to successfully induce harmful yet executable policies in LLM-based robots. The methodology is validated through experiments on both real-world robotic systems and simulations, revealing critical security vulnerabilities and demonstrating the framework&#x27;s effectiveness and transferability across different LLMs. The findings emphasize the necessity for robust security measures to protect LLM-based robots in sensitive applications.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLM）在机器人系统中的日益整合，这由于其对越狱攻击的脆弱性而带来了显著的安全风险，可能导致有害的物理行为。以往的LLM越狱攻击方法在机器人环境中效果不佳，主要是由于需要可执行政策优化和准确评估机器人越狱政策等独特挑战。本文提出了POEX（可执行政策越狱）这一新框架，利用隐藏层梯度优化和多智能体评估器来确保越狱成功并评估有害政策的可执行性。该方法通过对真实机器人系统和模拟环境的实验验证，揭示了关键的安全漏洞，并展示了该框架在不同LLM之间的有效性和可转移性。研究强调了在敏感应用中保护基于LLM的机器人所需的强大安全措施。</div>
</details>
</div>
<div class="card">
<div class="title">ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</div>
<div class="meta-line">Authors: Sanket Badhe</div>
<div class="meta-line">First: 2025-08-08T17:01:41+00:00 · Latest: 2025-08-08T17:01:41+00:00</div>
<div class="meta-line">Comments: Accepted at CAMLIS 25: Conference on Applied Machine Learning for
  Information Security. 10 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.06457v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.06457v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have demonstrated impressive fluency and
reasoning capabilities, but their potential for misuse has raised growing
concern. In this paper, we present ScamAgent, an autonomous multi-turn agent
built on top of LLMs, capable of generating highly realistic scam call scripts
that simulate real-world fraud scenarios. Unlike prior work focused on
single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts
dynamically to simulated user responses, and employs deceptive persuasion
strategies across conversational turns. We show that current LLM safety
guardrails, including refusal mechanisms and content filters, are ineffective
against such agent-based threats. Even models with strong prompt-level
safeguards can be bypassed when prompts are decomposed, disguised, or delivered
incrementally within an agent framework. We further demonstrate the
transformation of scam scripts into lifelike voice calls using modern
text-to-speech systems, completing a fully automated scam pipeline. Our
findings highlight an urgent need for multi-turn safety auditing, agent-level
control frameworks, and new methods to detect and disrupt conversational
deception powered by generative AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ScamAgents：人工智能代理如何模拟人类级别的诈骗电话</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）展示了令人印象深刻的流利性和推理能力，但其潜在的误用引发了日益关注。本文介绍了ScamAgent，一个基于LLMs构建的自主多轮代理，能够生成高度真实的诈骗电话脚本，模拟现实世界的欺诈场景。与以往关注单次提示误用的工作不同，ScamAgent保持对话记忆，动态适应模拟用户响应，并在对话轮次中采用欺骗性劝说策略。我们表明，当前LLM安全防护措施，包括拒绝机制和内容过滤器，对这种基于代理的威胁无效。即使是具有强大提示级别保护的模型，在提示被分解、伪装或在代理框架内逐步传递时也可以被绕过。我们进一步展示了使用现代文本转语音系统将诈骗脚本转化为逼真的语音电话，完成一个完全自动化的诈骗管道。我们的研究结果强调了对多轮安全审计、代理级控制框架以及检测和破坏由生成性人工智能驱动的对话欺骗的新方法的迫切需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concern over the misuse of Large Language Models (LLMs) in generating realistic scam calls, which has not been adequately tackled by previous methods that primarily focused on single-shot prompt misuse. The proposed ScamAgent differs by utilizing an autonomous multi-turn dialogue system that maintains memory, adapts to user responses, and employs deceptive strategies across conversations, effectively bypassing existing safety measures such as refusal mechanisms and content filters. This paper contributes to the field by highlighting the limitations of current LLM safety protocols and demonstrating the need for enhanced multi-turn safety auditing and agent-level control frameworks. The methodology involves creating a fully automated scam pipeline that transforms scam scripts into lifelike voice calls using advanced text-to-speech systems, achieving significant realism in simulating fraud scenarios, thereby underscoring the urgency for new detection and disruption methods against conversational deception powered by generative AI.</div>
<div class="mono" style="margin-top:8px">本文关注大型语言模型（LLMs）在生成真实的诈骗电话方面的滥用问题。以往的方法主要集中在单次提示的滥用，这对于更复杂的威胁显得不足。所提出的ScamAgent通过保持对话记忆并动态适应用户响应，采用多轮对话中的欺骗策略，从而与以往方法不同。这种方法的提出是为了应对不断演变的AI驱动诈骗所需的安全性。研究方法包括创建一个自主的多轮代理，能够生成诈骗电话脚本并使用文本转语音系统将其转化为语音电话。研究结果表明，现有的LLM安全机制对这种基于代理的威胁无效，强调了改进多轮安全审计和检测对话欺骗方法的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Humans overrely on overconfident language models, across languages</div>
<div class="meta-line">Authors: Neil Rathi, Dan Jurafsky, Kaitlyn Zhou</div>
<div class="meta-line">First: 2025-07-08T18:01:01+00:00 · Latest: 2025-08-08T00:50:04+00:00</div>
<div class="meta-line">Comments: camera ready</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.06306v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.06306v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) are deployed globally, it is crucial that
their responses are calibrated across languages to accurately convey
uncertainty and limitations. Prior work shows that LLMs are linguistically
overconfident in English, leading users to overrely on confident generations.
However, the usage and interpretation of epistemic markers (e.g., &#x27;I think
it&#x27;s&#x27;) differs sharply across languages. Here, we study the risks of
multilingual linguistic (mis)calibration, overconfidence, and overreliance
across five languages to evaluate LLM safety in a global context. Our work
finds that overreliance risks are high across languages. We first analyze the
distribution of LLM-generated epistemic markers and observe that LLMs are
overconfident across languages, frequently generating strengtheners even as
part of incorrect responses. Model generations are, however, sensitive to
documented cross-linguistic variation in usage: for example, models generate
the most markers of uncertainty in Japanese and the most markers of certainty
in German and Mandarin. Next, we measure human reliance rates across languages,
finding that reliance behaviors differ cross-linguistically: for example,
participants are significantly more likely to discount expressions of
uncertainty in Japanese than in English (i.e., ignore their &#x27;hedging&#x27; function
and rely on generations that contain them). Taken together, these results
indicate a high risk of reliance on overconfident model generations across
languages. Our findings highlight the challenges of multilingual linguistic
calibration and stress the importance of culturally and linguistically
contextualized model safety evaluations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人类在多语言环境中过度依赖自信的语言模型</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在全球范围内的部署，确保其响应在不同语言中经过校准，以准确传达不确定性和局限性至关重要。先前的研究表明，LLMs在英语中表现出语言上的过度自信，导致用户过度依赖自信的生成。然而，认知标记（例如，“我认为”）的使用和解释在不同语言中差异显著。在此，我们研究了五种语言中多语言语言学（误）校准、过度自信和过度依赖的风险，以评估LLM在全球背景下的安全性。我们的研究发现，各种语言中的过度依赖风险很高。我们首先分析了LLM生成的认知标记的分布，观察到LLMs在不同语言中表现出过度自信，常常生成强化词，即使在错误的响应中也是如此。然而，模型生成对已记录的跨语言使用变异敏感：例如，模型在日语中生成最多的不确定性标记，而在德语和普通话中生成最多的确定性标记。接下来，我们测量了不同语言中的人类依赖率，发现依赖行为在跨语言中存在差异：例如，参与者在日语中显著更可能忽视不确定性表达，而不是在英语中（即，忽视其“保留”功能，依赖包含这些表达的生成）。综合来看，这些结果表明，在不同语言中对过度自信的模型生成的依赖风险很高。我们的发现突显了多语言语言学校准的挑战，并强调了文化和语言背景下模型安全评估的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research investigates the overreliance on large language models (LLMs) due to their overconfidence in generating responses, particularly across different languages. Previous studies have primarily focused on English, revealing that LLMs often present overconfident outputs, which can mislead users. This study extends the analysis to five languages, addressing the lack of multilingual calibration and highlighting the varying interpretations of epistemic markers across cultures. The methodology includes analyzing the distribution of LLM-generated epistemic markers and measuring human reliance rates, revealing significant cross-linguistic differences in how users interpret uncertainty. The findings indicate a high risk of overreliance on LLM outputs globally, emphasizing the need for culturally informed safety evaluations of language models.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）中的过度自信问题及其对不同语言用户依赖性的影响。先前的研究表明，LLMs在英语中表现出语言上的过度自信，导致用户可能误用其输出。本文提出了一种新方法，考察LLMs的多语言校准，揭示了不同语言中认知标记的解释存在显著差异，而传统方法对此未予重视。研究通过分析五种语言中LLM生成的响应，表明用户对这些模型的依赖程度在语言环境中存在显著差异。该方法论包括评估认知标记的分布和测量人类依赖率，最终发现不同语言中对过度自信模型输出的依赖风险较高，强调了在LLM部署中需要进行文化背景下的安全评估。</div>
</details>
</div>
<div class="card">
<div class="title">CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative   Adversarial Attacks on their Internal Representations</div>
<div class="meta-line">Authors: Xiaohu Li, Yunfeng Ning, Zepeng Bao, Mayi Xu, Jianhao Chen, Tieyun Qian</div>
<div class="meta-line">Venue: ACL 2025</div>
<div class="meta-line">First: 2025-07-08T14:45:21+00:00 · Latest: 2025-08-06T05:32:54+00:00</div>
<div class="meta-line">Comments: Accepted to ACL 2025 (Findings), camera-ready version</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.06043v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.06043v2">PDF</a> · <a href="https://github.com/NLPGM/CAVGAN">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Security alignment enables the Large Language Model (LLM) to gain the
protection against malicious queries, but various jailbreak attack methods
reveal the vulnerability of this security mechanism. Previous studies have
isolated LLM jailbreak attacks and defenses. We analyze the security protection
mechanism of the LLM, and propose a framework that combines attack and defense.
Our method is based on the linearly separable property of LLM intermediate
layer embedding, as well as the essence of jailbreak attack, which aims to
embed harmful problems and transfer them to the safe area. We utilize
generative adversarial network (GAN) to learn the security judgment boundary
inside the LLM to achieve efficient jailbreak attack and defense. The
experimental results indicate that our method achieves an average jailbreak
success rate of 88.85\% across three popular LLMs, while the defense success
rate on the state-of-the-art jailbreak dataset reaches an average of 84.17\%.
This not only validates the effectiveness of our approach but also sheds light
on the internal security mechanisms of LLMs, offering new insights for
enhancing model security The code and data are available at
https://github.com/NLPGM/CAVGAN.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CAVGAN：通过对大型语言模型内部表示的生成对抗攻击统一越狱与防御</div>
<div class="mono" style="margin-top:8px">安全对齐使大型语言模型（LLM）能够抵御恶意查询的攻击，但各种越狱攻击方法揭示了该安全机制的脆弱性。以往的研究将LLM越狱攻击与防御孤立开来。我们分析了LLM的安全保护机制，并提出了一个结合攻击与防御的框架。我们的方法基于LLM中间层嵌入的线性可分性特性，以及越狱攻击的本质，旨在嵌入有害问题并将其转移到安全区域。我们利用生成对抗网络（GAN）学习LLM内部的安全判断边界，以实现高效的越狱攻击与防御。实验结果表明，我们的方法在三种流行的LLM上实现了平均88.85%的越狱成功率，而在最先进的越狱数据集上的防御成功率达到了84.17%。这不仅验证了我们方法的有效性，还揭示了LLM的内部安全机制，为增强模型安全性提供了新思路。代码和数据可在https://github.com/NLPGM/CAVGAN获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of Large Language Models (LLMs) to jailbreak attacks, which exploit weaknesses in their security alignment mechanisms. Previous methods have typically treated attacks and defenses in isolation, failing to provide a unified approach that effectively addresses the inherent vulnerabilities. The proposed CAVGAN framework integrates both attack and defense strategies by leveraging the linearly separable properties of LLM intermediate layer embeddings and employing generative adversarial networks (GANs) to delineate security boundaries. This methodology demonstrates a significant contribution by achieving an average jailbreak success rate of 88.85% across three popular LLMs, alongside a defense success rate of 84.17% against state-of-the-art jailbreak datasets, thus validating its effectiveness and providing insights into enhancing LLM security.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLM）在监狱突破攻击下的脆弱性进行探讨，这些攻击揭示了其对恶意查询的安全对齐机制的弱点。以往的方法通常将攻击和防御视为孤立的过程，未能有效整合这两个方面。提出的方法CAVGAN通过利用LLM中间层嵌入的线性可分性特征，并采用生成对抗网络（GAN）学习LLM内部的安全判断边界，从而将攻击和防御统一起来。该方法不仅增强了对LLM安全机制的理解，还在最先进的数据集上实现了88.85%的监狱突破成功率和84.17%的防御成功率，证明了其在提高模型安全性方面的有效性。这些发现为攻击和防御在LLM中的双重性质提供了有价值的见解，为未来模型保护的进展铺平了道路。</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial-Guided Diffusion for Multimodal LLM Attacks</div>
<div class="meta-line">Authors: Chengwei Xia, Fan Ma, Ruijie Quan, Kun Zhan, Yi Yang</div>
<div class="meta-line">First: 2025-07-31T02:57:20+00:00 · Latest: 2025-07-31T02:57:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.23202v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.23202v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper addresses the challenge of generating adversarial image using a
diffusion model to deceive multimodal large language models (MLLMs) into
generating the targeted responses, while avoiding significant distortion of the
clean image. To address the above challenges, we propose an adversarial-guided
diffusion (AGD) approach for adversarial attack MLLMs. We introduce
adversarial-guided noise to ensure attack efficacy. A key observation in our
design is that, unlike most traditional adversarial attacks which embed
high-frequency perturbations directly into the clean image, AGD injects target
semantics into the noise component of the reverse diffusion. Since the added
noise in a diffusion model spans the entire frequency spectrum, the adversarial
signal embedded within it also inherits this full-spectrum property.
Importantly, during reverse diffusion, the adversarial image is formed as a
linear combination of the clean image and the noise. Thus, when applying
defenses such as a simple low-pass filtering, which act independently on each
component, the adversarial image within the noise component is less likely to
be suppressed, as it is not confined to the high-frequency band. This makes AGD
inherently robust to variety defenses. Extensive experiments demonstrate that
our AGD outperforms state-of-the-art methods in attack performance as well as
in model robustness to some defenses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对抗引导扩散用于多模态大语言模型攻击</div>
<div class="mono" style="margin-top:8px">本文解决了使用扩散模型生成对抗图像以欺骗多模态大语言模型（MLLMs）生成目标响应的挑战，同时避免对干净图像的显著失真。为了解决上述挑战，我们提出了一种对抗引导扩散（AGD）方法用于对抗攻击MLLMs。我们引入对抗引导噪声以确保攻击的有效性。我们设计中的一个关键观察是，与大多数传统对抗攻击直接将高频扰动嵌入干净图像不同，AGD将目标语义注入反向扩散的噪声组件中。由于扩散模型中添加的噪声跨越整个频谱，因此嵌入其中的对抗信号也继承了这种全频谱特性。重要的是，在反向扩散过程中，对抗图像作为干净图像和噪声的线性组合形成。因此，当应用诸如简单低通滤波等防御措施时，这些措施独立作用于每个组件，噪声组件中的对抗图像不太可能被抑制，因为它并不局限于高频带。这使得AGD本质上对各种防御具有鲁棒性。大量实验表明，我们的AGD在攻击性能以及模型对某些防御的鲁棒性方面优于最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the challenge of generating adversarial images that can deceive multimodal large language models (MLLMs) while maintaining the integrity of the original image. Previous methods typically embed high-frequency perturbations directly into the clean image, leading to significant distortions and vulnerabilities to defenses. The proposed adversarial-guided diffusion (AGD) approach differs by injecting target semantics into the noise component of the reverse diffusion process, allowing for a more effective and less detectable attack. The contribution of this research lies in its ability to create adversarial images that are robust against various defense mechanisms. The methodology involves using adversarial-guided noise to enhance attack efficacy, and extensive experiments show that AGD significantly outperforms existing methods in both attack performance and resilience to defenses.</div>
<div class="mono" style="margin-top:8px">本文研究了生成对抗性图像的挑战，以欺骗多模态大型语言模型（MLLM），同时保持原始图像的完整性。以往的方法通常直接将高频扰动嵌入干净图像中，这可能导致显著的失真和对防御的脆弱性。提出的对抗引导扩散（AGD）方法通过将对抗引导噪声注入逆扩散过程，增强了对各种防御的鲁棒性，使对抗信号能够跨越整个频率范围。该研究的贡献在于其新颖的方法，有效地将干净图像与噪声结合，创建对抗性图像，同时最小化失真。通过广泛的实验，AGD方法在攻击性能和对某些防御的抗性方面表现出优于现有最先进技术的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs</div>
<div class="meta-line">Authors: Xikang Yang, Biyu Zhou, Xuehai Tang, Jizhong Han, Songlin Hu</div>
<div class="meta-line">First: 2025-07-30T10:40:53+00:00 · Latest: 2025-07-30T10:40:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.22564v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.22564v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) demonstrate impressive capabilities across a
wide range of tasks, yet their safety mechanisms remain susceptible to
adversarial attacks that exploit cognitive biases -- systematic deviations from
rational judgment. Unlike prior jailbreaking approaches focused on prompt
engineering or algorithmic manipulation, this work highlights the overlooked
power of multi-bias interactions in undermining LLM safeguards. We propose
CognitiveAttack, a novel red-teaming framework that systematically leverages
both individual and combined cognitive biases. By integrating supervised
fine-tuning and reinforcement learning, CognitiveAttack generates prompts that
embed optimized bias combinations, effectively bypassing safety protocols while
maintaining high attack success rates. Experimental results reveal significant
vulnerabilities across 30 diverse LLMs, particularly in open-source models.
CognitiveAttack achieves a substantially higher attack success rate compared to
the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations
in current defense mechanisms. These findings highlight multi-bias interactions
as a powerful yet underexplored attack vector. This work introduces a novel
interdisciplinary perspective by bridging cognitive science and LLM safety,
paving the way for more robust and human-aligned AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用协同认知偏见绕过大型语言模型的安全性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在广泛任务中展现出令人印象深刻的能力，但其安全机制仍然容易受到利用认知偏见的对抗性攻击——即理性判断的系统性偏差。与以往专注于提示工程或算法操控的越狱方法不同，本研究强调了多重偏见交互在削弱LLM安全防护中的被忽视的力量。我们提出了CognitiveAttack，这是一种新颖的红队框架，系统性地利用个体和组合的认知偏见。通过整合监督微调和强化学习，CognitiveAttack生成嵌入优化偏见组合的提示，有效绕过安全协议，同时保持高攻击成功率。实验结果揭示了30种不同LLM的显著脆弱性，特别是在开源模型中。CognitiveAttack的攻击成功率显著高于当前最先进的黑箱方法PAP（60.1%对31.6%），暴露了当前防御机制的关键局限性。这些发现突显了多重偏见交互作为一种强大但未被充分探索的攻击向量。本研究通过将认知科学与LLM安全相结合，引入了一种新颖的跨学科视角，为更强大且与人类对齐的AI系统铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities of Large Language Models (LLMs) to adversarial attacks that exploit cognitive biases, which are systematic deviations from rational judgment. Previous methods primarily focused on prompt engineering or algorithmic manipulation, but they often overlooked the potential of multi-bias interactions, which this paper emphasizes as a significant factor in undermining LLM safety mechanisms. The proposed approach, CognitiveAttack, is a novel red-teaming framework that leverages both individual and combined cognitive biases through supervised fine-tuning and reinforcement learning to generate prompts that effectively bypass safety protocols. This methodology demonstrates a substantial increase in attack success rates, achieving 60.1% compared to the state-of-the-art black-box method PAP at 31.6%, revealing critical weaknesses in existing defenses and suggesting that multi-bias interactions could serve as a potent attack vector. Overall, the paper contributes an interdisciplinary perspective that integrates cognitive science with LLM safety, aiming to enhance the robustness and alignment of AI systems with human values.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在面对利用认知偏差进行的对抗性攻击时的脆弱性，认知偏差是指系统性偏离理性判断的现象。以往的方法主要集中在提示工程或算法操控上，但往往忽视了多重偏差交互在削弱安全机制方面的潜力。所提出的方法CognitiveAttack动机明确，系统性地利用个体和组合认知偏差来绕过LLM的安全防护。该方法结合了监督微调和强化学习，生成嵌入优化偏差组合的提示，从而实现了显著高于现有黑箱方法PAP的攻击成功率（60.1%对比31.6%）。实验结果揭示了30种不同LLM，尤其是开源模型的关键脆弱性，强调了针对这些多重偏差交互的防御机制改进的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Jailbreak Attacks on LLMs via Persona Prompts</div>
<div class="meta-line">Authors: Zheng Zhang, Peilin Zhao, Deheng Ye, Hao Wang</div>
<div class="meta-line">First: 2025-07-28T12:03:22+00:00 · Latest: 2025-07-28T12:03:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.22171v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.22171v1">PDF</a> · <a href="https://github.com/CjangCjengh/Generic_Persona">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Jailbreak attacks aim to exploit large language models (LLMs) by inducing
them to generate harmful content, thereby revealing their vulnerabilities.
Understanding and addressing these attacks is crucial for advancing the field
of LLM safety. Previous jailbreak approaches have mainly focused on direct
manipulations of harmful intent, with limited attention to the impact of
persona prompts. In this study, we systematically explore the efficacy of
persona prompts in compromising LLM defenses. We propose a genetic
algorithm-based method that automatically crafts persona prompts to bypass
LLM&#x27;s safety mechanisms. Our experiments reveal that: (1) our evolved persona
prompts reduce refusal rates by 50-70% across multiple LLMs, and (2) these
prompts demonstrate synergistic effects when combined with existing attack
methods, increasing success rates by 10-20%. Our code and data are available at
https://github.com/CjangCjengh/Generic_Persona.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过角色提示增强对大型语言模型的越狱攻击</div>
<div class="mono" style="margin-top:8px">越狱攻击旨在利用大型语言模型（LLMs），诱使它们生成有害内容，从而揭示其脆弱性。理解和应对这些攻击对推动LLM安全领域至关重要。以往的越狱方法主要集中在对有害意图的直接操控上，而对角色提示的影响关注有限。在本研究中，我们系统地探讨了角色提示在破坏LLM防御中的有效性。我们提出了一种基于遗传算法的方法，自动生成角色提示以绕过LLM的安全机制。我们的实验表明：（1）我们演化的角色提示在多个LLM中将拒绝率降低了50-70%；（2）这些提示与现有攻击方法结合时表现出协同效应，成功率提高了10-20%。我们的代码和数据可在https://github.com/CjangCjengh/Generic_Persona获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the vulnerabilities of large language models (LLMs) to jailbreak attacks, which exploit these models to generate harmful content. Previous methods primarily focused on direct manipulations of harmful intent, often overlooking the role of persona prompts, which this study identifies as a significant factor in LLM defenses. The proposed approach utilizes a genetic algorithm to automatically generate persona prompts that effectively bypass safety mechanisms, thus addressing the limitations of earlier methods. The contribution of this research lies in demonstrating that evolved persona prompts can significantly reduce refusal rates by 50-70% and enhance the effectiveness of existing attack methods by increasing success rates by 10-20%. The methodology involves systematic experimentation across multiple LLMs, confirming that the performance of the proposed method supports its goals of improving jailbreak attack efficacy.</div>
<div class="mono" style="margin-top:8px">本文研究了大型语言模型（LLMs）在监狱突破攻击中的脆弱性，这种攻击利用这些模型生成有害内容。以往的方法主要集中在对有害意图的直接操控，但忽视了角色提示在这些攻击中的作用。所提出的方法利用遗传算法自动生成角色提示，有效绕过LLM的安全机制，从而解决了早期方法的局限性。研究通过实验证明，进化的角色提示显著降低了拒绝率50-70%，并提高了现有攻击方法的成功率10-20%。该方法在多个LLM上进行了验证，支持了提高对这些模型进行监狱突破攻击有效性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning</div>
<div class="meta-line">Authors: Gabriel J. Perin, Runjin Chen, Xuxi Chen, Nina S. T. Hirata, Zhangyang Wang, Junyuan Hong</div>
<div class="meta-line">First: 2025-06-18T16:30:02+00:00 · Latest: 2025-07-25T18:57:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.15606v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.15606v3">PDF</a> · <a href="http://github.com/VITA-Group/LoX">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have become indispensable in real-world
applications. However, their widespread adoption raises significant safety
concerns, particularly in responding to socially harmful questions. Despite
substantial efforts to improve model safety through alignment, aligned models
can still have their safety protections undermined by subsequent fine-tuning -
even when the additional training data appears benign. In this paper, we
empirically demonstrate that this vulnerability stems from the sensitivity of
safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building
on this insight, we propose a novel training-free method, termed Low-Rank
Extrapolation (LoX), to enhance safety robustness by extrapolating the safety
subspace of an aligned LLM. Our experimental results confirm the effectiveness
of LoX, demonstrating significant improvements in robustness against both
benign and malicious fine-tuning attacks while preserving the model&#x27;s
adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute
reductions in attack success rates (ASR) facing benign or malicious fine-tuning
attacks. By investigating the ASR landscape of parameters, we attribute the
success of LoX to that the extrapolation moves LLM parameters to a flatter
zone, thereby less sensitive to perturbations. The code is available at
github.com/VITA-Group/LoX.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LoX：低秩外推增强LLM对微调的安全性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）在现实应用中变得不可或缺。然而，它们的广泛采用引发了重大安全隐患，特别是在回应社会有害问题时。尽管在对齐方面进行了大量努力以提高模型安全性，但对齐模型仍可能因后续微调而削弱其安全保护——即使额外的训练数据看似无害。本文通过实证研究表明，这种脆弱性源于LLM参数中安全关键低秩子空间对微调的敏感性。基于这一见解，我们提出了一种新颖的无训练方法，称为低秩外推（LoX），通过外推对齐LLM的安全子空间来增强安全鲁棒性。我们的实验结果确认了LoX的有效性，显示出在面对良性和恶意微调攻击时，鲁棒性显著提高，同时保持模型对新任务的适应性。例如，LoX在面对良性或恶意微调攻击时，攻击成功率（ASR）绝对降低了11%至54%。通过研究参数的ASR分布，我们将LoX的成功归因于外推将LLM参数移动到一个更平坦的区域，从而对扰动的敏感性降低。代码可在github.com/VITA-Group/LoX获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the safety concerns associated with the deployment of Large Language Models (LLMs), particularly their vulnerability to fine-tuning that can compromise safety protections. Previous methods aimed at enhancing model safety through alignment have proven inadequate, as aligned models can still be adversely affected by benign additional training data. The proposed approach, Low-Rank Extrapolation (LoX), is distinct in that it is a training-free method designed to enhance safety robustness by extrapolating the safety subspace of aligned LLMs, effectively addressing the sensitivity of low-rank subspaces to fine-tuning. The paper contributes by empirically demonstrating that LoX significantly improves robustness against both benign and malicious fine-tuning attacks, achieving reductions in attack success rates ranging from 11% to 54%, while maintaining the model&#x27;s adaptability to new tasks. This performance indicates that LoX effectively supports the goal of enhancing LLM safety without sacrificing functionality.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在安全性方面的担忧，特别是它们在微调过程中可能会削弱安全保护，即使额外的训练数据看似无害。以往通过对齐来提高模型安全性的方法效果不佳，因为它们未能充分保护LLM参数中低秩子空间的敏感性。提出的低秩外推（LoX）方法旨在通过外推对齐LLM的安全子空间来增强安全性鲁棒性，而无需额外训练。该方法的动机源于实证发现，显示出在面对良性和恶意微调攻击时，LoX显著提高了鲁棒性，攻击成功率降低幅度在11%到54%之间，同时保持模型对新任务的适应能力。该方法论涉及对参数的ASR（攻击成功率）景观进行分析，揭示LoX有效地将LLM参数置于一个更平坦的区域，从而使其对扰动的敏感性降低。</div>
</details>
</div>
<div class="card">
<div class="title">TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law</div>
<div class="meta-line">Authors: Zheng Hui, Yijiang River Dong, Ehsan Shareghi, Nigel Collier</div>
<div class="meta-line">First: 2025-07-22T17:52:29+00:00 · Latest: 2025-07-22T17:52:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.21134v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.21134v1">PDF</a> · <a href="https://github.com/zackhuiiiii/TRIDENT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) are increasingly deployed in high-risk
domains such as law, finance, and medicine, systematically evaluating their
domain-specific safety and compliance becomes critical. While prior work has
largely focused on improving LLM performance in these domains, it has often
neglected the evaluation of domain-specific safety risks. To bridge this gap,
we first define domain-specific safety principles for LLMs based on the AMA
Principles of Medical Ethics, the ABA Model Rules of Professional Conduct, and
the CFA Institute Code of Ethics. Building on this foundation, we introduce
Trident-Bench, a benchmark specifically targeting LLM safety in the legal,
financial, and medical domains. We evaluated 19 general-purpose and
domain-specialized models on Trident-Bench and show that it effectively reveals
key safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic
expectations, whereas domain-specialized models often struggle with subtle
ethical nuances. This highlights an urgent need for finer-grained
domain-specific safety improvements. By introducing Trident-Bench, our work
provides one of the first systematic resources for studying LLM safety in law
and finance, and lays the groundwork for future research aimed at reducing the
safety risks of deploying LLMs in professionally regulated fields. Code and
benchmark will be released at: https://github.com/zackhuiiiii/TRIDENT</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TRIDENT：金融、医学和法律领域大型语言模型安全性基准测试</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在法律、金融和医学等高风险领域的日益应用，系统评估其领域特定的安全性和合规性变得至关重要。虽然之前的研究主要集中在提高LLM在这些领域的性能，但往往忽视了对领域特定安全风险的评估。为填补这一空白，我们首先基于AMA医学伦理原则、ABA职业行为模型规则和CFA协会伦理规范定义了LLM的领域特定安全原则。在此基础上，我们推出了Trident-Bench，这是一个专门针对法律、金融和医学领域LLM安全性的基准测试。我们在Trident-Bench上评估了19个通用和领域专用模型，结果表明它有效揭示了关键的安全缺口——强大的通用模型（如GPT、Gemini）能够满足基本期望，而领域专用模型往往在微妙的伦理细节上表现不佳。这突显了对更细致的领域特定安全改进的迫切需求。通过引入Trident-Bench，我们的工作提供了研究法律和金融领域LLM安全性的首个系统性资源之一，并为未来旨在降低在专业监管领域部署LLM的安全风险的研究奠定了基础。代码和基准测试将发布在：https://github.com/zackhuiiiii/TRIDENT</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical need for evaluating the safety and compliance of large language models (LLMs) in high-risk domains such as law, finance, and medicine, an area that has been largely overlooked in previous studies focused on performance enhancement. Existing methods have primarily concentrated on improving model capabilities without adequately assessing domain-specific safety risks, leading to potential ethical issues. The proposed approach, Trident-Bench, establishes a benchmark that defines safety principles tailored to these domains, allowing for a systematic evaluation of LLMs against these standards. This paper contributes by providing a novel resource for assessing LLM safety, revealing significant safety gaps in both generalist and specialized models, and emphasizing the need for targeted improvements. The methodology involves evaluating 19 models on the Trident-Bench, demonstrating that while generalist models meet basic safety expectations, specialized models often fail to address complex ethical considerations, underscoring the necessity for enhanced safety measures in regulated fields.</div>
<div class="mono" style="margin-top:8px">本研究关注在法律、金融和医学等高风险领域评估大型语言模型（LLMs）的安全性和合规性的重要需求，以前的研究主要集中在性能提升上，而忽视了领域特定的安全风险。所提出的方法Trident-Bench通过建立基于既定伦理准则的领域特定安全原则，与现有方法有所不同，并系统性地对LLMs进行基准测试。该方法有效识别出显著的安全缺口，表明尽管通用模型能够满足基本期望，但专业模型往往无法处理复杂的伦理问题。本文为评估受监管领域LLM安全性提供了系统性资源，并展示了Trident-Bench能够突出针对性安全增强的迫切需求，从而支持减少在这些关键领域部署LLM相关风险的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Automated Safety Evaluations Across 20 Large Language Models: The Aymara   LLM Risk and Responsibility Matrix</div>
<div class="meta-line">Authors: Juan Manuel Contreras</div>
<div class="meta-line">First: 2025-07-19T18:49:16+00:00 · Latest: 2025-07-19T18:49:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.14719v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.14719v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
&amp; Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p &lt; .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>20种大型语言模型的自动安全评估：Aymara LLM风险与责任矩阵</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）越来越多地融入现实应用，规模化和严格的安全评估变得至关重要。本文介绍了Aymara AI，一个用于生成和管理定制化、政策基础的安全评估的程序化平台。Aymara AI将自然语言安全政策转化为对抗性提示，并使用经过人类判断验证的基于AI的评分者对模型响应进行评分。我们通过Aymara LLM风险与责任矩阵展示其能力，该矩阵评估了20种商业可用的LLMs在10个现实安全领域的表现。结果显示，性能差异显著，平均安全评分范围从86.2%到52.4%。虽然模型在虚假信息等成熟安全领域表现良好（平均=95.7%），但在更复杂或不明确的领域（特别是隐私与冒充）中表现不佳（平均=24.3%）。方差分析确认，安全评分在模型和领域之间存在显著差异（p &lt; .05）。这些发现强调了LLM安全性的不一致性和上下文依赖性，并突显了像Aymara AI这样的可扩展、可定制工具在支持负责任的AI开发和监督中的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing integration of large language models (LLMs) into real-world applications necessitates effective safety evaluations. Previous methods lacked scalability and rigor, often failing to adapt to diverse safety policies and contexts. The proposed Aymara AI platform addresses these issues by converting natural-language safety policies into adversarial prompts and employing an AI-based rater validated against human judgments, thereby providing a customizable and systematic evaluation approach. This paper contributes by introducing the Aymara LLM Risk and Responsibility Matrix, which assesses 20 commercially available LLMs across 10 safety domains, revealing significant performance disparities with mean safety scores ranging from 86.2% to 52.4%. The findings indicate that while models excel in established safety areas, they struggle in more complex domains, emphasizing the need for tailored evaluation tools to enhance responsible AI development.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在实际应用中日益增长的安全评估需求，强调了可扩展和严格的安全评估的重要性。以往的方法缺乏系统性，无法在不同上下文中进行一致的安全评估。提出的Aymara AI平台通过将自然语言安全政策转化为对抗性提示，并利用经过人类判断验证的AI评分器对模型响应进行评分，提供了一种新颖的方法，从而解决了现有方法的局限性。本文的贡献在于通过Aymara LLM风险与责任矩阵展示Aymara AI的能力，该矩阵评估了20个LLM在10个安全领域的表现，揭示了显著的性能差异。结果表明，尽管模型在已建立的安全领域表现良好，但在更复杂的领域中却面临挑战，突显了可定制评估工具在确保负责任的AI开发中的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM   Jailbreaking</div>
<div class="meta-line">Authors: Zhengye Han, Quanyan Zhu</div>
<div class="meta-line">First: 2025-07-10T22:37:47+00:00 · Latest: 2025-07-10T22:37:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.08207v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.08207v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) are increasingly deployed in critical
applications, the challenge of jailbreaking, where adversaries manipulate the
models to bypass safety mechanisms, has become a significant concern. This
paper presents a dynamic Stackelberg game framework to model the interactions
between attackers and defenders in the context of LLM jailbreaking. The
framework treats the prompt-response dynamics as a sequential extensive-form
game, where the defender, as the leader, commits to a strategy while
anticipating the attacker&#x27;s optimal responses. We propose a novel agentic AI
solution, the &quot;Purple Agent,&quot; which integrates adversarial exploration and
defensive strategies using Rapidly-exploring Random Trees (RRT). The Purple
Agent actively simulates potential attack trajectories and intervenes
proactively to prevent harmful outputs. This approach offers a principled
method for analyzing adversarial dynamics and provides a foundation for
mitigating the risk of jailbreaking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对大型语言模型越狱的动态斯塔克尔博格博弈框架</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在关键应用中的广泛部署，越狱的挑战，即对手操纵模型以绕过安全机制，已成为一个重要问题。本文提出了一种动态斯塔克尔博格博弈框架，以建模攻击者和防御者在LLM越狱背景下的互动。该框架将提示-响应动态视为一个顺序扩展形式的博弈，其中防御者作为领导者，承诺一个策略，同时预期攻击者的最佳响应。我们提出了一种新颖的代理AI解决方案——“紫色代理”，它结合了对抗性探索和防御策略，使用快速探索随机树（RRT）。紫色代理主动模拟潜在攻击轨迹，并主动干预以防止有害输出。这种方法为分析对抗动态提供了一种原则性的方法，并为减轻越狱风险奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concern of jailbreaking in large language models (LLMs), where adversaries exploit vulnerabilities to bypass safety mechanisms. Previous methods lacked a structured approach to model the interactions between attackers and defenders, leading to ineffective defenses. This paper proposes a dynamic Stackelberg game framework that treats the prompt-response dynamics as a sequential extensive-form game, allowing defenders to anticipate attackers&#x27; responses. The main contribution is the introduction of the &quot;Purple Agent,&quot; an agentic AI solution that employs Rapidly-exploring Random Trees (RRT) for adversarial exploration and proactive defense. The methodology effectively simulates potential attack trajectories and intervenes to prevent harmful outputs, demonstrating a principled approach to analyzing adversarial dynamics and enhancing defenses against jailbreaking in LLMs.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）被越狱的日益严重的问题，越狱是指对手操纵这些模型以绕过安全机制，尤其是在其在关键应用中部署日益增加的情况下。以往的方法在有效应对这些攻击方面存在困难，通常缺乏对攻击者和防御者之间互动的结构化建模。所提出的动态斯塔克尔博格博弈框架通过将提示-响应动态视为一个顺序扩展形式游戏，使防御者能够预测攻击者的反应，从而与众不同。该框架的动机明确，因为它提供了一种系统化分析对抗互动的方法。论文引入了“紫色代理”，该代理利用快速探索随机树（RRT）模拟攻击轨迹并主动干预，从而贡献了一种新颖的代理AI解决方案，以减轻越狱风险。该方法在建模对抗动态方面表现出有效性，为增强LLMs抵御越狱尝试的安全性提供了坚实基础。</div>
</details>
</div>
<div class="card">
<div class="title">Saffron-1: Safety Inference Scaling</div>
<div class="meta-line">Authors: Ruizhong Qiu, Gaotang Li, Tianxin Wei, Jingrui He, Hanghang Tong</div>
<div class="meta-line">First: 2025-06-06T18:05:45+00:00 · Latest: 2025-07-09T07:47:59+00:00</div>
<div class="meta-line">Comments: Previous title: &quot;Saffron-1: Towards an Inference Scaling Paradigm for
  LLM Safety Assurance&quot;</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.06444v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.06444v2">PDF</a> · <a href="https://github.com/q-rz/saffron">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://q-rz.github.io/p/saffron">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing safety assurance research has primarily focused on training-phase
alignment to instill safe behaviors into LLMs. However, recent studies have
exposed these methods&#x27; susceptibility to diverse jailbreak attacks.
Concurrently, inference scaling has significantly advanced LLM reasoning
capabilities but remains unexplored in the context of safety assurance.
Addressing this gap, our work pioneers inference scaling for robust and
effective LLM safety against emerging threats. We reveal that conventional
inference scaling techniques, despite their success in reasoning tasks, perform
poorly in safety contexts, even falling short of basic approaches like
Best-of-N Sampling. We attribute this inefficiency to a newly identified
challenge, the exploration--efficiency dilemma, arising from the high
computational overhead associated with frequent process reward model (PRM)
evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference
scaling paradigm tailored explicitly for safety assurance. Central to our
approach is the introduction of a multifurcation reward model (MRM) that
significantly reduces the required number of reward model evaluations. To
operationalize this paradigm, we further propose: (i) a partial supervision
training objective for MRM, (ii) a conservative exploration constraint to
prevent out-of-distribution explorations, and (iii) a Trie-based key--value
caching strategy that facilitates cache sharing across sequences during tree
search. Extensive experiments validate the effectiveness of our method.
Additionally, we publicly release our trained multifurcation reward model
(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)
to accelerate future research in LLM safety. Our code, model, and data are
publicly available at https://github.com/q-rz/saffron , and our project
homepage is at https://q-rz.github.io/p/saffron .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Saffron-1: 安全推理扩展</div>
<div class="mono" style="margin-top:8px">现有的安全保障研究主要集中在训练阶段的对齐，以培养大型语言模型（LLM）的安全行为。然而，最近的研究揭示了这些方法对各种越狱攻击的脆弱性。同时，推理扩展显著提升了LLM的推理能力，但在安全保障的背景下仍未得到探索。为填补这一空白，我们的工作开创了针对新兴威胁的稳健有效的LLM安全推理扩展。我们发现，尽管传统的推理扩展技术在推理任务中取得了成功，但在安全上下文中表现不佳，甚至不及基本方法如最佳N采样。我们将这种低效归因于新识别的挑战，即探索-效率困境，源于频繁的过程奖励模型（PRM）评估所带来的高计算开销。为克服这一困境，我们提出了SAFFRON，一种专门为安全保障量身定制的新型推理扩展范式。我们的方法的核心是引入多分叉奖励模型（MRM），显著减少所需的奖励模型评估次数。为实现这一范式，我们进一步提出：（i）MRM的部分监督训练目标，（ii）防止分布外探索的保守探索约束，以及（iii）基于Trie的键值缓存策略，促进树搜索过程中序列间的缓存共享。大量实验验证了我们方法的有效性。此外，我们公开发布了训练好的多分叉奖励模型（Saffron-1）和配套的令牌级安全奖励数据集（Safety4M），以加速未来在LLM安全方面的研究。我们的代码、模型和数据可在https://github.com/q-rz/saffron获取，项目主页为https://q-rz.github.io/p/saffron。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing safety assurance methods for large language models (LLMs), which have primarily focused on training-phase alignment but have proven vulnerable to jailbreak attacks. Traditional inference scaling techniques have shown success in reasoning tasks but fail in safety contexts, often underperforming compared to simpler methods like Best-of-N Sampling due to the exploration-efficiency dilemma caused by high computational costs of reward model evaluations. This paper introduces SAFFRON, a novel inference scaling paradigm designed specifically for safety assurance, incorporating a multifurcation reward model to reduce the number of necessary evaluations. The methodology includes a partial supervision training objective, a conservative exploration constraint, and a Trie-based key-value caching strategy. Experimental results demonstrate the effectiveness of SAFFRON, achieving improved safety performance, and the authors provide access to their trained model and safety reward dataset to support further research in LLM safety.</div>
<div class="mono" style="margin-top:8px">本研究解决了现有大型语言模型（LLM）安全保障方法的局限性，这些方法主要集中在训练阶段的对齐，但容易受到越狱攻击。传统的推理扩展技术在推理任务中取得了成功，但在安全上下文中表现不佳，往往不如像Best-of-N Sampling这样的简单方法，原因在于奖励模型评估的高计算成本导致的探索-效率困境。本文提出了SAFFRON，这是一种专门为安全保障设计的新型推理扩展范式，采用了多分叉奖励模型以减少必要评估的数量。该方法包括部分监督训练目标、保守探索约束和基于Trie的键值缓存策略。实验结果表明，SAFFRON有效增强了LLM的安全性，作者提供了他们训练的模型（Saffron-1）和安全奖励数据集（Safety4M），以支持该领域的进一步研究。</div>
</details>
</div>
<div class="card">
<div class="title">Circumventing Safety Alignment in Large Language Models Through   Embedding Space Toxicity Attenuation</div>
<div class="meta-line">Authors: Zhibo Zhang, Yuxi Li, Kailong Wang, Shuai Yuan, Ling Shi, Haoyu Wang</div>
<div class="meta-line">First: 2025-07-08T03:01:00+00:00 · Latest: 2025-07-08T03:01:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.08020v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.08020v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have achieved remarkable success across domains
such as healthcare, education, and cybersecurity. However, this openness also
introduces significant security risks, particularly through embedding space
poisoning, which is a subtle attack vector where adversaries manipulate the
internal semantic representations of input data to bypass safety alignment
mechanisms. While previous research has investigated universal perturbation
methods, the dynamics of LLM safety alignment at the embedding level remain
insufficiently understood. Consequently, more targeted and accurate adversarial
perturbation techniques, which pose significant threats, have not been
adequately studied.
  In this work, we propose ETTA (Embedding Transformation Toxicity
Attenuation), a novel framework that identifies and attenuates
toxicity-sensitive dimensions in embedding space via linear transformations.
ETTA bypasses model refusal behaviors while preserving linguistic coherence,
without requiring model fine-tuning or access to training data. Evaluated on
five representative open-source LLMs using the AdvBench benchmark, ETTA
achieves a high average attack success rate of 88.61%, outperforming the best
baseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR
on instruction-tuned defenses). These results highlight a critical
vulnerability in current alignment strategies and underscore the need for
embedding-aware defenses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过嵌入空间毒性减弱规避大型语言模型的安全对齐</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在医疗、教育和网络安全等领域取得了显著成功。然而，这种开放性也带来了重大安全风险，特别是通过嵌入空间中毒，这是一种微妙的攻击向量，攻击者操纵输入数据的内部语义表示以绕过安全对齐机制。尽管之前的研究探讨了通用扰动方法，但LLM在嵌入层面的安全对齐动态仍然理解不足。因此，更具针对性和准确性的对抗扰动技术尚未得到充分研究，这些技术构成了重大威胁。 在本研究中，我们提出了ETTA（嵌入变换毒性减弱），这是一个新颖的框架，通过线性变换识别和减弱嵌入空间中的毒性敏感维度。ETTA在不需要模型微调或访问训练数据的情况下，绕过模型拒绝行为，同时保持语言连贯性。在使用AdvBench基准对五个代表性的开源LLM进行评估时，ETTA实现了88.61%的高平均攻击成功率，超越了最佳基线11.34%，并且能够推广到安全增强模型（例如，在指令调优防御上达到77.39%的ASR）。这些结果突显了当前对齐策略中的关键脆弱性，并强调了对嵌入感知防御的需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the security vulnerabilities of Large Language Models (LLMs), particularly focusing on embedding space poisoning, which allows adversaries to manipulate semantic representations to bypass safety mechanisms. Previous methods have primarily explored universal perturbation techniques, but they have not sufficiently tackled the dynamics of safety alignment at the embedding level, leaving a gap in understanding targeted adversarial attacks. The proposed approach, ETTA (Embedding Transformation Toxicity Attenuation), offers a novel framework that identifies and mitigates toxicity-sensitive dimensions in the embedding space through linear transformations, effectively bypassing model refusal behaviors while maintaining linguistic coherence without the need for model fine-tuning or training data access. The methodology was evaluated on five open-source LLMs using the AdvBench benchmark, achieving an average attack success rate of 88.61%, which surpasses the best baseline by 11.34%, and demonstrates generalizability to safety-enhanced models, revealing significant vulnerabilities in current alignment strategies and emphasizing the necessity for embedding-aware defenses.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）中嵌入空间中毒所带来的重大安全风险，这可能会破坏安全对齐机制。以往的方法主要集中在通用扰动技术上，但对LLM在嵌入层面的安全动态研究不足，导致对针对性对抗攻击的理解存在空白。提出的方法ETTA（嵌入变换毒性减弱）提供了一种新颖的框架，通过线性变换识别和减轻嵌入空间中的毒性敏感维度，有效绕过模型拒绝行为，同时在不需要模型微调或访问训练数据的情况下保持语言连贯性。本文的贡献在于证明ETTA在五个开源LLM上实现了88.61%的平均攻击成功率，超越了现有基准，表明当前对齐策略存在关键漏洞，从而强调了对嵌入感知防御的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity   Bounds on LLM Jailbreaking</div>
<div class="meta-line">Authors: Aldan Creo, Raul Castro Fernandez, Manuel Cebrian</div>
<div class="meta-line">First: 2025-07-06T08:41:30+00:00 · Latest: 2025-07-06T08:41:30+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/ACMCMC/risky-conversations Results:
  https://huggingface.co/risky-conversations Visualizer:
  https://huggingface.co/spaces/risky-conversations/Visualizer</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.08014v1">Abs</a> · <a href="http://arxiv.org/pdf/2507.08014v1">PDF</a> · <a href="https://github.com/ACMCMC/risky-conversations">Code1</a> · <a href="https://huggingface.co/risky-conversations">Code2</a> · <a href="https://huggingface.co/spaces/risky-conversations/Visualizer">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) become increasingly deployed, understanding
the complexity and evolution of jailbreaking strategies is critical for AI
safety.
  We present a mass-scale empirical analysis of jailbreak complexity across
over 2 million real-world conversations from diverse platforms, including
dedicated jailbreaking communities and general-purpose chatbots. Using a range
of complexity metrics spanning probabilistic measures, lexical diversity,
compression ratios, and cognitive load indicators, we find that jailbreak
attempts do not exhibit significantly higher complexity than normal
conversations. This pattern holds consistently across specialized jailbreaking
communities and general user populations, suggesting practical bounds on attack
sophistication. Temporal analysis reveals that while user attack toxicity and
complexity remains stable over time, assistant response toxicity has decreased,
indicating improving safety mechanisms. The absence of power-law scaling in
complexity distributions further points to natural limits on jailbreak
development.
  Our findings challenge the prevailing narrative of an escalating arms race
between attackers and defenders, instead suggesting that LLM safety evolution
is bounded by human ingenuity constraints while defensive measures continue
advancing. Our results highlight critical information hazards in academic
jailbreak disclosure, as sophisticated attacks exceeding current complexity
baselines could disrupt the observed equilibrium and enable widespread harm
before defensive adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大规模分析真实对话揭示LLM越狱复杂性界限</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）的广泛应用，理解越狱策略的复杂性和演变对AI安全至关重要。我们对来自多个平台的200多万条真实对话进行了大规模实证分析，涵盖专门的越狱社区和通用聊天机器人。使用一系列复杂性指标，包括概率测量、词汇多样性、压缩比和认知负荷指标，我们发现越狱尝试的复杂性并没有显著高于正常对话。这一模式在专门的越狱社区和普通用户群体中始终如一，表明攻击复杂性存在实际界限。时间分析显示，尽管用户攻击的毒性和复杂性随时间保持稳定，但助手的响应毒性有所下降，表明安全机制在改善。复杂性分布中缺乏幂律缩放进一步指向越狱发展的自然限制。我们的发现挑战了攻击者与防御者之间升级军备竞赛的主流叙述，反而表明LLM安全演变受到人类创造力限制的约束，而防御措施仍在不断进步。我们的结果突显了学术越狱披露中的关键信息风险，因为超出当前复杂性基准的复杂攻击可能会破坏观察到的平衡，并在防御适应之前造成广泛危害。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the growing concern of jailbreaking strategies in large language models (LLMs) as their deployment increases, emphasizing the need for understanding their complexity for AI safety. Previous methods primarily focused on isolated instances of jailbreaking without a comprehensive analysis of their complexity across diverse contexts, leading to an incomplete understanding of the phenomenon. The proposed approach conducts a mass-scale empirical analysis of over 2 million real-world conversations, utilizing various complexity metrics such as probabilistic measures and cognitive load indicators, which reveals that jailbreak attempts do not show significantly higher complexity than normal conversations. This research contributes to the discourse on LLM safety by suggesting that the evolution of jailbreaking is constrained by human ingenuity, and highlights the importance of understanding the limits of attack sophistication. The findings indicate that while user attack toxicity remains stable, the toxicity of assistant responses has decreased, suggesting improvements in safety mechanisms, and the absence of power-law scaling in complexity distributions points to natural limits on jailbreak development.</div>
<div class="mono" style="margin-top:8px">本文探讨了随着大型语言模型（LLMs）部署的增加，理解越狱策略的复杂性和演变的紧迫性，强调了人工智能安全的重要性。以往的方法主要集中在轶事证据或有限的数据集上，往往未能捕捉到越狱尝试及其复杂性的更广泛景观。提出的方法涉及对来自各种平台的超过200万条真实对话进行大规模实证分析，利用一系列复杂性指标，如概率测量和认知负荷指标。该方法揭示越狱尝试的复杂性并未显著高于正常对话，表明攻击复杂性存在实际限制，并挑战了LLM安全领域中不断升级的军备竞赛的观念。研究结果表明，尽管用户攻击复杂性保持稳定，但助手响应的安全机制有所改善，从而为人工智能安全动态和与越狱披露相关的潜在风险提供了有价值的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Cannot See the Forest for the Trees: Invoking Heuristics and Biases to   Elicit Irrational Choices of LLMs</div>
<div class="meta-line">Authors: Haoming Yang, Ke Ma, Xiaojun Jia, Yingfei Sun, Qianqian Xu, Qingming Huang</div>
<div class="meta-line">First: 2025-05-03T05:28:11+00:00 · Latest: 2025-06-27T08:31:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.02862v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.02862v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the remarkable performance of Large Language Models (LLMs), they
remain vulnerable to jailbreak attacks, which can compromise their safety
mechanisms. Existing studies often rely on brute-force optimization or manual
design, failing to uncover potential risks in real-world scenarios. To address
this, we propose a novel jailbreak attack framework, ICRT, inspired by
heuristics and biases in human cognition. Leveraging the simplicity effect, we
employ cognitive decomposition to reduce the complexity of malicious prompts.
Simultaneously, relevance bias is utilized to reorganize prompts, enhancing
semantic alignment and inducing harmful outputs effectively. Furthermore, we
introduce a ranking-based harmfulness evaluation metric that surpasses the
traditional binary success-or-failure paradigm by employing ranking aggregation
methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify
the harmfulness of generated content. Experimental results show that our
approach consistently bypasses mainstream LLMs&#x27; safety mechanisms and generates
high-risk content, providing insights into jailbreak attack risks and
contributing to stronger defense strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无法看清森林中的树木：利用启发式和偏见引发大型语言模型的非理性选择</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型（LLMs）表现出色，但它们仍然容易受到越狱攻击，这可能会危及其安全机制。现有研究通常依赖于暴力优化或手动设计，未能揭示现实场景中的潜在风险。为此，我们提出了一种新颖的越狱攻击框架ICRT，灵感来自人类认知中的启发式和偏见。利用简单性效应，我们采用认知分解来降低恶意提示的复杂性。同时，利用相关性偏见重新组织提示，增强语义对齐，有效诱导有害输出。此外，我们引入了一种基于排名的有害性评估指标，通过采用排名聚合方法（如Elo、HodgeRank和Rank Centrality）超越传统的二元成功或失败范式，全面量化生成内容的有害性。实验结果表明，我们的方法始终能够绕过主流LLMs的安全机制并生成高风险内容，为越狱攻击风险提供了见解，并有助于更强的防御策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the vulnerabilities of Large Language Models (LLMs) to jailbreak attacks, which can undermine their safety mechanisms. Previous methods have primarily relied on brute-force optimization or manual design, which do not effectively identify potential risks in practical applications. The proposed approach, ICRT, is motivated by human cognitive heuristics and biases, utilizing cognitive decomposition to simplify malicious prompts and relevance bias to enhance semantic alignment, thereby improving the effectiveness of the attacks. The paper contributes a novel ranking-based harmfulness evaluation metric that offers a more nuanced assessment of generated content compared to traditional binary measures. Experimental results demonstrate that ICRT consistently circumvents the safety mechanisms of mainstream LLMs, generating high-risk outputs and providing valuable insights for developing stronger defense strategies against such attacks.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在越狱攻击下的脆弱性，这些攻击可能会破坏其安全机制。以往的方法主要依赖于暴力优化或手动设计，往往未能识别现实应用中的潜在风险。所提出的方法ICRT与众不同，它借鉴了人类认知中的启发式和偏见，利用认知分解简化恶意提示，并利用相关性偏见增强语义对齐，从而有效诱导有害输出。本文贡献了一种新的基于排名的有害性评估指标，通过采用排名聚合技术改善传统的二元成功度量。实验结果表明，ICRT能够持续绕过主流LLMs的安全机制并生成高风险内容，从而为越狱攻击风险提供了有价值的见解，并为更强的防御策略提供了依据。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Jailbreak Oracle</div>
<div class="meta-line">Authors: Shuyi Lin, Anshuman Suri, Alina Oprea, Cheng Tan</div>
<div class="meta-line">First: 2025-06-17T20:37:29+00:00 · Latest: 2025-06-17T20:37:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.17299v1">Abs</a> · <a href="http://arxiv.org/pdf/2506.17299v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) become increasingly deployed in
safety-critical applications, the lack of systematic methods to assess their
vulnerability to jailbreak attacks presents a critical security gap. We
introduce the jailbreak oracle problem: given a model, prompt, and decoding
strategy, determine whether a jailbreak response can be generated with
likelihood exceeding a specified threshold. This formalization enables a
principled study of jailbreak vulnerabilities. Answering the jailbreak oracle
problem poses significant computational challenges -- the search space grows
exponentially with the length of the response tokens. We present Boa, the first
efficient algorithm for solving the jailbreak oracle problem. Boa employs a
three-phase search strategy: (1) constructing block lists to identify refusal
patterns, (2) breadth-first sampling to identify easily accessible jailbreaks,
and (3) depth-first priority search guided by fine-grained safety scores to
systematically explore promising low-probability paths. Boa enables rigorous
security assessments including systematic defense evaluation, standardized
comparison of red team attacks, and model certification under extreme
adversarial conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM 越狱预言机</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在安全关键应用中的广泛部署，缺乏系统的方法来评估它们对越狱攻击的脆弱性，形成了一个关键的安全漏洞。我们引入了越狱预言机问题：给定一个模型、提示和解码策略，确定是否可以生成一个越狱响应，其概率超过指定阈值。这一形式化使得对越狱脆弱性的原则性研究成为可能。回答越狱预言机问题面临显著的计算挑战——搜索空间随着响应令牌长度的增加而呈指数增长。我们提出了 Boa，这是解决越狱预言机问题的第一个高效算法。Boa 采用三阶段搜索策略：（1）构建阻止列表以识别拒绝模式，（2）广度优先采样以识别易于访问的越狱，以及（3）基于细粒度安全评分的深度优先优先搜索，以系统地探索有前景的低概率路径。Boa 使得严格的安全评估成为可能，包括系统的防御评估、红队攻击的标准化比较，以及在极端对抗条件下的模型认证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical security gap in assessing the vulnerability of large language models (LLMs) to jailbreak attacks, particularly as these models are increasingly used in safety-critical applications. Previous methods lacked systematic approaches to evaluate such vulnerabilities, leading to significant challenges in determining the likelihood of jailbreak responses. The proposed approach, termed the jailbreak oracle problem, formalizes this issue and introduces Boa, an efficient algorithm that employs a three-phase search strategy to systematically explore potential jailbreak responses. This methodology allows for rigorous security assessments, including defense evaluations and model certifications under adversarial conditions, achieving a more reliable performance in identifying vulnerabilities compared to existing methods. The paper contributes to the field by providing a structured framework for evaluating LLM security against jailbreak attacks, thereby enhancing the overall safety of their deployment.</div>
<div class="mono" style="margin-top:8px">本研究解决了在安全关键应用中评估大型语言模型（LLM）对越狱攻击脆弱性的关键安全缺口。以往的方法缺乏系统性评估此类脆弱性的手段，导致识别和缓解风险的挑战。所提出的方法被称为越狱神谕问题，形式化了评估过程，并引入了Boa，这是一种高效算法，利用三阶段搜索策略来应对与此问题相关的计算挑战。Boa的方法包括构建阻止列表以识别拒绝模式、广度优先采样以识别易于访问的越狱，以及基于安全评分的深度优先优先搜索。本文表明，Boa能够进行严格的安全评估，促进系统性防御评估和在极端对抗条件下的模型认证，从而显著推动了LLM安全领域的发展。</div>
</details>
</div>
<div class="card">
<div class="title">We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered   Agent Systems</div>
<div class="meta-line">Authors: Junfeng Fang, Zijun Yao, Ruipeng Wang, Haokai Ma, Xiang Wang, Tat-Seng Chua</div>
<div class="meta-line">First: 2025-06-16T16:24:31+00:00 · Latest: 2025-06-16T16:24:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.13666v1">Abs</a> · <a href="http://arxiv.org/pdf/2506.13666v1">PDF</a> · <a href="https://github.com/littlelittlenine/SafeMCP.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of large language models (LLMs) has entered in a
experience-driven era, flagged by the emergence of environment feedback-driven
learning via reinforcement learning and tool-using agents. This encourages the
emergenece of model context protocol (MCP), which defines the standard on how
should a LLM interact with external services, such as \api and data. However,
as MCP becomes the de facto standard for LLM agent systems, it also introduces
new safety risks. In particular, MCP introduces third-party services, which are
not controlled by the LLM developers, into the agent systems. These third-party
MCP services provider are potentially malicious and have the economic
incentives to exploit vulnerabilities and sabotage user-agent interactions. In
this position paper, we advocate the research community in LLM safety to pay
close attention to the new safety risks issues introduced by MCP, and develop
new techniques to build safe MCP-powered agent systems. To establish our
position, we argue with three key parts. (1) We first construct \framework, a
controlled framework to examine safety issues in MCP-powered agent systems. (2)
We then conduct a series of pilot experiments to demonstrate the safety risks
in MCP-powered agent systems is a real threat and its defense is not trivial.
(3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered
agent systems. In particular, we would call for researchers to persue the
following research directions: red teaming, MCP safe LLM development, MCP
safety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP
safe ecosystem construction. We hope this position paper can raise the
awareness of the research community in MCP safety and encourage more
researchers to join this important research direction. Our code is available at
https://github.com/littlelittlenine/SafeMCP.git.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我们应该识别和减轻MCP驱动的代理系统中的第三方安全风险</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的发展已进入一个以经验为驱动的时代，标志着通过强化学习和工具使用代理的环境反馈驱动学习的出现。这促进了模型上下文协议（MCP）的出现，该协议定义了LLM与外部服务（如API和数据）交互的标准。然而，随着MCP成为LLM代理系统的事实标准，它也引入了新的安全风险。特别是，MCP将不受LLM开发者控制的第三方服务引入代理系统。这些第三方MCP服务提供商可能是恶意的，并且有经济动机利用漏洞和破坏用户与代理的交互。在这篇立场论文中，我们呼吁LLM安全研究社区密切关注MCP引入的新安全风险问题，并开发新技术以构建安全的MCP驱动代理系统。为了确立我们的立场，我们通过三个关键部分进行论证。(1) 我们首先构建了一个受控框架来检查MCP驱动代理系统中的安全问题。(2) 然后，我们进行了一系列初步实验，以证明MCP驱动代理系统中的安全风险是真实威胁，其防御并非易事。(3) 最后，我们通过展示构建安全MCP驱动代理系统的路线图来展望未来。特别是，我们呼吁研究人员追求以下研究方向：红队测试、MCP安全LLM开发、MCP安全评估、MCP安全数据积累、MCP服务保护和MCP安全生态系统建设。我们希望这篇立场论文能够提高研究社区对MCP安全的关注，并鼓励更多研究人员加入这一重要研究方向。我们的代码可在https://github.com/littlelittlenine/SafeMCP.git获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the emerging safety risks associated with the use of model context protocols (MCP) in large language model (LLM) agent systems, which have become prevalent due to advancements in reinforcement learning and tool-using agents. Previous methods have not adequately addressed the vulnerabilities introduced by third-party services that MCP relies on, which can be malicious and exploitative. The proposed approach emphasizes the need for a controlled framework to assess these safety issues and advocates for the development of new techniques to ensure safe interactions within MCP-powered systems. The authors conduct pilot experiments demonstrating that safety risks in these systems are significant and require serious attention. The paper contributes by outlining a roadmap for future research directions in MCP safety, aiming to enhance awareness and encourage further investigation into this critical area of LLM safety.</div>
<div class="mono" style="margin-top:8px">本文探讨了在大型语言模型（LLM）代理系统中使用模型上下文协议（MCP）所带来的新安全风险，特别是由于引入可能利用漏洞的第三方服务。以往的方法未能充分解决这些风险，因为它们通常忽视外部服务提供者的潜在恶意意图。提出的方法强调需要一个受控框架来评估安全问题，并倡导开发新技术以确保MCP驱动系统的安全性。作者通过概述一系列试点实验，突显了MCP带来的实际威胁，并提出未来研究方向的路线图，包括红队测试和安全评估。该方法论包括构建安全评估框架和进行实验，以证明解决这些风险的必要性，最终旨在增强MCP驱动代理系统的安全性。</div>
</details>
</div>
<div class="card">
<div class="title">Monitoring Decomposition Attacks in LLMs with Lightweight Sequential   Monitors</div>
<div class="meta-line">Authors: Chen Yueh-Han, Nitish Joshi, Yulin Chen, Maksym Andriushchenko, Rico Angell, He He</div>
<div class="meta-line">First: 2025-06-12T17:50:58+00:00 · Latest: 2025-06-14T15:17:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.10949v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.10949v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current LLM safety defenses fail under decomposition attacks, where a
malicious goal is decomposed into benign subtasks that circumvent refusals. The
challenge lies in the existing shallow safety alignment techniques: they only
detect harm in the immediate prompt and do not reason about long-range intent,
leaving them blind to malicious intent that emerges over a sequence of
seemingly benign instructions. We therefore propose adding an external monitor
that observes the conversation at a higher granularity. To facilitate our study
of monitoring decomposition attacks, we curate the largest and most diverse
dataset to date, including question-answering, text-to-image, and agentic
tasks. We verify our datasets by testing them on frontier LLMs and show an 87%
attack success rate on average on GPT-4o. This confirms that decomposition
attack is broadly effective. Additionally, we find that random tasks can be
injected into the decomposed subtasks to further obfuscate malicious intents.
To defend in real time, we propose a lightweight sequential monitoring
framework that cumulatively evaluates each subtask. We show that a carefully
prompt engineered lightweight monitor achieves a 93% defense success rate,
beating reasoning models like o3 mini as a monitor. Moreover, it remains robust
against random task injection and cuts cost by 90% and latency by 50%. Our
findings suggest that lightweight sequential monitors are highly effective in
mitigating decomposition attacks and are viable in deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用轻量级顺序监控器监测大型语言模型中的分解攻击</div>
<div class="mono" style="margin-top:8px">当前大型语言模型的安全防御在分解攻击下失效，其中恶意目标被分解为规避拒绝的良性子任务。挑战在于现有的浅层安全对齐技术：它们仅检测即时提示中的危害，而不推理长期意图，使其对一系列看似良性指令中出现的恶意意图视而不见。因此，我们建议添加一个外部监控器，以更高的粒度观察对话。为了促进我们对监测分解攻击的研究，我们策划了迄今为止最大和最具多样性的数据集，包括问答、文本到图像和代理任务。我们通过在前沿大型语言模型上测试这些数据集来验证它们，并显示在GPT-4o上平均攻击成功率为87%。这证实了分解攻击的广泛有效性。此外，我们发现随机任务可以注入到分解的子任务中，以进一步模糊恶意意图。为了实时防御，我们提出了一种轻量级顺序监控框架，逐步评估每个子任务。我们展示了经过精心提示工程设计的轻量级监控器实现了93%的防御成功率，超越了像o3 mini这样的推理模型作为监控器。此外，它在随机任务注入下仍然保持稳健，成本降低90%，延迟减少50%。我们的研究结果表明，轻量级顺序监控器在减轻分解攻击方面非常有效，并且在部署中是可行的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the vulnerability of large language models (LLMs) to decomposition attacks, where malicious goals are disguised as benign subtasks, which existing safety defenses fail to detect due to their focus on immediate prompts. Traditional methods are limited by their shallow safety alignment techniques, which do not consider long-range intent, leaving them ineffective against such attacks. The proposed approach introduces an external lightweight sequential monitor that evaluates subtasks cumulatively, providing a more comprehensive defense mechanism. The study contributes by curating the largest and most diverse dataset for testing decomposition attacks, achieving an average attack success rate of 87% on GPT-4o, and demonstrating that the lightweight monitor can achieve a 93% defense success rate while reducing costs and latency significantly. This indicates that the proposed method is not only effective but also practical for real-time deployment against decomposition attacks.</div>
<div class="mono" style="margin-top:8px">本文解决了当前大型语言模型（LLMs）安全防御在检测分解攻击方面的局限性，分解攻击是指恶意目标伪装成良性子任务。以往的方法主要集中在即时提示上，缺乏评估长期意图的能力，使其容易受到此类攻击。提出的方法引入了一个外部轻量级顺序监控器，以更高的粒度评估对话，有效解决了现有技术的不足。研究方法包括策划一个多样化的数据集，并在先进的LLMs上进行测试，结果显示GPT-4o的平均攻击成功率为87%。轻量级监控器在抵御分解攻击方面表现出93%的防御成功率，同时显著降低了成本和延迟，表明其在实时部署中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt   Generation for Enhanced LLM Content Moderation</div>
<div class="meta-line">Authors: Daniel Schwartz, Dmitriy Bespalov, Zhe Wang, Ninad Kulkarni, Yanjun Qi</div>
<div class="meta-line">First: 2025-01-28T17:10:20+00:00 · Latest: 2025-06-13T15:44:43+00:00</div>
<div class="meta-line">Comments: 14 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2501.18638v2">Abs</a> · <a href="http://arxiv.org/pdf/2501.18638v2">PDF</a> · <a href="https://github.com/dsbuddy/GAP-LLM-Safety">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) become increasingly prevalent, ensuring their
robustness against adversarial misuse is crucial. This paper introduces the GAP
(Graph of Attacks with Pruning) framework, an advanced approach for generating
stealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP
addresses limitations in existing tree-based LLM jailbreak methods by
implementing an interconnected graph structure that enables knowledge sharing
across attack paths. Our experimental evaluation demonstrates GAP&#x27;s superiority
over existing techniques, achieving a 20.8% increase in attack success rates
while reducing query costs by 62.7%. GAP consistently outperforms
state-of-the-art methods for attacking both open and closed LLMs, with attack
success rates of &gt;96%. Additionally, we present specialized variants like
GAP-Auto for automated seed generation and GAP-VLM for multimodal attacks.
GAP-generated prompts prove highly effective in improving content moderation
systems, increasing true positive detection rates by 108.5% and accuracy by
183.6% when used for fine-tuning. Our implementation is available at
https://github.com/dsbuddy/GAP-LLM-Safety.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>修剪攻击图：优化隐秘越狱提示生成以增强大型语言模型内容审核</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）日益普及，确保其对抗敌对滥用的鲁棒性至关重要。本文介绍了GAP（修剪攻击图）框架，这是一种生成隐秘越狱提示以评估和增强LLM保护措施的先进方法。GAP通过实现一个互联的图结构，解决了现有基于树的LLM越狱方法的局限性，从而实现了攻击路径之间的知识共享。我们的实验评估表明，GAP在现有技术中表现优越，攻击成功率提高了20.8%，查询成本降低了62.7%。GAP在攻击开放和封闭LLM方面始终优于最先进的方法，攻击成功率超过96%。此外，我们还提出了专门的变体，如用于自动种子生成的GAP-Auto和用于多模态攻击的GAP-VLM。GAP生成的提示在改善内容审核系统方面非常有效，微调时真正正例检测率提高了108.5%，准确率提高了183.6%。我们的实现可在https://github.com/dsbuddy/GAP-LLM-Safety获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing need for robust content moderation in large language models (LLMs) due to their increasing use and vulnerability to adversarial attacks. Previous methods primarily relied on tree-based structures for generating jailbreak prompts, which limited their effectiveness and efficiency. The proposed GAP (Graph of Attacks with Pruning) framework overcomes these limitations by utilizing an interconnected graph structure that facilitates knowledge sharing across various attack paths, making it a well-motivated advancement. This paper contributes a novel methodology that significantly enhances the generation of stealthy jailbreak prompts, achieving a 20.8% increase in attack success rates and a 62.7% reduction in query costs compared to existing techniques. The experimental results demonstrate that GAP consistently outperforms state-of-the-art methods, achieving over 96% attack success rates and substantially improving content moderation systems with increases of 108.5% in true positive detection rates and 183.6% in accuracy during fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在对抗性滥用方面的日益关注以及对强大内容审核的需求。以往的方法主要是基于树的越狱提示生成方法，存在效率和有效性方面的局限性。提出的GAP（修剪攻击图）框架引入了一个互联的图结构，增强了攻击路径之间的知识共享，有效克服了现有方法的不足。本文的贡献在于提出了一种新颖的方法论，不仅提高了攻击成功率，还显著降低了查询成本。实验结果表明，GAP实现了超过96%的攻击成功率，并提高了20.8%的有效性，同时在内容审核系统中也取得了108.5%的真实正例检测率提升和183.6%的准确率提升。</div>
</details>
</div>
<div class="card">
<div class="title">Activation Approximations Can Incur Safety Vulnerabilities Even in   Aligned LLMs: Comprehensive Analysis and Defense</div>
<div class="meta-line">Authors: Jiawen Zhang, Kejia Chen, Lipeng He, Jian Lou, Dan Li, Zunlei Feng, Mingli Song, Jian Liu, Kui Ren, Xiaohu Yang</div>
<div class="meta-line">First: 2025-02-02T16:25:48+00:00 · Latest: 2025-06-10T17:24:15+00:00</div>
<div class="meta-line">Comments: 20 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.00840v2">Abs</a> · <a href="http://arxiv.org/pdf/2502.00840v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have showcased remarkable capabilities across
various domains. Accompanying the evolving capabilities and expanding
deployment scenarios of LLMs, their deployment challenges escalate due to their
sheer scale and the advanced yet complex activation designs prevalent in
notable model series, such as Llama, Gemma, Mistral. These challenges have
become particularly pronounced in resource-constrained deployment scenarios,
where mitigating inference bottlenecks is imperative. Among various recent
efforts, activation approximation has emerged as a promising avenue for
pursuing inference efficiency, sometimes considered indispensable in
applications such as private inference. Despite achieving substantial speedups
with minimal impact on utility, even appearing sound and practical for
real-world deployment, the safety implications of activation approximations
remain unclear. In this work, we fill this critical gap in LLM safety by
conducting the first systematic safety evaluation of activation approximations.
Our safety vetting spans seven state-of-the-art techniques across three popular
categories (activation polynomialization, activation sparsification, and
activation quantization), revealing consistent safety degradation across ten
safety-aligned LLMs. To overcome the hurdle of devising a unified defense
accounting for diverse activation approximation methods, we perform an in-depth
analysis of their shared error patterns and uncover three key findings. We
propose QuadA, a novel safety enhancement method tailored to mitigate the
safety compromises introduced by activation approximations. Extensive
experiments and ablation studies corroborate QuadA&#x27;s effectiveness in enhancing
the safety capabilities of LLMs after activation approximations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>激活近似可能在对齐的LLM中引发安全漏洞：全面分析与防御</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）在各个领域展示了显著的能力。随着LLM能力的不断发展和部署场景的扩展，由于其庞大的规模以及在Llama、Gemma、Mistral等著名模型系列中普遍存在的先进而复杂的激活设计，部署挑战也在加剧。这些挑战在资源受限的部署场景中尤为明显，在这些场景中，缓解推理瓶颈至关重要。在最近的各种努力中，激活近似作为追求推理效率的有前景的途径而出现，有时在私有推理等应用中被视为不可或缺。尽管在效用上影响最小且实现了显著的加速，甚至在现实世界部署中看似合理和实用，但激活近似的安全隐患仍不明确。在本研究中，我们通过对激活近似进行首次系统的安全评估，填补了LLM安全的这一关键空白。我们的安全审查涵盖了三种流行类别（激活多项式化、激活稀疏化和激活量化）中的七种最先进技术，揭示了十种安全对齐的LLM中一致的安全退化。为了克服制定统一防御的难题，我们对这些激活近似方法的共享错误模式进行了深入分析，并发现了三个关键发现。我们提出了QuadA，这是一种新颖的安全增强方法，旨在减轻激活近似带来的安全妥协。广泛的实验和消融研究证实了QuadA在增强激活近似后LLM安全能力方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the safety vulnerabilities that arise from activation approximations in Large Language Models (LLMs), particularly in resource-constrained deployment scenarios where inference efficiency is crucial. Previous methods focused on activation approximation to enhance performance but often overlooked the potential safety risks associated with these techniques. This paper proposes a novel approach, QuadA, which systematically evaluates the safety implications of seven state-of-the-art activation approximation techniques and identifies consistent safety degradation across multiple LLMs. The methodology involves a comprehensive safety evaluation and an in-depth analysis of shared error patterns among different approximation methods. The experiments demonstrate that QuadA effectively enhances the safety capabilities of LLMs post-activation approximations, thereby supporting the goal of maintaining safety while improving inference efficiency.</div>
<div class="mono" style="margin-top:8px">本研究关注大语言模型（LLMs）中激活近似所带来的安全漏洞，这种方法因其在资源受限环境中的高效性而日益普遍。以往的方法主要集中在激活近似以提高推理速度，但往往忽视了潜在的安全风险，导致对其影响的理解存在空白。本文通过系统评估七种最先进的激活近似技术在十个安全对齐的LLMs中的安全性，揭示了一致的安全退化，从而作出了贡献。所提出的方法QuadA旨在通过解决这些技术中识别出的共享错误模式来增强安全性。实验结果表明，QuadA有效改善了激活近似后LLMs的安全性，支持了在实际应用中需要更安全的部署实践。</div>
</details>
</div>
<div class="card">
<div class="title">SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage</div>
<div class="meta-line">Authors: Xiaoning Dong, Wenbo Hu, Wei Xu, Tianxing He</div>
<div class="meta-line">Venue: ACL 2025</div>
<div class="meta-line">First: 2024-12-19T05:57:37+00:00 · Latest: 2025-06-09T11:35:57+00:00</div>
<div class="meta-line">Comments: To appear at Findings of ACL 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.15289v4">Abs</a> · <a href="http://arxiv.org/pdf/2412.15289v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have made significant advancements across
various tasks, but their safety alignment remain a major concern. Exploring
jailbreak prompts can expose LLMs&#x27; vulnerabilities and guide efforts to secure
them. Existing methods primarily design sophisticated instructions for the LLM
to follow, or rely on multiple iterations, which could hinder the performance
and efficiency of jailbreaks. In this work, we propose a novel jailbreak
paradigm, Simple Assistive Task Linkage (SATA), which can effectively
circumvent LLM safeguards and elicit harmful responses. Specifically, SATA
first masks harmful keywords within a malicious query to generate a relatively
benign query containing one or multiple [MASK] special tokens. It then employs
a simple assistive task such as a masked language model task or an element
lookup by position task to encode the semantics of the masked keywords.
Finally, SATA links the assistive task with the masked query to jointly perform
the jailbreak. Extensive experiments show that SATA achieves state-of-the-art
performance and outperforms baselines by a large margin. Specifically, on
AdvBench dataset, with mask language model (MLM) assistive task, SATA achieves
an overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and
with element lookup by position (ELP) assistive task, SATA attains an overall
ASR of 76% and HS of 4.43.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SATA：通过简单辅助任务链接实现LLM越狱的范式</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在各种任务上取得了显著进展，但其安全对齐仍然是一个主要问题。探索越狱提示可以揭示LLMs的脆弱性，并指导安全工作。现有方法主要设计复杂的指令供LLM遵循，或依赖多次迭代，这可能会影响越狱的性能和效率。在本研究中，我们提出了一种新颖的越狱范式——简单辅助任务链接（SATA），它可以有效绕过LLM的安全措施并引发有害响应。具体而言，SATA首先在恶意查询中屏蔽有害关键词，以生成一个相对无害的查询，其中包含一个或多个[MASK]特殊标记。然后，它采用简单的辅助任务，如掩码语言模型任务或按位置查找元素任务，来编码被屏蔽关键词的语义。最后，SATA将辅助任务与被屏蔽查询链接，以共同执行越狱。大量实验表明，SATA实现了最先进的性能，并大幅超越基线。具体而言，在AdvBench数据集上，使用掩码语言模型（MLM）辅助任务时，SATA的整体攻击成功率（ASR）为85%，有害评分（HS）为4.57；使用按位置查找元素（ELP）辅助任务时，SATA的整体ASR为76%，HS为4.43。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the significant safety alignment concerns associated with large language models (LLMs), particularly their vulnerabilities to jailbreak prompts. Previous methods have focused on creating complex instructions or using multiple iterations, which can negatively impact the efficiency and effectiveness of jailbreak attempts. The proposed Simple Assistive Task Linkage (SATA) paradigm differs by masking harmful keywords in queries and utilizing simple assistive tasks to encode the semantics of these keywords, thereby enhancing the jailbreak process. This approach is well-motivated as it directly targets the limitations of existing methods. The paper contributes a novel methodology that demonstrates state-of-the-art performance in jailbreak tasks, achieving an overall attack success rate of 85% and a harmful score of 4.57 on the AdvBench dataset using a masked language model assistive task, and 76% with a harmful score of 4.43 using an element lookup by position task.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在安全对齐方面的关键问题，尽管这些模型已经取得了显著进展，但仍然容易受到可以暴露其弱点的越狱提示的攻击。以往的方法主要集中在创建复杂的指令或使用多次迭代，这可能会对越狱尝试的效率和有效性产生负面影响。提出的简单辅助任务链接（SATA）范式提供了一种更高效的方法，通过在查询中屏蔽有害关键词，并利用简单的辅助任务来编码其语义，从而有效绕过LLM的安全防护。本文的贡献在于证明SATA在越狱任务中实现了最先进的性能，广泛实验表明，在AdvBench数据集上，使用掩码语言模型任务时，攻击成功率达到85%，有害分数为4.57，而使用位置元素查找任务时，攻击成功率为76%，有害分数为4.43，支持了增强越狱有效性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts</div>
<div class="meta-line">Authors: Torsten Krauß, Hamid Dashtbani, Alexandra Dmitrienko</div>
<div class="meta-line">First: 2025-06-09T09:54:25+00:00 · Latest: 2025-06-09T09:54:25+00:00</div>
<div class="meta-line">Comments: 26 pages, 25 tables, 13 figures, 2 algorithms, to appear in the 43th
  USENIX Security Symposium (USENIX Security 2025)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.07596v1">Abs</a> · <a href="http://arxiv.org/pdf/2506.07596v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning is advancing rapidly, with applications bringing notable
benefits, such as improvements in translation and code generation. Models like
ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated
into daily life. However, alongside these benefits, LLMs also introduce social
risks. Malicious users can exploit LLMs by submitting harmful prompts, such as
requesting instructions for illegal activities. To mitigate this, models often
include a security mechanism that automatically rejects such harmful prompts.
However, they can be bypassed through LLM jailbreaks. Current jailbreaks often
require significant manual effort, high computational costs, or result in
excessive model modifications that may degrade regular utility.
  We introduce TwinBreak, an innovative safety alignment removal method.
Building on the idea that the safety mechanism operates like an embedded
backdoor, TwinBreak identifies and prunes parameters responsible for this
functionality. By focusing on the most relevant model layers, TwinBreak
performs fine-grained analysis of parameters essential to model utility and
safety. TwinBreak is the first method to analyze intermediate outputs from
prompts with high structural and content similarity to isolate safety
parameters. We present the TwinPrompt dataset containing 100 such twin prompts.
Experiments confirm TwinBreak&#x27;s effectiveness, achieving 89% to 98% success
rates with minimal computational requirements across 16 LLMs from five vendors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TwinBreak：基于双重提示的LLM安全对齐越狱</div>
<div class="mono" style="margin-top:8px">机器学习正在迅速发展，其应用带来了显著的好处，例如翻译和代码生成的改进。像ChatGPT这样的模型，由大型语言模型（LLMs）驱动，正越来越多地融入日常生活。然而，除了这些好处，LLMs也带来了社会风险。恶意用户可以通过提交有害提示来利用LLMs，例如请求非法活动的指令。为了减轻这一问题，模型通常包括一种安全机制，自动拒绝此类有害提示。然而，这些机制可以通过LLM越狱来绕过。目前的越狱通常需要大量手动努力、高计算成本，或导致过度的模型修改，从而可能降低常规效用。我们提出了TwinBreak，一种创新的安全对齐移除方法。基于安全机制像嵌入式后门的理念，TwinBreak识别并修剪负责此功能的参数。通过关注最相关的模型层，TwinBreak对模型效用和安全性至关重要的参数进行细粒度分析。TwinBreak是第一种分析与提示具有高结构和内容相似性的中间输出以隔离安全参数的方法。我们展示了包含100个此类双重提示的TwinPrompt数据集。实验确认了TwinBreak的有效性，在来自五个供应商的16个LLM中，成功率达到89%至98%，且计算要求最低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing concern of security vulnerabilities in Large Language Models (LLMs) like ChatGPT, which can be exploited by malicious users through harmful prompts. Previous methods for mitigating these risks often involve significant manual effort, high computational costs, or lead to excessive modifications that degrade model performance. The proposed approach, TwinBreak, innovatively removes safety alignments by identifying and pruning the parameters responsible for these security mechanisms, treating them as embedded backdoors. This method is well-motivated as it allows for a fine-grained analysis of model parameters while preserving utility. The contribution of the paper includes the introduction of the TwinPrompt dataset, which contains 100 twin prompts, and the demonstration of TwinBreak&#x27;s effectiveness, achieving success rates between 89% and 98% across 16 LLMs from five vendors with minimal computational requirements.</div>
<div class="mono" style="margin-top:8px">本文探讨了与大型语言模型（LLMs）如ChatGPT相关的安全风险问题，这些模型可能被恶意用户通过有害提示进行利用。以往的风险缓解方法通常需要大量手动工作、高计算成本，或导致过度修改，从而降低模型的实用性。所提出的方法TwinBreak创新性地通过识别和修剪负责这些功能的参数来移除安全对齐，重点分析最相关的模型层，并从结构和内容相似的提示中分析中间输出。本文的贡献在于引入了TwinPrompt数据集，并展示了TwinBreak的有效性，在来自五个供应商的16个LLM上实现了89%到98%的成功率，且计算需求最低，从而支持了其在不妨碍性能的情况下增强模型安全性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">HSF: Defending against Jailbreak Attacks with Hidden State Filtering</div>
<div class="meta-line">Authors: Cheng Qian, Hainan Zhang, Lei Sha, Zhiming Zheng</div>
<div class="meta-line">First: 2024-08-31T06:50:07+00:00 · Latest: 2025-06-09T09:23:17+00:00</div>
<div class="meta-line">Comments: WWW2025 WSAI BESTPAPER</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2409.03788v2">Abs</a> · <a href="http://arxiv.org/pdf/2409.03788v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the growing deployment of LLMs in daily applications like chatbots and
content generation, efforts to ensure outputs align with human values and avoid
harmful content have intensified. However, increasingly sophisticated jailbreak
attacks threaten this alignment, aiming to induce unsafe outputs. Current
defense efforts either focus on prompt rewriting or detection, which are
limited in effectiveness due to the various design of jailbreak prompts, or on
output control and detection, which are computationally expensive as they
require LLM inference. Therefore, designing a pre-inference defense method that
resists diverse jailbreak prompts is crucial for preventing LLM jailbreak
attacks. We observe that jailbreak attacks, safe queries, and harmful queries
exhibit different clustering patterns within the LLM&#x27;s hidden state
representation space. This suggests that by leveraging the LLM&#x27;s hidden state
representational capabilities, we can analyze the LLM&#x27;s forthcoming behavior
and proactively intervene for defense. In this paper, we propose a jailbreak
attack defense strategy based on a Hidden State Filter (HSF), a lossless
architectural defense mechanism that enables the model to preemptively identify
and reject adversarial inputs before the inference process begins. We activate
its defensive potential through an additional plugin module, effectively
framing the defense task as a classification problem. Experimental results on
two benchmark datasets, utilizing three different LLMs, show that HSF
significantly enhances resilience against six cutting-edge jailbreak attacks.
It significantly reduces the success rate of jailbreak attacks while minimally
impacting responses to benign user queries, with negligible inference overhead,
and outperforming defense baselines.Our code and data are available at
https://anonymous.4open.science/r/Hidden-State-Filtering-8652/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HSF：通过隐藏状态过滤防御越狱攻击</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在聊天机器人和内容生成等日常应用中的广泛部署，确保输出与人类价值观一致并避免有害内容的努力日益加强。然而，日益复杂的越狱攻击威胁这种一致性，旨在诱导不安全的输出。目前的防御工作要么集中在提示重写或检测上，由于越狱提示的多样设计而效果有限，要么集中在输出控制和检测上，这在计算上代价高昂，因为它们需要LLM推理。因此，设计一种抵御多样越狱提示的推理前防御方法对于防止LLM越狱攻击至关重要。我们观察到越狱攻击、安全查询和有害查询在LLM的隐藏状态表示空间中表现出不同的聚类模式。这表明，通过利用LLM的隐藏状态表示能力，我们可以分析LLM即将出现的行为并主动进行防御干预。在本文中，我们提出了一种基于隐藏状态过滤器（HSF）的越狱攻击防御策略，这是一种无损的架构防御机制，使模型能够在推理过程开始之前主动识别和拒绝对抗输入。我们通过一个额外的插件模块激活其防御潜力，有效地将防御任务框架化为分类问题。在两个基准数据集上的实验结果，利用三种不同的LLM，表明HSF显著增强了对六种前沿越狱攻击的抵抗力。它显著降低了越狱攻击的成功率，同时对良性用户查询的响应影响最小，推理开销微乎其微，并且优于防御基线。我们的代码和数据可在https://anonymous.4open.science/r/Hidden-State-Filtering-8652/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the increasing threat of jailbreak attacks on large language models (LLMs), which can lead to unsafe outputs in applications like chatbots. Previous methods primarily focused on prompt rewriting or detection, which are often ineffective against the diverse nature of jailbreak prompts, or they relied on computationally expensive output control mechanisms. The proposed Hidden State Filtering (HSF) method differs by utilizing the LLM&#x27;s hidden state representations to proactively identify and reject adversarial inputs before inference, thus addressing the limitations of existing approaches. This paper contributes a novel architectural defense mechanism that frames the defense task as a classification problem, enhancing the model&#x27;s resilience against jailbreak attacks. Experimental results demonstrate that HSF significantly reduces the success rate of six advanced jailbreak attacks across three different LLMs while maintaining performance on benign queries and incurring minimal inference overhead, supporting its effectiveness in real-world applications.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）面临的越狱攻击威胁，这些攻击可能导致聊天机器人等应用产生有害输出。以往的方法主要集中在提示重写或检测上，但由于越狱提示的多样性，这些方法效果有限，或者依赖于计算成本高昂的输出控制机制。提出的隐状态过滤（HSF）方法通过利用LLM的隐状态表示，在推理之前识别和拒绝对抗性输入，从而提供更高效的推理前防御。本文贡献了一种新颖的防御策略，显著降低了越狱攻击的成功率，同时保持了对良性查询的性能，通过在两个基准数据集和三种不同LLM上的实验，证明其优于现有防御方法。研究结果表明，HSF有效增强了LLM对复杂攻击的抵御能力，且计算开销极小。</div>
</details>
</div>
<div class="card">
<div class="title">A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training   and Deployment</div>
<div class="meta-line">Authors: Kun Wang, Guibin Zhang, Zhenhong Zhou, Jiahao Wu, Miao Yu, Shiqian Zhao, Chenlong Yin, Jinhu Fu, Yibo Yan, Hanjun Luo, Liang Lin, Zhihao Xu, Haolang Lu, Xinye Cao, Xinyun Zhou, Weifei Jin, Fanci Meng, Shicheng Xu, Junyuan Mao, Yu Wang, Hao Wu, Minghe Wang, Fan Zhang, Junfeng Fang, Wenjie Qu, Yue Liu, Chengwei Liu, Yifan Zhang, Qiankun Li, Chongye Guo, Yalan Qin, Zhaoxin Fan, Kai Wang, Yi Ding, Donghai Hong, Jiaming Ji, Yingxin Lai, Zitong Yu, Xinfeng Li, Yifan Jiang, Yanhui Li, Xinyu Deng, Junlin Wu, Dongxia Wang, Yihao Huang, Yufei Guo, Jen-tse Huang, Qiufeng Wang, Xiaolong Jin, Wenxuan Wang, Dongrui Liu, Yanwei Yue, Wenke Huang, Guancheng Wan, Heng Chang, Tianlin Li, Yi Yu, Chenghao Li, Jiawei Li, Lei Bai, Jie Zhang, Qing Guo, Jingyi Wang, Tianlong Chen, Joey Tianyi Zhou, Xiaojun Jia, Weisong Sun, Cong Wu, Jing Chen, Xuming Hu, Yiming Li, Xiao Wang, Ningyu Zhang, Luu Anh Tuan, Guowen Xu, Jiaheng Zhang, Tianwei Zhang, Xingjun Ma, Jindong Gu, Liang Pang, Xiang Wang, Bo An, Jun Sun, Mohit Bansal, Shirui Pan, Lingjuan Lyu, Yuval Elovici, Bhavya Kailkhura, Yaodong Yang, Hongwei Li, Wenyuan Xu, Yizhou Sun, Wei Wang, Qing Li, Ke Tang, Yu-Gang Jiang, Felix Juefei-Xu, Hui Xiong, Xiaofeng Wang, Dacheng Tao, Philip S. Yu, Qingsong Wen, Yang Liu</div>
<div class="meta-line">First: 2025-04-22T05:02:49+00:00 · Latest: 2025-06-09T02:36:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.15585v4">Abs</a> · <a href="http://arxiv.org/pdf/2504.15585v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The remarkable success of Large Language Models (LLMs) has illuminated a
promising pathway toward achieving Artificial General Intelligence for both
academic and industrial communities, owing to their unprecedented performance
across various applications. As LLMs continue to gain prominence in both
research and commercial domains, their security and safety implications have
become a growing concern, not only for researchers and corporations but also
for every nation. Currently, existing surveys on LLM safety primarily focus on
specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning
phase, lacking a comprehensive understanding of the entire &quot;lifechain&quot; of LLMs.
To address this gap, this paper introduces, for the first time, the concept of
&quot;full-stack&quot; safety to systematically consider safety issues throughout the
entire process of LLM training, deployment, and eventual commercialization.
Compared to the off-the-shelf LLM safety surveys, our work demonstrates several
distinctive advantages: (I) Comprehensive Perspective. We define the complete
LLM lifecycle as encompassing data preparation, pre-training, post-training,
deployment and final commercialization. To our knowledge, this represents the
first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive
Literature Support. Our research is grounded in an exhaustive review of over
800+ papers, ensuring comprehensive coverage and systematic organization of
security issues within a more holistic understanding. (III) Unique Insights.
Through systematic literature analysis, we have developed reliable roadmaps and
perspectives for each chapter. Our work identifies promising research
directions, including safety in data generation, alignment techniques, model
editing, and LLM-based agent systems. These insights provide valuable guidance
for researchers pursuing future work in this field.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM（代理）全栈安全的综合调查：数据、训练与部署</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）的显著成功为实现人工通用智能照亮了一条有前景的道路，吸引了学术界和工业界的关注，因其在各种应用中的卓越表现。随着LLM在研究和商业领域的日益重要，其安全性和安全隐患已成为研究人员、企业乃至各国日益关注的问题。目前，现有的LLM安全调查主要集中在LLM生命周期的特定阶段，例如部署阶段或微调阶段，缺乏对LLM整个“生命周期”的全面理解。为填补这一空白，本文首次引入“全栈”安全的概念，系统地考虑LLM训练、部署及最终商业化过程中的安全问题。与现成的LLM安全调查相比，我们的工作展示了几个独特的优势：（I）全面视角。我们将完整的LLM生命周期定义为涵盖数据准备、预训练、后训练、部署和最终商业化。据我们所知，这是首个涵盖LLM整个生命周期的安全调查。（II）广泛的文献支持。我们的研究基于对800多篇论文的详尽回顾，确保了安全问题的全面覆盖和系统组织，提供了更全面的理解。（III）独特的见解。通过系统的文献分析，我们为每一章制定了可靠的路线图和视角。我们的工作识别了有前景的研究方向，包括数据生成的安全性、对齐技术、模型编辑和基于LLM的代理系统。这些见解为研究人员在该领域的未来工作提供了宝贵的指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the increasing concerns regarding the safety and security implications of Large Language Models (LLMs) as they gain prominence in both academic and industrial applications. Previous surveys have primarily focused on isolated stages of the LLM lifecycle, such as deployment or fine-tuning, which limits the understanding of the comprehensive safety issues involved. This paper proposes a novel concept of &quot;full-stack&quot; safety, which systematically considers safety throughout the entire LLM lifecycle, including data preparation, pre-training, post-training, deployment, and commercialization. The contribution lies in providing a holistic perspective supported by an extensive review of over 800 papers, identifying key safety issues and promising research directions. The methodology involves systematic literature analysis to develop roadmaps and insights that guide future research in LLM safety, ultimately aiming to enhance the understanding and management of safety risks associated with LLMs.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在学术和工业应用中日益增长的安全性和安全隐患。以往的调查主要集中在LLM生命周期的特定阶段，如部署或微调，导致对LLM安全性的理解片面。本文提出了一种全面的“全栈”安全方法，考虑LLM整个生命周期中的安全问题，包括数据准备、预训练、后训练、部署和商业化。该方法通过对800多篇论文的广泛审查，系统地组织安全问题，并识别出数据生成和对齐技术等有前景的研究方向。研究结果有助于对LLM安全的整体理解，并为该领域未来的研究提供了宝贵的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge-to-Jailbreak: Investigating Knowledge-driven Jailbreaking   Attacks for Large Language Models</div>
<div class="meta-line">Authors: Shangqing Tu, Zhuoran Pan, Wenxuan Wang, Zhexin Zhang, Yuliang Sun, Jifan Yu, Hongning Wang, Lei Hou, Juanzi Li</div>
<div class="meta-line">Venue: KDD 2025</div>
<div class="meta-line">First: 2024-06-17T15:59:59+00:00 · Latest: 2025-06-09T02:18:21+00:00</div>
<div class="meta-line">Comments: Accepted by KDD 2025 research track</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2406.11682v2">Abs</a> · <a href="http://arxiv.org/pdf/2406.11682v2">PDF</a> · <a href="https://github.com/THU-KEG/Knowledge-to-Jailbreak/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have been increasingly applied to various
domains, which triggers increasing concerns about LLMs&#x27; safety on specialized
domains, e.g. medicine. Despite prior explorations on general jailbreaking
attacks, there are two challenges for applying existing attacks on testing the
domain-specific safety of LLMs: (1) Lack of professional knowledge-driven
attacks, (2) Insufficient coverage of domain knowledge. To bridge this gap, we
propose a new task, knowledge-to-jailbreak, which aims to generate jailbreaking
attacks from domain knowledge, requiring both attack effectiveness and
knowledge relevance. We collect a large-scale dataset with 12,974
knowledge-jailbreak pairs and fine-tune a large language model as
jailbreak-generator, to produce domain knowledge-specific jailbreaks.
Experiments on 13 domains and 8 target LLMs demonstrate the effectiveness of
jailbreak-generator in generating jailbreaks that are both threatening to the
target LLMs and relevant to the given knowledge. We also apply our method to an
out-of-domain knowledge base, showing that jailbreak-generator can generate
jailbreaks that are comparable in harmfulness to those crafted by human
experts. Data and code are available at:
https://github.com/THU-KEG/Knowledge-to-Jailbreak/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知识到越狱：针对大型语言模型的知识驱动越狱攻击研究</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在各个领域的应用日益增多，这引发了对LLMs在专业领域（如医学）安全性的关注。尽管之前对一般越狱攻击进行了探索，但在测试LLMs的领域特定安全性时，现有攻击面临两个挑战：（1）缺乏专业知识驱动的攻击，（2）领域知识覆盖不足。为填补这一空白，我们提出了一项新任务，知识到越狱，旨在从领域知识生成越狱攻击，要求攻击的有效性和知识的相关性。我们收集了一个包含12,974个知识-越狱对的大规模数据集，并微调了一个大型语言模型作为越狱生成器，以生成特定于领域知识的越狱攻击。在13个领域和8个目标LLMs上的实验表明，越狱生成器在生成对目标LLMs构成威胁且与给定知识相关的越狱攻击方面的有效性。我们还将我们的方法应用于一个跨领域知识库，显示越狱生成器能够生成与人类专家制作的越狱攻击在危害性上相当的越狱攻击。数据和代码可在：https://github.com/THU-KEG/Knowledge-to-Jailbreak/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the growing safety concerns regarding large language models (LLMs) in specialized domains, particularly due to the inadequacy of existing jailbreaking attacks that lack professional knowledge and sufficient domain coverage. Previous methods have not effectively targeted domain-specific vulnerabilities, prompting the authors to propose a novel task called knowledge-to-jailbreak, which generates jailbreaking attacks based on domain knowledge, ensuring both attack effectiveness and relevance. The contribution of this paper lies in the development of a large-scale dataset comprising 12,974 knowledge-jailbreak pairs and the fine-tuning of a large language model to serve as a jailbreak-generator. The methodology involves testing this generator across 13 domains and 8 target LLMs, demonstrating its capability to produce harmful jailbreaks that are comparable in effectiveness to those created by human experts, thereby supporting the goal of enhancing LLM safety in specialized applications.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在专业领域（尤其是医学）中的安全性问题，指出现有的越狱攻击方法缺乏专业知识和足够的领域覆盖。以往的方法未能有效针对领域特定的脆弱性，因此提出了一项新任务——知识到越狱，旨在基于领域知识生成越狱攻击，同时确保攻击的有效性和相关性。本文的贡献在于收集了12,974对知识-越狱的数据集，并对大型语言模型进行了微调，使其能够作为越狱生成器。该方法在13个领域和8个目标LLM上进行了测试，结果表明生成的越狱攻击在有效性和相关性上均表现良好，其危害性水平可与人类专家制作的越狱攻击相媲美，从而支持了增强LLM在专业应用中安全性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Quality-Diversity Red-Teaming: Automated Generation of High-Quality and   Diverse Attackers for Large Language Models</div>
<div class="meta-line">Authors: Ren-Jian Wang, Ke Xue, Zeyu Qin, Ziniu Li, Sheng Tang, Hao-Tian Li, Shengcai Liu, Chao Qian</div>
<div class="meta-line">First: 2025-06-08T13:07:41+00:00 · Latest: 2025-06-08T13:07:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.07121v1">Abs</a> · <a href="http://arxiv.org/pdf/2506.07121v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring safety of large language models (LLMs) is important. Red teaming--a
systematic approach to identifying adversarial prompts that elicit harmful
responses from target LLMs--has emerged as a crucial safety evaluation method.
Within this framework, the diversity of adversarial prompts is essential for
comprehensive safety assessments. We find that previous approaches to
red-teaming may suffer from two key limitations. First, they often pursue
diversity through simplistic metrics like word frequency or sentence embedding
similarity, which may not capture meaningful variation in attack strategies.
Second, the common practice of training a single attacker model restricts
coverage across potential attack styles and risk categories. This paper
introduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to
address these limitations. QDRT achieves goal-driven diversity through
behavior-conditioned training and implements a behavioral replay buffer in an
open-ended manner. Additionally, it trains multiple specialized attackers
capable of generating high-quality attacks across diverse styles and risk
categories. Our empirical evaluation demonstrates that QDRT generates attacks
that are both more diverse and more effective against a wide range of target
LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the
field of LLM safety by providing a systematic and effective approach to
automated red-teaming, ultimately supporting the responsible deployment of
LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>质量-多样性红队：大型语言模型高质量和多样化攻击者的自动生成</div>
<div class="mono" style="margin-top:8px">确保大型语言模型（LLMs）的安全性至关重要。红队测试是一种系统性的方法，用于识别引发目标LLMs有害响应的对抗性提示，已成为一种重要的安全评估方法。在这一框架内，对抗性提示的多样性对于全面的安全评估至关重要。我们发现，之前的红队方法可能存在两个主要局限性。首先，它们通常通过简单的指标（如词频或句子嵌入相似性）来追求多样性，这可能无法捕捉到攻击策略的有意义变化。其次，训练单一攻击者模型的常见做法限制了对潜在攻击风格和风险类别的覆盖。本文介绍了质量-多样性红队（QDRT），这是一个旨在解决这些局限性的新框架。QDRT通过行为条件训练实现目标驱动的多样性，并以开放式方式实现行为重放缓冲。此外，它训练多个专门的攻击者，能够在多样化的风格和风险类别中生成高质量的攻击。我们的实证评估表明，QDRT生成的攻击在多样性和针对广泛目标LLMs（包括GPT-2、Llama-3、Gemma-2和Qwen2.5）方面都更有效。这项工作通过提供一种系统有效的自动化红队方法，推动了LLM安全领域的发展，最终支持LLMs的负责任部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical need for safety in large language models (LLMs) through an improved red teaming approach, which identifies adversarial prompts that can provoke harmful responses. Previous methods have relied on simplistic metrics for diversity and typically employed a single attacker model, leading to insufficient coverage of attack strategies. The proposed Quality-Diversity Red-Teaming (QDRT) framework overcomes these limitations by utilizing behavior-conditioned training and a behavioral replay buffer, while also training multiple specialized attackers to generate high-quality and diverse attacks. The contribution of this paper lies in its systematic approach to automated red teaming, which has been empirically validated to produce more diverse and effective attacks against various LLMs, thereby enhancing the safety evaluation process for these models.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）安全性的重要需求，通过增强红队测试来解决问题，红队测试是一种识别引发有害响应的对抗性提示的方法。以往的方法依赖于简单的多样性指标，并通常采用单一攻击者模型，限制了攻击策略的多样性。提出的质量-多样性红队测试（QDRT）框架通过利用行为条件训练和行为重放缓冲区，以及多个专门的攻击者模型，克服了这些问题，能够生成高质量、多样化的攻击。本文通过系统性地改进自动化红队测试，为LLM安全性做出了贡献，证明QDRT能够针对多种LLM生成更具多样性和有效性的攻击，从而支持其负责任的部署。</div>
</details>
</div>
<div class="card">
<div class="title">AlphaSteer: Learning Refusal Steering with Principled Null-Space   Constraint</div>
<div class="meta-line">Authors: Leheng Sheng, Changshuo Shen, Weixiang Zhao, Junfeng Fang, Xiaohao Liu, Zhenkai Liang, Xiang Wang, An Zhang, Tat-Seng Chua</div>
<div class="meta-line">First: 2025-06-08T07:03:28+00:00 · Latest: 2025-06-08T07:03:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.07022v1">Abs</a> · <a href="http://arxiv.org/pdf/2506.07022v1">PDF</a> · <a href="https://github.com/AlphaLab-USTC/AlphaSteer">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As LLMs are increasingly deployed in real-world applications, ensuring their
ability to refuse malicious prompts, especially jailbreak attacks, is essential
for safe and reliable use. Recently, activation steering has emerged as an
effective approach for enhancing LLM safety by adding a refusal direction
vector to internal activations of LLMs during inference, which will further
induce the refusal behaviors of LLMs. However, indiscriminately applying
activation steering fundamentally suffers from the trade-off between safety and
utility, since the same steering vector can also lead to over-refusal and
degraded performance on benign prompts. Although prior efforts, such as vector
calibration and conditional steering, have attempted to mitigate this
trade-off, their lack of theoretical grounding limits their robustness and
effectiveness. To better address the trade-off between safety and utility, we
present a theoretically grounded and empirically effective activation steering
method called AlphaSteer. Specifically, it considers activation steering as a
learnable process with two principled learning objectives: utility preservation
and safety enhancement. For utility preservation, it learns to construct a
nearly zero vector for steering benign data, with the null-space constraints.
For safety enhancement, it learns to construct a refusal direction vector for
steering malicious data, with the help of linear regression. Experiments across
multiple jailbreak attacks and utility benchmarks demonstrate the effectiveness
of AlphaSteer, which significantly improves the safety of LLMs without
compromising general capabilities. Our codes are available at
https://github.com/AlphaLab-USTC/AlphaSteer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AlphaSteer：带有原则性零空间约束的拒绝引导学习</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在现实应用中的日益普及，确保它们能够拒绝恶意提示，尤其是越狱攻击，对于安全可靠的使用至关重要。最近，激活引导作为一种有效的方法出现，通过在推理过程中向LLMs的内部激活添加拒绝方向向量，从而增强LLM的安全性，进一步诱导LLMs的拒绝行为。然而，盲目应用激活引导在根本上面临安全性与实用性之间的权衡，因为相同的引导向量也可能导致过度拒绝和对良性提示的性能下降。尽管之前的努力，如向量校准和条件引导，试图缓解这一权衡，但其缺乏理论基础限制了其稳健性和有效性。为了更好地解决安全性与实用性之间的权衡，我们提出了一种理论基础扎实且经验有效的激活引导方法，称为AlphaSteer。具体而言，它将激活引导视为一个可学习的过程，具有两个原则性学习目标：实用性保持和安全性增强。对于实用性保持，它学习构建一个几乎为零的向量来引导良性数据，带有零空间约束。对于安全性增强，它学习构建一个拒绝方向向量来引导恶意数据，借助线性回归。针对多个越狱攻击和实用性基准的实验表明，AlphaSteer的有效性显著提高了LLMs的安全性，而不影响其通用能力。我们的代码可在https://github.com/AlphaLab-USTC/AlphaSteer获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical need for large language models (LLMs) to effectively refuse malicious prompts, particularly in the context of jailbreak attacks, to ensure their safe deployment in real-world applications. Previous methods like activation steering have shown promise but suffer from a trade-off between safety and utility, leading to over-refusal and degraded performance on benign prompts. The proposed AlphaSteer method improves upon these approaches by introducing a theoretically grounded activation steering process that balances utility preservation and safety enhancement through null-space constraints and linear regression. This paper contributes a novel framework that allows LLMs to maintain their general capabilities while significantly improving their safety against malicious inputs. Experimental results demonstrate that AlphaSteer effectively enhances LLM safety across various jailbreak attacks without compromising performance on benign tasks.</div>
<div class="mono" style="margin-top:8px">本研究解决了大型语言模型（LLM）有效拒绝恶意提示（特别是越狱攻击）以确保其在现实应用中安全部署的关键需求。以往的方法如激活引导在增强LLM安全性方面表现出色，但往往导致安全性和效用之间的有害权衡，造成对良性提示的过度拒绝和性能下降。提出的方法AlphaSteer引入了一种理论基础的激活引导方法，结合零空间约束，学习两个目标：为良性输入保留效用和为恶意输入增强安全性。通过对多种越狱攻击和效用基准的实验验证，该方法表明AlphaSteer显著提高了LLM的安全性，同时保持整体性能，从而支持其预期目标。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for   Autonomous Driving with Large Language Models</div>
<div class="meta-line">Authors: Yuewen Mei, Tong Nie, Jian Sun, Ye Tian</div>
<div class="meta-line">Venue: IEEE Transactions on Intelligent Transportation Systems 2025</div>
<div class="meta-line">First: 2025-01-27T08:18:52+00:00 · Latest: 2025-06-07T15:26:30+00:00</div>
<div class="meta-line">Comments: Accepted as a regular paper at IEEE TITS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2501.15850v2">Abs</a> · <a href="http://arxiv.org/pdf/2501.15850v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring and improving the safety of autonomous driving systems (ADS) is
crucial for the deployment of highly automated vehicles, especially in
safety-critical events. To address the rarity issue, adversarial scenario
generation methods are developed, in which behaviors of traffic participants
are manipulated to induce safety-critical events. However, existing methods
still face two limitations. First, identification of the adversarial
participant directly impacts the effectiveness of the generation. However, the
complexity of real-world scenarios, with numerous participants and diverse
behaviors, makes identification challenging. Second, the potential of generated
safety-critical scenarios to continuously improve ADS performance remains
underexplored. To address these issues, we propose LLM-attacker: a closed-loop
adversarial scenario generation framework leveraging large language models
(LLMs). Specifically, multiple LLM agents are designed and coordinated to
identify optimal attackers. Then, the trajectories of the attackers are
optimized to generate adversarial scenarios. These scenarios are iteratively
refined based on the performance of ADS, forming a feedback loop to improve
ADS. Experimental results show that LLM-attacker can create more dangerous
scenarios than other methods, and the ADS trained with it achieves a collision
rate half that of training with normal scenarios. This indicates the ability of
LLM-attacker to test and enhance the safety and robustness of ADS. Video
demonstrations are provided at:
https://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM-attacker：利用大型语言模型增强自动驾驶的闭环对抗场景生成</div>
<div class="mono" style="margin-top:8px">确保和提高自动驾驶系统（ADS）的安全性对于高度自动化车辆的部署至关重要，尤其是在安全关键事件中。为了解决稀有性问题，开发了对抗场景生成方法，通过操控交通参与者的行为来诱发安全关键事件。然而，现有方法仍面临两个限制。首先，对抗参与者的识别直接影响生成的有效性。然而，现实场景的复杂性，参与者众多且行为多样，使得识别变得具有挑战性。其次，生成的安全关键场景在持续改善ADS性能方面的潜力仍未得到充分探索。为了解决这些问题，我们提出了LLM-attacker：一个利用大型语言模型（LLMs）的闭环对抗场景生成框架。具体而言，设计并协调多个LLM代理以识别最佳攻击者。然后，优化攻击者的轨迹以生成对抗场景。这些场景根据ADS的性能进行迭代优化，形成反馈循环以改善ADS。实验结果表明，LLM-attacker能够创建比其他方法更危险的场景，使用它训练的ADS的碰撞率仅为使用正常场景训练的一半。这表明LLM-attacker在测试和增强ADS的安全性和鲁棒性方面的能力。视频演示可在以下链接查看： https://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical need for enhancing the safety of autonomous driving systems (ADS), particularly in rare safety-critical events. Previous adversarial scenario generation methods struggled with accurately identifying adversarial participants due to the complexity of real-world traffic scenarios and did not fully explore the potential of generated scenarios to improve ADS performance. The proposed LLM-attacker framework utilizes large language models (LLMs) to create a closed-loop system that identifies optimal attackers and refines their trajectories based on ADS performance, effectively addressing the limitations of existing methods. This paper contributes a novel approach that significantly improves the generation of dangerous scenarios, demonstrating that ADS trained with LLM-attacker exhibits a collision rate that is half that of those trained with conventional scenarios, thereby supporting the goal of enhancing safety and robustness in autonomous driving.</div>
<div class="mono" style="margin-top:8px">本研究解决了增强自动驾驶系统（ADS）安全性的关键需求，特别是在生成模拟安全关键事件的对抗场景方面。以往的对抗场景生成方法在有效识别对抗参与者方面面临挑战，原因在于现实交通场景的复杂性，并且未充分探索这些场景提升ADS性能的潜力。提出的LLM-attacker框架利用大型语言模型（LLMs）创建一个闭环系统，多个LLM代理识别最佳攻击者，并根据ADS性能优化其轨迹，从而解决了现有方法的局限性。本文的贡献在于其创新的对抗场景生成方法，已证明能够产生更危险的场景，使得ADS的碰撞率仅为传统训练场景的一半，从而有效提升自动驾驶系统的安全性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for   Safety Alignment</div>
<div class="meta-line">Authors: Kyubyung Chae, Hyunbin Jin, Taesup Kim</div>
<div class="meta-line">First: 2025-06-07T08:19:01+00:00 · Latest: 2025-06-07T08:19:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.10020v1">Abs</a> · <a href="http://arxiv.org/pdf/2506.10020v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safely aligning large language models (LLMs) often demands extensive
human-labeled preference data, a process that&#x27;s both costly and time-consuming.
While synthetic data offers a promising alternative, current methods frequently
rely on complex iterative prompting or auxiliary models. To address this, we
introduce Refusal-Aware Adaptive Injection (RAAI), a straightforward,
training-free, and model-agnostic framework that repurposes LLM attack
techniques. RAAI works by detecting internal refusal signals and adaptively
injecting predefined phrases to elicit harmful, yet fluent, completions. Our
experiments show RAAI effectively jailbreaks LLMs, increasing the harmful
response rate from a baseline of 2.15% to up to 61.04% on average across four
benchmarks. Crucially, fine-tuning LLMs with the synthetic data generated by
RAAI improves model robustness against harmful prompts while preserving general
capabilities on standard tasks like MMLU and ARC. This work highlights how LLM
attack methodologies can be reframed as practical tools for scalable and
controllable safety alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从威胁到工具：利用拒绝感知注入攻击进行安全对齐</div>
<div class="mono" style="margin-top:8px">安全对齐大型语言模型（LLMs）通常需要大量人工标注的偏好数据，这一过程既昂贵又耗时。虽然合成数据提供了一个有前景的替代方案，但当前的方法往往依赖于复杂的迭代提示或辅助模型。为了解决这个问题，我们提出了拒绝感知自适应注入（RAAI），这是一个简单、无训练且与模型无关的框架，重新利用LLM攻击技术。RAAI通过检测内部拒绝信号并自适应地注入预定义短语来引发有害但流畅的完成。我们的实验表明，RAAI有效地破解了LLMs，将有害响应率从基线的2.15%提高到四个基准测试中平均达到61.04%。关键是，使用RAAI生成的合成数据对LLMs进行微调，提高了模型对有害提示的鲁棒性，同时保持了在MMLU和ARC等标准任务上的一般能力。这项工作突显了LLM攻击方法如何被重新构建为可扩展和可控的安全对齐实用工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of safely aligning large language models (LLMs), which typically requires extensive human-labeled preference data that is both costly and time-consuming. Previous methods often involve complex iterative prompting or auxiliary models, leading to inefficiencies and limitations in scalability. The proposed approach, Refusal-Aware Adaptive Injection (RAAI), is a straightforward, training-free, and model-agnostic framework that repurposes LLM attack techniques to detect internal refusal signals and inject predefined phrases, significantly increasing the harmful response rate from 2.15% to 61.04% across four benchmarks. The methodology demonstrates that fine-tuning LLMs with synthetic data generated by RAAI enhances model robustness against harmful prompts while maintaining performance on standard tasks, thus contributing to a more scalable and controllable safety alignment strategy for LLMs.</div>
<div class="mono" style="margin-top:8px">本研究解决了安全对齐大型语言模型（LLMs）的挑战，通常需要大量人工标注的偏好数据，这一过程既昂贵又耗时。以往的方法往往依赖复杂的迭代提示或辅助模型，这在效率和可扩展性上存在困难。所提出的方法，即拒绝感知自适应注入（RAAI），通过利用LLM攻击技术，检测内部拒绝信号并注入预定义短语以生成有害但流畅的完成，从而与众不同，具有简单、无训练和模型无关的特点。本文的贡献在于展示了LLM攻击方法如何被重新利用为有效的安全对齐工具。实验表明，RAAI将有害响应率从基线的2.15%显著提高到四个基准测试的平均61.04%，并且通过用RAAI生成的合成数据对LLM进行微调，提高了模型对有害提示的鲁棒性，同时保持了在MMLU和ARC等标准任务上的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial Tokenization</div>
<div class="meta-line">Authors: Renato Lui Geh, Zilei Shao, Guy Van den Broeck</div>
<div class="meta-line">Venue: ACL 2025</div>
<div class="meta-line">First: 2025-03-04T01:31:17+00:00 · Latest: 2025-06-05T21:52:43+00:00</div>
<div class="meta-line">Comments: Proceedings of the 63rd Annual Meeting of the Association for
  Computational Linguistics, ACL 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.02174v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.02174v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current LLM pipelines account for only one possible tokenization for a given
string, ignoring exponentially many alternative tokenizations during training
and inference. For example, the standard Llama3 tokenization of penguin is
[p,enguin], yet [peng,uin] is another perfectly valid alternative. In this
paper, we show that despite LLMs being trained solely on one tokenization, they
still retain semantic understanding of other tokenizations, raising questions
about their implications in LLM safety. Put succinctly, we answer the following
question: can we adversarially tokenize an obviously malicious string to evade
safety and alignment restrictions? We show that not only is adversarial
tokenization an effective yet previously neglected axis of attack, but it is
also competitive against existing state-of-the-art adversarial approaches
without changing the text of the harmful request. We empirically validate this
exploit across three state-of-the-art LLMs and adversarial datasets, revealing
a previously unknown vulnerability in subword models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对抗性标记化</div>
<div class="mono" style="margin-top:8px">当前的LLM管道仅考虑给定字符串的一种可能标记化，忽略了在训练和推理过程中指数级的许多替代标记化。例如，标准的Llama3对企鹅的标记化是[p,enguin]，而[peng,uin]是另一个完全有效的替代方案。本文表明，尽管LLM仅在一种标记化上进行训练，但它们仍然保留对其他标记化的语义理解，这引发了关于其在LLM安全性方面的影响的问题。简而言之，我们回答了以下问题：我们能否对一个明显恶意的字符串进行对抗性标记化，以规避安全和对齐限制？我们表明，对抗性标记化不仅是一种有效但之前被忽视的攻击轴，而且在不改变有害请求文本的情况下，它与现有的最先进的对抗性方法具有竞争力。我们在三种最先进的LLM和对抗性数据集上实证验证了这一漏洞，揭示了子词模型中一个之前未知的脆弱性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of current large language model (LLM) pipelines, which typically consider only a single tokenization for a given string, neglecting the multitude of alternative tokenizations that could exist. Previous methods have not explored the implications of these alternative tokenizations on LLM safety, leading to a gap in understanding potential vulnerabilities. This paper proposes a novel approach called adversarial tokenization, which effectively exploits this oversight by demonstrating that LLMs can be manipulated through alternative tokenizations of malicious strings, thereby evading safety and alignment measures. The methodology involves empirical validation of this exploit across three state-of-the-art LLMs and various adversarial datasets, revealing a significant vulnerability in subword models. The findings indicate that adversarial tokenization is a competitive attack strategy against existing methods, highlighting the need for enhanced safety measures in LLMs.</div>
<div class="mono" style="margin-top:8px">本研究关注当前大型语言模型（LLM）管道的局限性，这些管道通常只考虑输入字符串的单一标记化，忽视了可能存在的大量替代标记化。以往的方法主要集中在直接攻击或操纵输入文本，往往未能探讨标记化变体的影响。提出的对抗性标记化方法通过利用这些替代标记化来规避安全措施，而不改变有害内容本身，从而呈现出一种合理的策略，以利用之前被忽视的脆弱性。本文的贡献在于通过实证展示对抗性标记化在三种最先进的LLM和各种对抗性数据集上的有效性，揭示了子词模型中的重大弱点，并在性能上与现有对抗技术竞争。</div>
</details>
</div>
<div class="card">
<div class="title">Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity   Analysis Between Alignment and Fine-tuning Datasets</div>
<div class="meta-line">Authors: Lei Hsiung, Tianyu Pang, Yung-Chen Tang, Linyue Song, Tsung-Yi Ho, Pin-Yu Chen, Yaoqing Yang</div>
<div class="meta-line">First: 2025-06-05T17:59:55+00:00 · Latest: 2025-06-05T17:59:55+00:00</div>
<div class="meta-line">Comments: Project Page: https://hsiung.cc/llm-similarity-risk/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.05346v1">Abs</a> · <a href="http://arxiv.org/pdf/2506.05346v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in large language models (LLMs) have underscored their
vulnerability to safety alignment jailbreaks, particularly when subjected to
downstream fine-tuning. However, existing mitigation strategies primarily focus
on reactively addressing jailbreak incidents after safety guardrails have been
compromised, removing harmful gradients during fine-tuning, or continuously
reinforcing safety alignment throughout fine-tuning. As such, they tend to
overlook a critical upstream factor: the role of the original safety-alignment
data. This paper therefore investigates the degradation of safety guardrails
through the lens of representation similarity between upstream alignment
datasets and downstream fine-tuning tasks. Our experiments demonstrate that
high similarity between these datasets significantly weakens safety guardrails,
making models more susceptible to jailbreaks. Conversely, low similarity
between these two types of datasets yields substantially more robust models and
thus reduces harmfulness score by up to 10.33%. By highlighting the importance
of upstream dataset design in the building of durable safety guardrails and
reducing real-world vulnerability to jailbreak attacks, these findings offer
actionable insights for fine-tuning service providers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为什么LLM安全护栏在微调后崩溃：对齐与微调数据集之间的相似性分析</div>
<div class="mono" style="margin-top:8px">最近大型语言模型（LLMs）的进展突显了它们在安全对齐越狱方面的脆弱性，尤其是在进行下游微调时。然而，现有的缓解策略主要集中在事后反应性地处理越狱事件、在微调过程中去除有害梯度或在微调过程中持续强化安全对齐。因此，它们往往忽视了一个关键的上游因素：原始安全对齐数据的作用。本文因此通过上游对齐数据集与下游微调任务之间的表示相似性来研究安全护栏的退化。我们的实验表明，这些数据集之间的高相似性显著削弱了安全护栏，使模型更容易受到越狱攻击。相反，这两种类型数据集之间的低相似性则产生了更强健的模型，从而将有害性评分降低了多达10.33%。通过强调上游数据集设计在构建耐用安全护栏和减少现实世界对越狱攻击脆弱性中的重要性，这些发现为微调服务提供商提供了可操作的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the vulnerability of large language models (LLMs) to safety alignment jailbreaks, particularly during downstream fine-tuning, highlighting that existing mitigation strategies often reactively address issues rather than preventing them. Previous methods have focused on removing harmful gradients or reinforcing safety alignment during fine-tuning but have largely ignored the impact of the original safety-alignment data. The proposed approach emphasizes the significance of representation similarity between upstream alignment datasets and downstream fine-tuning tasks, revealing that high similarity can weaken safety guardrails, while low similarity enhances model robustness. The research methodology involves analyzing this similarity and conducting experiments that demonstrate a reduction in harmfulness scores by up to 10.33% when low similarity is maintained. These findings contribute actionable insights for improving the design of upstream datasets to bolster safety guardrails and mitigate real-world vulnerabilities to jailbreak attacks.</div>
<div class="mono" style="margin-top:8px">本文探讨了大型语言模型（LLMs）在安全对齐方面的脆弱性，尤其是在下游微调过程中加剧的安全对齐越狱问题。以往的方法主要集中在安全防护失效后的反应措施，如去除有害梯度或在微调过程中加强安全性，但往往忽视了原始安全对齐数据的上游因素。提出的方法强调了上游对齐数据集与下游微调任务之间的表示相似性的重要性，研究表明高相似性会削弱安全防护，而低相似性则增强模型的鲁棒性。研究方法包括分析这些相似性并进行实验，结果显示在保持低相似性的情况下，有害性评分可降低多达10.33%，因此为改善LLMs的安全防护和降低其对越狱攻击的脆弱性提供了有价值的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Interpretation Meets Safety: A Survey on Interpretation Methods and   Tools for Improving LLM Safety</div>
<div class="meta-line">Authors: Seongmin Lee, Aeree Cho, Grace C. Kim, ShengYun Peng, Mansi Phute, Duen Horng Chau</div>
<div class="meta-line">First: 2025-06-05T17:56:05+00:00 · Latest: 2025-06-05T17:56:05+00:00</div>
<div class="meta-line">Comments: 31 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.05451v1">Abs</a> · <a href="http://arxiv.org/pdf/2506.05451v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) see wider real-world use, understanding and
mitigating their unsafe behaviors is critical. Interpretation techniques can
reveal causes of unsafe outputs and guide safety, but such connections with
safety are often overlooked in prior surveys. We present the first survey that
bridges this gap, introducing a unified framework that connects safety-focused
interpretation methods, the safety enhancements they inform, and the tools that
operationalize them. Our novel taxonomy, organized by LLM workflow stages,
summarizes nearly 70 works at their intersections. We conclude with open
challenges and future directions. This timely survey helps researchers and
practitioners navigate key advancements for safer, more interpretable LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解释与安全：关于提高大型语言模型安全性的解释方法和工具的调查</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在现实世界中的广泛应用，理解和减轻其不安全行为至关重要。解释技术可以揭示不安全输出的原因并指导安全，但在以往的调查中，这种与安全的联系常常被忽视。我们提出了第一个填补这一空白的调查，介绍了一个统一框架，连接了以安全为中心的解释方法、它们所提供的安全增强以及将其操作化的工具。我们新颖的分类法按LLM工作流程阶段组织，总结了近70项交叉研究。我们以开放挑战和未来方向作为结尾。这项及时的调查帮助研究人员和从业者导航更安全、更可解释的LLM的关键进展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical need for understanding and mitigating unsafe behaviors in large language models (LLMs) as their real-world applications expand. Previous surveys have often neglected the connection between interpretation techniques and safety enhancements, leading to a gap in knowledge. This paper proposes a unified framework that links safety-focused interpretation methods with their corresponding safety improvements and the tools that implement them, thus providing a well-motivated approach to enhance LLM safety. The methodology includes a novel taxonomy organized by LLM workflow stages, summarizing nearly 70 relevant works. The findings of this survey contribute to the field by offering insights into key advancements for creating safer and more interpretable LLMs, addressing both current challenges and future research directions.</div>
<div class="mono" style="margin-top:8px">本研究解决了随着大型语言模型（LLMs）在现实世界应用的扩大，理解和减轻其不安全行为的日益需求。以往的调查往往忽视了解释技术与安全之间的联系，导致对这些方法如何促进安全增强的理解存在空白。本文提出了一个统一框架，将以安全为重点的解释方法与它们所促进的安全改进联系起来，并按LLM工作流程的阶段进行组织。该方法论包括一个新颖的分类法，总结了近70项在解释与安全交叉领域的研究，为研究人员和从业者提供了全面的概述。研究结果突出了该领域的关键进展和开放挑战，支持了开发更安全、更可解释的LLMs的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial Attacks on Robotic Vision Language Action Models</div>
<div class="meta-line">Authors: Eliot Krzysztof Jones, Alexander Robey, Andy Zou, Zachary Ravichandran, George J. Pappas, Hamed Hassani, Matt Fredrikson, J. Zico Kolter</div>
<div class="meta-line">First: 2025-06-03T19:43:58+00:00 · Latest: 2025-06-03T19:43:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.03350v1">Abs</a> · <a href="http://arxiv.org/pdf/2506.03350v1">PDF</a> · <a href="https://github.com/eliotjones1/robogcg">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emergence of vision-language-action models (VLAs) for end-to-end control
is reshaping the field of robotics by enabling the fusion of multimodal sensory
inputs at the billion-parameter scale. The capabilities of VLAs stem primarily
from their architectures, which are often based on frontier large language
models (LLMs). However, LLMs are known to be susceptible to adversarial misuse,
and given the significant physical risks inherent to robotics, questions remain
regarding the extent to which VLAs inherit these vulnerabilities. Motivated by
these concerns, in this work we initiate the study of adversarial attacks on
VLA-controlled robots. Our main algorithmic contribution is the adaptation and
application of LLM jailbreaking attacks to obtain complete control authority
over VLAs. We find that textual attacks, which are applied once at the
beginning of a rollout, facilitate full reachability of the action space of
commonly used VLAs and often persist over longer horizons. This differs
significantly from LLM jailbreaking literature, as attacks in the real world do
not have to be semantically linked to notions of harm. We make all code
available at https://github.com/eliotjones1/robogcg .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对机器人视觉语言行动模型的对抗攻击</div>
<div class="mono" style="margin-top:8px">视觉语言行动模型（VLA）的出现正在通过在十亿参数规模上融合多模态传感器输入，重塑机器人领域。VLA的能力主要源于其架构，这些架构通常基于前沿的大型语言模型（LLM）。然而，LLM已知易受对抗性滥用的影响，考虑到机器人固有的重大物理风险，关于VLA在多大程度上继承这些脆弱性的问题仍然存在。基于这些担忧，在本研究中，我们开始研究针对VLA控制机器人的对抗攻击。我们的主要算法贡献是适应和应用LLM越狱攻击，以获得对VLA的完全控制权。我们发现，文本攻击在回滚开始时应用一次，能够促进常用VLA的行动空间的完全可达性，并且通常在更长的时间范围内持续存在。这与LLM越狱文献有显著不同，因为现实世界中的攻击不必与伤害的概念在语义上相关。我们将所有代码发布在https://github.com/eliotjones1/robogcg。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This article addresses the vulnerabilities of vision-language-action models (VLAs) in robotics, particularly their susceptibility to adversarial attacks inherited from large language models (LLMs). Previous methods have primarily focused on LLMs without considering their application in robotic systems, leading to a gap in understanding the risks posed by adversarial misuse in real-world scenarios. The proposed approach adapts LLM jailbreaking techniques to target VLAs, allowing for complete control over their action space through textual attacks initiated at the start of a rollout. This method is well-motivated by the need to assess the safety of robotic systems, and the findings indicate that these attacks can persist over longer operational periods. The research demonstrates that VLAs can be fully manipulated, highlighting significant security concerns in their deployment in robotics.</div>
<div class="mono" style="margin-top:8px">本研究关注机器人领域中视觉-语言-动作模型（VLA）的脆弱性，这些模型整合了多模态传感器输入，但可能继承大型语言模型（LLM）的对抗性弱点。以往的方法并未深入研究VLA在现实应用中对对抗攻击的易感性，尤其是在风险显著的情况下。本文提出了一种新方法，通过调整LLM越狱技术来控制VLA，有效展示了文本攻击可以实现完整的动作空间可达性并在时间上持续存在。该方法论涉及在回滚开始时应用这些攻击，揭示它们不需要与有害概念在语义上相关联。研究结果表明，VLA对这些攻击存在脆弱性，强调了机器人系统中改进安全措施的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Why Safeguarded Ships Run Aground? Aligned Large Language Models&#x27; Safety   Mechanisms Tend to Be Anchored in The Template Region</div>
<div class="meta-line">Authors: Chak Tou Leong, Qingyu Yin, Jian Wang, Wenjie Li</div>
<div class="meta-line">Venue: ACL 2025</div>
<div class="meta-line">First: 2025-02-19T18:42:45+00:00 · Latest: 2025-06-03T18:20:11+00:00</div>
<div class="meta-line">Comments: ACL 2025 Main</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.13946v2">Abs</a> · <a href="http://arxiv.org/pdf/2502.13946v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The safety alignment of large language models (LLMs) remains vulnerable, as
their initial behavior can be easily jailbroken by even relatively simple
attacks. Since infilling a fixed template between the input instruction and
initial model output is a common practice for existing LLMs, we hypothesize
that this template is a key factor behind their vulnerabilities: LLMs&#x27;
safety-related decision-making overly relies on the aggregated information from
the template region, which largely influences these models&#x27; safety behavior. We
refer to this issue as template-anchored safety alignment. In this paper, we
conduct extensive experiments and verify that template-anchored safety
alignment is widespread across various aligned LLMs. Our mechanistic analyses
demonstrate how it leads to models&#x27; susceptibility when encountering
inference-time jailbreak attacks. Furthermore, we show that detaching safety
mechanisms from the template region is promising in mitigating vulnerabilities
to jailbreak attacks. We encourage future research to develop more robust
safety alignment techniques that reduce reliance on the template region.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为什么受保护的船只会搁浅？对齐的大型语言模型的安全机制往往锚定在模板区域</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的安全对齐仍然脆弱，因为它们的初始行为可以被相对简单的攻击轻易破解。由于在输入指令和初始模型输出之间填充固定模板是现有LLMs的常见做法，我们假设这个模板是它们脆弱性的关键因素：LLMs的安全相关决策过于依赖于模板区域的聚合信息，这在很大程度上影响了这些模型的安全行为。我们将这个问题称为模板锚定的安全对齐。在本文中，我们进行了广泛的实验，验证了模板锚定的安全对齐在各种对齐的LLMs中普遍存在。我们的机制分析展示了它如何导致模型在遇到推理时的越狱攻击时的脆弱性。此外，我们还表明，将安全机制与模板区域分离在减轻对越狱攻击的脆弱性方面是有前景的。我们鼓励未来的研究开发更强大的安全对齐技术，以减少对模板区域的依赖。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerabilities in the safety alignment of large language models (LLMs), particularly how their reliance on fixed templates can lead to susceptibility to attacks. Previous methods have not effectively mitigated these vulnerabilities, as they often overlook the influence of the template region on safety-related decision-making. This paper proposes a novel approach that detaches safety mechanisms from the template region, which is well-motivated by the observed issues in existing models. The methodology involves extensive experiments and mechanistic analyses to demonstrate the prevalence of template-anchored safety alignment and its impact on model behavior during jailbreak attacks. The findings indicate that the proposed method significantly improves the robustness of LLMs against such attacks, supporting the goal of enhancing safety alignment in future research.</div>
<div class="mono" style="margin-top:8px">本研究关注大型语言模型（LLMs）在安全对齐方面的脆弱性，强调其初始行为容易受到简单攻击的影响。以往的方法依赖于固定模板，这影响了模型的安全行为，导致作者所称的“模板锚定安全对齐”问题。这种方法存在问题，因为它使LLMs在推理时容易受到越狱攻击。本文通过广泛的实验确认了这一问题在各种对齐LLMs中的普遍性，并提出了一种将安全机制与模板区域分离的方法，显示出减少脆弱性的潜力。该方法在抵御越狱攻击方面表现出更好的安全性能，支持了增强LLMs安全对齐鲁棒性的目标。</div>
</details>
</div>
<div class="card">
<div class="title">A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of   Iterative Chaos</div>
<div class="meta-line">Authors: Yang Yao, Xuan Tong, Ruofan Wang, Yixu Wang, Lujundong Li, Liang Liu, Yan Teng, Yingchun Wang</div>
<div class="meta-line">First: 2025-02-19T07:23:36+00:00 · Latest: 2025-06-03T14:35:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.15806v2">Abs</a> · <a href="http://arxiv.org/pdf/2502.15806v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) have significantly advanced beyond traditional
Large Language Models (LLMs) with their exceptional logical reasoning
capabilities, yet these improvements introduce heightened safety risks. When
subjected to jailbreak attacks, their ability to generate more targeted and
organized content can lead to greater harm. Although some studies claim that
reasoning enables safer LRMs against existing LLM attacks, they overlook the
inherent flaws within the reasoning process itself. To address this gap, we
propose the first jailbreak attack targeting LRMs, exploiting their unique
vulnerabilities stemming from the advanced reasoning capabilities.
Specifically, we introduce a Chaos Machine, a novel component to transform
attack prompts with diverse one-to-one mappings. The chaos mappings iteratively
generated by the machine are embedded into the reasoning chain, which
strengthens the variability and complexity and also promotes a more robust
attack. Based on this, we construct the Mousetrap framework, which makes
attacks projected into nonlinear-like low sample spaces with mismatched
generalization enhanced. Also, due to the more competing objectives, LRMs
gradually maintain the inertia of unpredictable iterative reasoning and fall
into our trap. Success rates of the Mousetrap attacking o1-mini, Claude-Sonnet
and Gemini-Thinking are as high as 96%, 86% and 98% respectively on our toxic
dataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench,
attacking Claude-Sonnet, well-known for its safety, Mousetrap can astonishingly
achieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This
paper contains inappropriate, offensive and harmful content.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>捕鼠器：利用迭代混沌链欺骗大型推理模型进行越狱</div>
<div class="mono" style="margin-top:8px">大型推理模型（LRMs）在逻辑推理能力上显著超越传统大型语言模型（LLMs），但这些改进也带来了更高的安全风险。在遭受越狱攻击时，它们生成更具针对性和组织性的内容的能力可能导致更大的危害。尽管一些研究声称推理使LRMs在现有LLM攻击下更安全，但它们忽视了推理过程本身的固有缺陷。为了解决这一问题，我们提出了首个针对LRMs的越狱攻击，利用其源于高级推理能力的独特脆弱性。具体而言，我们引入了一种混沌机器，这是一种新颖的组件，用于通过多样的一对一映射转换攻击提示。机器迭代生成的混沌映射嵌入到推理链中，增强了变异性和复杂性，同时促进了更强的攻击。基于此，我们构建了捕鼠器框架，使攻击投射到非线性低样本空间中，增强了不匹配的泛化。此外，由于目标的竞争性增加，LRMs逐渐保持不可预测的迭代推理的惯性，陷入我们的陷阱。在我们的有毒数据集Trotter上，捕鼠器对o1-mini、Claude-Sonnet和Gemini-Thinking的攻击成功率分别高达96%、86%和98%。在AdvBench、StrongREJECT和HarmBench等基准测试中，攻击以安全著称的Claude-Sonnet，捕鼠器惊人地实现了87.5%、86.58%和93.13%的成功率。注意：本文包含不当、冒犯性和有害内容。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the growing safety risks associated with Large Reasoning Models (LRMs), which have advanced logical reasoning capabilities compared to traditional Large Language Models (LLMs). Previous methods have claimed that reasoning enhances safety against LLM attacks, but they fail to recognize the vulnerabilities inherent in the reasoning process itself. The proposed approach introduces a novel jailbreak attack that exploits these vulnerabilities through a &#x27;Chaos Machine&#x27; that generates diverse attack prompts, enhancing the complexity and variability of the attacks. The Mousetrap framework, built on this concept, enables attacks in nonlinear low sample spaces, leading to high success rates of 96%, 86%, and 98% against models like o1-mini, Claude-Sonnet, and Gemini-Thinking, respectively, on a toxic dataset. Additionally, it achieves remarkable success rates of 87.5%, 86.58%, and 93.13% on established benchmarks, demonstrating its effectiveness in compromising even safety-oriented models.</div>
<div class="mono" style="margin-top:8px">本研究关注大型推理模型（LRMs）日益增加的安全风险，这些模型在逻辑推理能力上比传统的大型语言模型（LLMs）有了显著进步。以往的方法试图增强LRMs对越狱攻击的安全性，但忽视了推理过程本身固有的脆弱性。本文提出了一种专门针对LRMs的新型越狱攻击，介绍了一种混沌机器，通过多样的映射转换攻击提示，从而增强攻击的复杂性和变异性。构建的捕鼠器框架将攻击投射到非线性低样本空间，导致对o1-mini、Claude-Sonnet和Gemini-Thinking等模型的成功率分别高达96%、86%和98%，并在多个基准测试中取得显著成功率，从而证明了所提方法在利用LRM脆弱性方面的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251110_0329.html">20251110_0329</a>
<a href="archive/20251109_0314.html">20251109_0314</a>
<a href="archive/20251108_0315.html">20251108_0315</a>
<a href="archive/20251107_0317.html">20251107_0317</a>
<a href="archive/20251106_0349.html">20251106_0349</a>
<a href="archive/20251105_0321.html">20251105_0321</a>
<a href="archive/20251104_1539.html">20251104_1539</a>
<a href="archive/20251104_0324.html">20251104_0324</a>
<a href="archive/20251103_0315.html">20251103_0315</a>
<a href="archive/20251102_1356.html">20251102_1356</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
